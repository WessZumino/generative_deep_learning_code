{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.GAN import GAN\n",
    "from utils.loaders import load_safari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = '0035'\n",
    "RUN_FOLDER = os.path.join(\"./run\", RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = load_safari('camel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13f42da20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[200,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(input_dim = (28,28,1)\n",
    "        , discriminator_conv_filters = [64,64,128,128]\n",
    "        , discriminator_conv_kernel_size = [5,5,5,5]\n",
    "        , discriminator_conv_strides = [2,2,2,1]\n",
    "        , discriminator_conv_padding = 'same'\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'relu'\n",
    "        , discriminator_dropout_rate = None\n",
    "        , discriminator_learning_rate = 0.0008\n",
    "        , generator_initial_dense_layer_size = (7, 7, 64)\n",
    "        , generator_upsample = [2,2, 1, 1]\n",
    "        , generator_conv_filters = [128,64, 64,1]\n",
    "        , generator_conv_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_strides = [1,1, 1, 1]\n",
    "        , generator_conv_padding = 'same'\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0004\n",
    "        , optimiser = 'rmsprop'\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "gan.save(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 720,833\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_0 (Conv2DTran (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_1 (Conv2DTran (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_2 (Conv2DTran (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_3 (Conv2DTran (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 844,161\n",
      "Trainable params: 837,377\n",
      "Non-trainable params: 6,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 6000\n",
    "PRINT_EVERY_N_BATCHES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfoster/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (0.728)(R 0.690, F 0.767)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.676] [G acc: 1.000]\n",
      "1 [D loss: (1.750)(R 0.642, F 2.858)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.671] [G acc: 1.000]\n",
      "2 [D loss: (0.692)(R 0.668, F 0.716)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.661] [G acc: 1.000]\n",
      "3 [D loss: (0.705)(R 0.662, F 0.748)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.664] [G acc: 1.000]\n",
      "4 [D loss: (0.699)(R 0.666, F 0.733)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.666] [G acc: 1.000]\n",
      "5 [D loss: (0.699)(R 0.668, F 0.731)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.670] [G acc: 1.000]\n",
      "6 [D loss: (0.699)(R 0.670, F 0.727)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.673] [G acc: 1.000]\n",
      "7 [D loss: (0.698)(R 0.672, F 0.724)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.675] [G acc: 1.000]\n",
      "8 [D loss: (0.698)(R 0.674, F 0.722)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.677] [G acc: 1.000]\n",
      "9 [D loss: (0.697)(R 0.676, F 0.718)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.680] [G acc: 1.000]\n",
      "10 [D loss: (0.696)(R 0.677, F 0.715)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.683] [G acc: 1.000]\n",
      "11 [D loss: (0.699)(R 0.677, F 0.721)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.683] [G acc: 1.000]\n",
      "12 [D loss: (0.695)(R 0.679, F 0.711)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.685] [G acc: 1.000]\n",
      "13 [D loss: (0.697)(R 0.679, F 0.716)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.686] [G acc: 0.992]\n",
      "14 [D loss: (0.694)(R 0.682, F 0.707)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.687] [G acc: 1.000]\n",
      "15 [D loss: (0.698)(R 0.681, F 0.715)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.685] [G acc: 1.000]\n",
      "16 [D loss: (0.700)(R 0.684, F 0.717)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.689] [G acc: 0.969]\n",
      "17 [D loss: (0.694)(R 0.685, F 0.703)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.688] [G acc: 1.000]\n",
      "18 [D loss: (0.695)(R 0.686, F 0.705)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.694] [G acc: 0.414]\n",
      "19 [D loss: (0.694)(R 0.688, F 0.701)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.691] [G acc: 0.930]\n",
      "20 [D loss: (0.693)(R 0.687, F 0.699)] [D acc: (0.500)(0.992, 0.008)] [G loss: 0.693] [G acc: 0.555]\n",
      "21 [D loss: (0.696)(R 0.688, F 0.703)] [D acc: (0.484)(0.969, 0.000)] [G loss: 0.692] [G acc: 0.875]\n",
      "22 [D loss: (0.694)(R 0.688, F 0.699)] [D acc: (0.496)(0.984, 0.008)] [G loss: 0.693] [G acc: 0.625]\n",
      "23 [D loss: (0.694)(R 0.690, F 0.699)] [D acc: (0.473)(0.945, 0.000)] [G loss: 0.694] [G acc: 0.312]\n",
      "24 [D loss: (0.694)(R 0.689, F 0.698)] [D acc: (0.488)(0.945, 0.031)] [G loss: 0.694] [G acc: 0.453]\n",
      "25 [D loss: (0.695)(R 0.690, F 0.699)] [D acc: (0.438)(0.875, 0.000)] [G loss: 0.695] [G acc: 0.125]\n",
      "26 [D loss: (0.694)(R 0.691, F 0.698)] [D acc: (0.430)(0.820, 0.039)] [G loss: 0.693] [G acc: 0.414]\n",
      "27 [D loss: (0.694)(R 0.690, F 0.698)] [D acc: (0.461)(0.914, 0.008)] [G loss: 0.694] [G acc: 0.180]\n",
      "28 [D loss: (0.694)(R 0.690, F 0.698)] [D acc: (0.453)(0.875, 0.031)] [G loss: 0.694] [G acc: 0.266]\n",
      "29 [D loss: (0.694)(R 0.691, F 0.696)] [D acc: (0.453)(0.883, 0.023)] [G loss: 0.695] [G acc: 0.070]\n",
      "30 [D loss: (0.694)(R 0.691, F 0.696)] [D acc: (0.418)(0.781, 0.055)] [G loss: 0.694] [G acc: 0.242]\n",
      "31 [D loss: (0.694)(R 0.692, F 0.696)] [D acc: (0.469)(0.875, 0.062)] [G loss: 0.694] [G acc: 0.219]\n",
      "32 [D loss: (0.693)(R 0.691, F 0.695)] [D acc: (0.484)(0.867, 0.102)] [G loss: 0.694] [G acc: 0.281]\n",
      "33 [D loss: (0.707)(R 0.691, F 0.723)] [D acc: (0.418)(0.836, 0.000)] [G loss: 0.694] [G acc: 0.398]\n",
      "34 [D loss: (0.695)(R 0.692, F 0.698)] [D acc: (0.316)(0.594, 0.039)] [G loss: 0.695] [G acc: 0.188]\n",
      "35 [D loss: (0.698)(R 0.690, F 0.706)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.693] [G acc: 0.883]\n",
      "36 [D loss: (0.694)(R 0.691, F 0.696)] [D acc: (0.465)(0.930, 0.000)] [G loss: 0.693] [G acc: 0.789]\n",
      "37 [D loss: (0.694)(R 0.692, F 0.696)] [D acc: (0.434)(0.867, 0.000)] [G loss: 0.693] [G acc: 0.773]\n",
      "38 [D loss: (0.694)(R 0.692, F 0.696)] [D acc: (0.465)(0.930, 0.000)] [G loss: 0.693] [G acc: 0.500]\n",
      "39 [D loss: (0.694)(R 0.692, F 0.696)] [D acc: (0.426)(0.852, 0.000)] [G loss: 0.693] [G acc: 0.617]\n",
      "40 [D loss: (0.693)(R 0.692, F 0.695)] [D acc: (0.453)(0.898, 0.008)] [G loss: 0.693] [G acc: 0.383]\n",
      "41 [D loss: (0.693)(R 0.692, F 0.695)] [D acc: (0.438)(0.875, 0.000)] [G loss: 0.694] [G acc: 0.219]\n",
      "42 [D loss: (0.694)(R 0.692, F 0.695)] [D acc: (0.379)(0.758, 0.000)] [G loss: 0.693] [G acc: 0.344]\n",
      "43 [D loss: (0.694)(R 0.692, F 0.695)] [D acc: (0.426)(0.844, 0.008)] [G loss: 0.694] [G acc: 0.125]\n",
      "44 [D loss: (0.693)(R 0.692, F 0.695)] [D acc: (0.398)(0.781, 0.016)] [G loss: 0.693] [G acc: 0.141]\n",
      "45 [D loss: (0.693)(R 0.692, F 0.694)] [D acc: (0.438)(0.805, 0.070)] [G loss: 0.694] [G acc: 0.055]\n",
      "46 [D loss: (0.693)(R 0.692, F 0.695)] [D acc: (0.434)(0.852, 0.016)] [G loss: 0.694] [G acc: 0.039]\n",
      "47 [D loss: (0.693)(R 0.693, F 0.694)] [D acc: (0.402)(0.758, 0.047)] [G loss: 0.694] [G acc: 0.078]\n",
      "48 [D loss: (0.694)(R 0.691, F 0.697)] [D acc: (0.461)(0.922, 0.000)] [G loss: 0.701] [G acc: 0.000]\n",
      "49 [D loss: (0.697)(R 0.701, F 0.694)] [D acc: (0.027)(0.000, 0.055)] [G loss: 0.694] [G acc: 0.008]\n",
      "50 [D loss: (0.693)(R 0.692, F 0.695)] [D acc: (0.449)(0.773, 0.125)] [G loss: 0.694] [G acc: 0.039]\n",
      "51 [D loss: (0.695)(R 0.689, F 0.702)] [D acc: (0.512)(0.906, 0.117)] [G loss: 0.700] [G acc: 0.000]\n",
      "52 [D loss: (0.697)(R 0.697, F 0.697)] [D acc: (0.113)(0.203, 0.023)] [G loss: 0.697] [G acc: 0.000]\n",
      "53 [D loss: (0.698)(R 0.691, F 0.706)] [D acc: (0.359)(0.719, 0.000)] [G loss: 0.705] [G acc: 0.039]\n",
      "54 [D loss: (0.691)(R 0.682, F 0.700)] [D acc: (0.473)(0.930, 0.016)] [G loss: 0.709] [G acc: 0.016]\n",
      "55 [D loss: (0.781)(R 0.681, F 0.882)] [D acc: (0.355)(0.711, 0.000)] [G loss: 0.782] [G acc: 0.000]\n",
      "56 [D loss: (0.750)(R 0.789, F 0.711)] [D acc: (0.000)(0.000, 0.000)] [G loss: 0.700] [G acc: 0.000]\n",
      "57 [D loss: (0.701)(R 0.694, F 0.707)] [D acc: (0.215)(0.430, 0.000)] [G loss: 0.693] [G acc: 0.383]\n",
      "58 [D loss: (0.696)(R 0.686, F 0.705)] [D acc: (0.477)(0.953, 0.000)] [G loss: 0.691] [G acc: 0.914]\n",
      "59 [D loss: (0.695)(R 0.685, F 0.706)] [D acc: (0.477)(0.945, 0.008)] [G loss: 0.691] [G acc: 0.953]\n",
      "60 [D loss: (0.695)(R 0.683, F 0.706)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.691] [G acc: 0.945]\n",
      "61 [D loss: (0.694)(R 0.683, F 0.705)] [D acc: (0.480)(0.961, 0.000)] [G loss: 0.690] [G acc: 0.984]\n",
      "62 [D loss: (0.695)(R 0.682, F 0.708)] [D acc: (0.477)(0.953, 0.000)] [G loss: 0.690] [G acc: 1.000]\n",
      "63 [D loss: (0.694)(R 0.684, F 0.705)] [D acc: (0.480)(0.961, 0.000)] [G loss: 0.690] [G acc: 0.984]\n",
      "64 [D loss: (0.695)(R 0.683, F 0.707)] [D acc: (0.492)(0.984, 0.000)] [G loss: 0.690] [G acc: 1.000]\n",
      "65 [D loss: (0.694)(R 0.686, F 0.703)] [D acc: (0.492)(0.984, 0.000)] [G loss: 0.690] [G acc: 0.977]\n",
      "66 [D loss: (0.694)(R 0.685, F 0.703)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.691] [G acc: 1.000]\n",
      "67 [D loss: (0.693)(R 0.686, F 0.701)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.691] [G acc: 0.930]\n",
      "68 [D loss: (0.694)(R 0.685, F 0.703)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.692] [G acc: 0.789]\n",
      "69 [D loss: (0.694)(R 0.686, F 0.702)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.692] [G acc: 0.719]\n",
      "70 [D loss: (0.693)(R 0.686, F 0.700)] [D acc: (0.492)(0.984, 0.000)] [G loss: 0.693] [G acc: 0.422]\n",
      "71 [D loss: (0.694)(R 0.686, F 0.702)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.693] [G acc: 0.469]\n",
      "72 [D loss: (0.694)(R 0.686, F 0.701)] [D acc: (0.488)(0.977, 0.000)] [G loss: 0.694] [G acc: 0.227]\n",
      "73 [D loss: (0.695)(R 0.687, F 0.702)] [D acc: (0.453)(0.906, 0.000)] [G loss: 0.693] [G acc: 0.469]\n",
      "74 [D loss: (0.692)(R 0.685, F 0.700)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.694] [G acc: 0.234]\n",
      "75 [D loss: (0.693)(R 0.686, F 0.700)] [D acc: (0.488)(0.961, 0.016)] [G loss: 0.695] [G acc: 0.211]\n",
      "76 [D loss: (0.694)(R 0.684, F 0.704)] [D acc: (0.500)(0.984, 0.016)] [G loss: 0.694] [G acc: 0.344]\n",
      "77 [D loss: (0.692)(R 0.685, F 0.699)] [D acc: (0.531)(0.984, 0.078)] [G loss: 0.696] [G acc: 0.062]\n",
      "78 [D loss: (0.694)(R 0.686, F 0.702)] [D acc: (0.441)(0.867, 0.016)] [G loss: 0.699] [G acc: 0.000]\n",
      "79 [D loss: (0.700)(R 0.680, F 0.720)] [D acc: (0.504)(1.000, 0.008)] [G loss: 0.702] [G acc: 0.008]\n",
      "80 [D loss: (0.696)(R 0.690, F 0.701)] [D acc: (0.344)(0.570, 0.117)] [G loss: 0.699] [G acc: 0.023]\n",
      "81 [D loss: (0.698)(R 0.685, F 0.711)] [D acc: (0.434)(0.859, 0.008)] [G loss: 0.695] [G acc: 0.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 [D loss: (0.696)(R 0.685, F 0.707)] [D acc: (0.422)(0.844, 0.000)] [G loss: 0.694] [G acc: 0.281]\n",
      "83 [D loss: (0.692)(R 0.683, F 0.700)] [D acc: (0.500)(0.969, 0.031)] [G loss: 0.694] [G acc: 0.398]\n",
      "84 [D loss: (0.694)(R 0.681, F 0.706)] [D acc: (0.465)(0.914, 0.016)] [G loss: 0.693] [G acc: 0.406]\n",
      "85 [D loss: (0.693)(R 0.681, F 0.705)] [D acc: (0.480)(0.945, 0.016)] [G loss: 0.695] [G acc: 0.219]\n",
      "86 [D loss: (0.694)(R 0.680, F 0.708)] [D acc: (0.465)(0.930, 0.000)] [G loss: 0.694] [G acc: 0.289]\n",
      "87 [D loss: (0.693)(R 0.681, F 0.706)] [D acc: (0.484)(0.945, 0.023)] [G loss: 0.696] [G acc: 0.242]\n",
      "88 [D loss: (0.695)(R 0.682, F 0.709)] [D acc: (0.453)(0.891, 0.016)] [G loss: 0.695] [G acc: 0.180]\n",
      "89 [D loss: (0.690)(R 0.682, F 0.697)] [D acc: (0.562)(0.922, 0.203)] [G loss: 0.693] [G acc: 0.414]\n",
      "90 [D loss: (0.707)(R 0.672, F 0.741)] [D acc: (0.453)(0.898, 0.008)] [G loss: 0.735] [G acc: 0.000]\n",
      "91 [D loss: (0.703)(R 0.707, F 0.698)] [D acc: (0.066)(0.070, 0.062)] [G loss: 0.692] [G acc: 0.781]\n",
      "92 [D loss: (0.690)(R 0.685, F 0.695)] [D acc: (0.707)(0.984, 0.430)] [G loss: 0.692] [G acc: 0.547]\n",
      "93 [D loss: (0.699)(R 0.678, F 0.721)] [D acc: (0.453)(0.883, 0.023)] [G loss: 0.693] [G acc: 0.523]\n",
      "94 [D loss: (0.695)(R 0.684, F 0.706)] [D acc: (0.395)(0.773, 0.016)] [G loss: 0.691] [G acc: 0.641]\n",
      "95 [D loss: (0.700)(R 0.681, F 0.719)] [D acc: (0.414)(0.820, 0.008)] [G loss: 0.693] [G acc: 0.531]\n",
      "96 [D loss: (0.695)(R 0.683, F 0.706)] [D acc: (0.422)(0.766, 0.078)] [G loss: 0.689] [G acc: 0.773]\n",
      "97 [D loss: (0.699)(R 0.683, F 0.715)] [D acc: (0.434)(0.867, 0.000)] [G loss: 0.696] [G acc: 0.336]\n",
      "98 [D loss: (0.726)(R 0.688, F 0.764)] [D acc: (0.414)(0.828, 0.000)] [G loss: 0.703] [G acc: 0.062]\n",
      "99 [D loss: (0.689)(R 0.681, F 0.696)] [D acc: (0.625)(0.898, 0.352)] [G loss: 0.698] [G acc: 0.227]\n",
      "100 [D loss: (0.696)(R 0.678, F 0.714)] [D acc: (0.453)(0.789, 0.117)] [G loss: 0.776] [G acc: 0.000]\n",
      "101 [D loss: (0.713)(R 0.717, F 0.709)] [D acc: (0.102)(0.109, 0.094)] [G loss: 0.692] [G acc: 0.578]\n",
      "102 [D loss: (0.697)(R 0.677, F 0.717)] [D acc: (0.488)(0.938, 0.039)] [G loss: 0.693] [G acc: 0.422]\n",
      "103 [D loss: (0.694)(R 0.681, F 0.708)] [D acc: (0.457)(0.859, 0.055)] [G loss: 0.691] [G acc: 0.664]\n",
      "104 [D loss: (0.693)(R 0.677, F 0.708)] [D acc: (0.547)(0.969, 0.125)] [G loss: 0.691] [G acc: 0.539]\n",
      "105 [D loss: (0.702)(R 0.679, F 0.725)] [D acc: (0.375)(0.750, 0.000)] [G loss: 0.700] [G acc: 0.219]\n",
      "106 [D loss: (0.693)(R 0.682, F 0.704)] [D acc: (0.461)(0.742, 0.180)] [G loss: 0.685] [G acc: 0.766]\n",
      "107 [D loss: (0.706)(R 0.678, F 0.735)] [D acc: (0.426)(0.836, 0.016)] [G loss: 0.692] [G acc: 0.547]\n",
      "108 [D loss: (0.697)(R 0.685, F 0.710)] [D acc: (0.473)(0.844, 0.102)] [G loss: 0.687] [G acc: 0.797]\n",
      "109 [D loss: (0.701)(R 0.685, F 0.717)] [D acc: (0.410)(0.789, 0.031)] [G loss: 0.718] [G acc: 0.008]\n",
      "110 [D loss: (0.693)(R 0.696, F 0.690)] [D acc: (0.562)(0.359, 0.766)] [G loss: 0.705] [G acc: 0.109]\n",
      "111 [D loss: (0.699)(R 0.688, F 0.711)] [D acc: (0.441)(0.625, 0.258)] [G loss: 0.726] [G acc: 0.000]\n",
      "112 [D loss: (0.700)(R 0.701, F 0.700)] [D acc: (0.352)(0.320, 0.383)] [G loss: 0.697] [G acc: 0.328]\n",
      "113 [D loss: (0.700)(R 0.685, F 0.714)] [D acc: (0.402)(0.805, 0.000)] [G loss: 0.696] [G acc: 0.391]\n",
      "114 [D loss: (0.696)(R 0.687, F 0.705)] [D acc: (0.426)(0.812, 0.039)] [G loss: 0.689] [G acc: 0.805]\n",
      "115 [D loss: (0.695)(R 0.685, F 0.705)] [D acc: (0.461)(0.883, 0.039)] [G loss: 0.689] [G acc: 0.828]\n",
      "116 [D loss: (0.695)(R 0.685, F 0.704)] [D acc: (0.469)(0.906, 0.031)] [G loss: 0.690] [G acc: 0.750]\n",
      "117 [D loss: (0.696)(R 0.686, F 0.705)] [D acc: (0.449)(0.898, 0.000)] [G loss: 0.691] [G acc: 0.695]\n",
      "118 [D loss: (0.695)(R 0.686, F 0.703)] [D acc: (0.441)(0.852, 0.031)] [G loss: 0.691] [G acc: 0.656]\n",
      "119 [D loss: (0.695)(R 0.686, F 0.703)] [D acc: (0.449)(0.867, 0.031)] [G loss: 0.692] [G acc: 0.641]\n",
      "120 [D loss: (0.695)(R 0.687, F 0.702)] [D acc: (0.453)(0.859, 0.047)] [G loss: 0.692] [G acc: 0.648]\n",
      "121 [D loss: (0.693)(R 0.686, F 0.700)] [D acc: (0.426)(0.812, 0.039)] [G loss: 0.694] [G acc: 0.438]\n",
      "122 [D loss: (0.696)(R 0.685, F 0.707)] [D acc: (0.523)(0.852, 0.195)] [G loss: 0.693] [G acc: 0.484]\n",
      "123 [D loss: (0.695)(R 0.688, F 0.702)] [D acc: (0.469)(0.820, 0.117)] [G loss: 0.692] [G acc: 0.547]\n",
      "124 [D loss: (0.694)(R 0.688, F 0.701)] [D acc: (0.461)(0.797, 0.125)] [G loss: 0.693] [G acc: 0.484]\n",
      "125 [D loss: (0.693)(R 0.688, F 0.699)] [D acc: (0.488)(0.844, 0.133)] [G loss: 0.693] [G acc: 0.531]\n",
      "126 [D loss: (0.694)(R 0.688, F 0.699)] [D acc: (0.469)(0.766, 0.172)] [G loss: 0.693] [G acc: 0.445]\n",
      "127 [D loss: (0.696)(R 0.687, F 0.704)] [D acc: (0.492)(0.836, 0.148)] [G loss: 0.695] [G acc: 0.359]\n",
      "128 [D loss: (0.695)(R 0.689, F 0.701)] [D acc: (0.473)(0.797, 0.148)] [G loss: 0.697] [G acc: 0.234]\n",
      "129 [D loss: (0.693)(R 0.687, F 0.699)] [D acc: (0.441)(0.711, 0.172)] [G loss: 0.692] [G acc: 0.570]\n",
      "130 [D loss: (0.696)(R 0.688, F 0.704)] [D acc: (0.496)(0.859, 0.133)] [G loss: 0.693] [G acc: 0.477]\n",
      "131 [D loss: (0.696)(R 0.685, F 0.706)] [D acc: (0.469)(0.875, 0.062)] [G loss: 0.695] [G acc: 0.383]\n",
      "132 [D loss: (0.694)(R 0.688, F 0.699)] [D acc: (0.477)(0.789, 0.164)] [G loss: 0.693] [G acc: 0.445]\n",
      "133 [D loss: (0.694)(R 0.687, F 0.701)] [D acc: (0.500)(0.867, 0.133)] [G loss: 0.698] [G acc: 0.219]\n",
      "134 [D loss: (0.695)(R 0.688, F 0.702)] [D acc: (0.383)(0.727, 0.039)] [G loss: 0.692] [G acc: 0.555]\n",
      "135 [D loss: (0.695)(R 0.686, F 0.704)] [D acc: (0.477)(0.867, 0.086)] [G loss: 0.702] [G acc: 0.164]\n",
      "136 [D loss: (0.696)(R 0.690, F 0.702)] [D acc: (0.332)(0.594, 0.070)] [G loss: 0.693] [G acc: 0.453]\n",
      "137 [D loss: (0.711)(R 0.684, F 0.737)] [D acc: (0.449)(0.852, 0.047)] [G loss: 0.690] [G acc: 0.586]\n",
      "138 [D loss: (0.723)(R 0.678, F 0.767)] [D acc: (0.488)(0.914, 0.062)] [G loss: 0.691] [G acc: 0.641]\n",
      "139 [D loss: (0.697)(R 0.686, F 0.708)] [D acc: (0.410)(0.773, 0.047)] [G loss: 0.699] [G acc: 0.125]\n",
      "140 [D loss: (0.695)(R 0.689, F 0.701)] [D acc: (0.418)(0.766, 0.070)] [G loss: 0.692] [G acc: 0.641]\n",
      "141 [D loss: (0.693)(R 0.686, F 0.700)] [D acc: (0.488)(0.906, 0.070)] [G loss: 0.692] [G acc: 0.508]\n",
      "142 [D loss: (0.694)(R 0.685, F 0.704)] [D acc: (0.453)(0.844, 0.062)] [G loss: 0.691] [G acc: 0.688]\n",
      "143 [D loss: (0.697)(R 0.686, F 0.707)] [D acc: (0.414)(0.773, 0.055)] [G loss: 0.694] [G acc: 0.492]\n",
      "144 [D loss: (0.695)(R 0.686, F 0.703)] [D acc: (0.426)(0.805, 0.047)] [G loss: 0.692] [G acc: 0.586]\n",
      "145 [D loss: (0.694)(R 0.685, F 0.704)] [D acc: (0.461)(0.852, 0.070)] [G loss: 0.696] [G acc: 0.281]\n",
      "146 [D loss: (0.694)(R 0.688, F 0.699)] [D acc: (0.395)(0.695, 0.094)] [G loss: 0.694] [G acc: 0.398]\n",
      "147 [D loss: (0.695)(R 0.685, F 0.704)] [D acc: (0.453)(0.875, 0.031)] [G loss: 0.693] [G acc: 0.578]\n",
      "148 [D loss: (0.694)(R 0.684, F 0.704)] [D acc: (0.484)(0.922, 0.047)] [G loss: 0.691] [G acc: 0.719]\n",
      "149 [D loss: (0.695)(R 0.684, F 0.705)] [D acc: (0.477)(0.891, 0.062)] [G loss: 0.695] [G acc: 0.352]\n",
      "150 [D loss: (0.698)(R 0.686, F 0.710)] [D acc: (0.418)(0.766, 0.070)] [G loss: 0.694] [G acc: 0.461]\n",
      "151 [D loss: (0.698)(R 0.681, F 0.716)] [D acc: (0.480)(0.859, 0.102)] [G loss: 0.695] [G acc: 0.453]\n",
      "152 [D loss: (0.694)(R 0.683, F 0.704)] [D acc: (0.430)(0.758, 0.102)] [G loss: 0.696] [G acc: 0.281]\n",
      "153 [D loss: (0.702)(R 0.682, F 0.723)] [D acc: (0.434)(0.742, 0.125)] [G loss: 0.707] [G acc: 0.219]\n",
      "154 [D loss: (0.692)(R 0.689, F 0.696)] [D acc: (0.434)(0.555, 0.312)] [G loss: 0.716] [G acc: 0.000]\n",
      "155 [D loss: (0.725)(R 0.669, F 0.781)] [D acc: (0.457)(0.891, 0.023)] [G loss: 0.697] [G acc: 0.375]\n",
      "156 [D loss: (0.696)(R 0.681, F 0.712)] [D acc: (0.469)(0.688, 0.250)] [G loss: 0.730] [G acc: 0.023]\n",
      "157 [D loss: (0.702)(R 0.701, F 0.702)] [D acc: (0.383)(0.398, 0.367)] [G loss: 0.862] [G acc: 0.000]\n",
      "158 [D loss: (0.725)(R 0.727, F 0.723)] [D acc: (0.160)(0.266, 0.055)] [G loss: 0.694] [G acc: 0.391]\n",
      "159 [D loss: (0.697)(R 0.672, F 0.722)] [D acc: (0.453)(0.844, 0.062)] [G loss: 0.694] [G acc: 0.414]\n",
      "160 [D loss: (0.687)(R 0.666, F 0.709)] [D acc: (0.590)(0.930, 0.250)] [G loss: 0.700] [G acc: 0.297]\n",
      "161 [D loss: (0.693)(R 0.673, F 0.712)] [D acc: (0.441)(0.688, 0.195)] [G loss: 0.688] [G acc: 0.516]\n",
      "162 [D loss: (0.704)(R 0.654, F 0.753)] [D acc: (0.508)(0.883, 0.133)] [G loss: 0.687] [G acc: 0.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 [D loss: (0.688)(R 0.664, F 0.711)] [D acc: (0.617)(0.875, 0.359)] [G loss: 0.682] [G acc: 0.633]\n",
      "164 [D loss: (0.705)(R 0.655, F 0.754)] [D acc: (0.488)(0.789, 0.188)] [G loss: 0.696] [G acc: 0.344]\n",
      "165 [D loss: (0.700)(R 0.665, F 0.734)] [D acc: (0.473)(0.773, 0.172)] [G loss: 0.686] [G acc: 0.633]\n",
      "166 [D loss: (0.699)(R 0.665, F 0.732)] [D acc: (0.504)(0.867, 0.141)] [G loss: 0.726] [G acc: 0.281]\n",
      "167 [D loss: (0.695)(R 0.683, F 0.706)] [D acc: (0.402)(0.648, 0.156)] [G loss: 0.696] [G acc: 0.312]\n",
      "168 [D loss: (0.696)(R 0.668, F 0.724)] [D acc: (0.477)(0.844, 0.109)] [G loss: 0.689] [G acc: 0.633]\n",
      "169 [D loss: (0.692)(R 0.670, F 0.714)] [D acc: (0.488)(0.852, 0.125)] [G loss: 0.687] [G acc: 0.664]\n",
      "170 [D loss: (0.698)(R 0.670, F 0.725)] [D acc: (0.469)(0.820, 0.117)] [G loss: 0.689] [G acc: 0.625]\n",
      "171 [D loss: (0.697)(R 0.676, F 0.718)] [D acc: (0.422)(0.758, 0.086)] [G loss: 0.687] [G acc: 0.617]\n",
      "172 [D loss: (0.698)(R 0.671, F 0.725)] [D acc: (0.434)(0.805, 0.062)] [G loss: 0.685] [G acc: 0.828]\n",
      "173 [D loss: (0.695)(R 0.673, F 0.717)] [D acc: (0.504)(0.922, 0.086)] [G loss: 0.687] [G acc: 0.727]\n",
      "174 [D loss: (0.696)(R 0.676, F 0.715)] [D acc: (0.465)(0.859, 0.070)] [G loss: 0.689] [G acc: 0.617]\n",
      "175 [D loss: (0.695)(R 0.675, F 0.715)] [D acc: (0.465)(0.898, 0.031)] [G loss: 0.688] [G acc: 0.703]\n",
      "176 [D loss: (0.694)(R 0.676, F 0.711)] [D acc: (0.551)(0.922, 0.180)] [G loss: 0.692] [G acc: 0.531]\n",
      "177 [D loss: (0.697)(R 0.680, F 0.715)] [D acc: (0.453)(0.812, 0.094)] [G loss: 0.692] [G acc: 0.523]\n",
      "178 [D loss: (0.695)(R 0.672, F 0.718)] [D acc: (0.477)(0.844, 0.109)] [G loss: 0.690] [G acc: 0.602]\n",
      "179 [D loss: (0.691)(R 0.674, F 0.708)] [D acc: (0.527)(0.867, 0.188)] [G loss: 0.693] [G acc: 0.469]\n",
      "180 [D loss: (0.691)(R 0.674, F 0.709)] [D acc: (0.520)(0.844, 0.195)] [G loss: 0.694] [G acc: 0.422]\n",
      "181 [D loss: (0.693)(R 0.674, F 0.713)] [D acc: (0.477)(0.758, 0.195)] [G loss: 0.689] [G acc: 0.633]\n",
      "182 [D loss: (0.696)(R 0.672, F 0.720)] [D acc: (0.488)(0.867, 0.109)] [G loss: 0.694] [G acc: 0.516]\n",
      "183 [D loss: (0.690)(R 0.669, F 0.712)] [D acc: (0.504)(0.859, 0.148)] [G loss: 0.692] [G acc: 0.531]\n",
      "184 [D loss: (0.692)(R 0.668, F 0.715)] [D acc: (0.477)(0.812, 0.141)] [G loss: 0.689] [G acc: 0.555]\n",
      "185 [D loss: (0.697)(R 0.668, F 0.725)] [D acc: (0.492)(0.883, 0.102)] [G loss: 0.696] [G acc: 0.445]\n",
      "186 [D loss: (0.716)(R 0.666, F 0.765)] [D acc: (0.387)(0.703, 0.070)] [G loss: 0.689] [G acc: 0.531]\n",
      "187 [D loss: (0.717)(R 0.662, F 0.773)] [D acc: (0.445)(0.852, 0.039)] [G loss: 0.759] [G acc: 0.055]\n",
      "188 [D loss: (0.722)(R 0.745, F 0.699)] [D acc: (0.277)(0.141, 0.414)] [G loss: 0.730] [G acc: 0.070]\n",
      "189 [D loss: (0.755)(R 0.626, F 0.885)] [D acc: (0.477)(0.953, 0.000)] [G loss: 0.701] [G acc: 0.414]\n",
      "190 [D loss: (0.694)(R 0.653, F 0.735)] [D acc: (0.473)(0.859, 0.086)] [G loss: 0.717] [G acc: 0.195]\n",
      "191 [D loss: (0.705)(R 0.666, F 0.743)] [D acc: (0.410)(0.734, 0.086)] [G loss: 0.751] [G acc: 0.133]\n",
      "192 [D loss: (0.708)(R 0.656, F 0.760)] [D acc: (0.406)(0.797, 0.016)] [G loss: 0.693] [G acc: 0.547]\n",
      "193 [D loss: (0.707)(R 0.654, F 0.759)] [D acc: (0.402)(0.766, 0.039)] [G loss: 0.697] [G acc: 0.438]\n",
      "194 [D loss: (0.694)(R 0.656, F 0.731)] [D acc: (0.457)(0.844, 0.070)] [G loss: 0.691] [G acc: 0.508]\n",
      "195 [D loss: (0.705)(R 0.650, F 0.759)] [D acc: (0.453)(0.852, 0.055)] [G loss: 0.676] [G acc: 0.617]\n",
      "196 [D loss: (0.696)(R 0.638, F 0.753)] [D acc: (0.488)(0.875, 0.102)] [G loss: 0.679] [G acc: 0.625]\n",
      "197 [D loss: (0.714)(R 0.660, F 0.768)] [D acc: (0.406)(0.766, 0.047)] [G loss: 0.691] [G acc: 0.531]\n",
      "198 [D loss: (0.691)(R 0.651, F 0.731)] [D acc: (0.488)(0.875, 0.102)] [G loss: 0.678] [G acc: 0.805]\n",
      "199 [D loss: (0.698)(R 0.652, F 0.744)] [D acc: (0.500)(0.930, 0.070)] [G loss: 0.681] [G acc: 0.734]\n",
      "200 [D loss: (0.696)(R 0.657, F 0.734)] [D acc: (0.473)(0.914, 0.031)] [G loss: 0.682] [G acc: 0.633]\n",
      "201 [D loss: (0.702)(R 0.628, F 0.777)] [D acc: (0.477)(0.875, 0.078)] [G loss: 0.679] [G acc: 0.656]\n",
      "202 [D loss: (0.700)(R 0.648, F 0.751)] [D acc: (0.449)(0.844, 0.055)] [G loss: 0.703] [G acc: 0.430]\n",
      "203 [D loss: (0.692)(R 0.655, F 0.728)] [D acc: (0.438)(0.812, 0.062)] [G loss: 0.698] [G acc: 0.398]\n",
      "204 [D loss: (0.898)(R 0.640, F 1.156)] [D acc: (0.445)(0.867, 0.023)] [G loss: 0.678] [G acc: 0.664]\n",
      "205 [D loss: (0.715)(R 0.652, F 0.778)] [D acc: (0.457)(0.875, 0.039)] [G loss: 0.679] [G acc: 0.672]\n",
      "206 [D loss: (0.688)(R 0.652, F 0.724)] [D acc: (0.527)(0.852, 0.203)] [G loss: 0.667] [G acc: 0.844]\n",
      "207 [D loss: (0.701)(R 0.644, F 0.758)] [D acc: (0.473)(0.891, 0.055)] [G loss: 0.678] [G acc: 0.727]\n",
      "208 [D loss: (0.694)(R 0.649, F 0.739)] [D acc: (0.473)(0.891, 0.055)] [G loss: 0.664] [G acc: 0.805]\n",
      "209 [D loss: (0.705)(R 0.643, F 0.767)] [D acc: (0.430)(0.820, 0.039)] [G loss: 0.734] [G acc: 0.133]\n",
      "210 [D loss: (0.707)(R 0.685, F 0.730)] [D acc: (0.320)(0.570, 0.070)] [G loss: 0.685] [G acc: 0.664]\n",
      "211 [D loss: (0.689)(R 0.625, F 0.754)] [D acc: (0.512)(0.969, 0.055)] [G loss: 0.675] [G acc: 0.766]\n",
      "212 [D loss: (0.699)(R 0.645, F 0.753)] [D acc: (0.477)(0.891, 0.062)] [G loss: 0.682] [G acc: 0.711]\n",
      "213 [D loss: (0.698)(R 0.649, F 0.748)] [D acc: (0.465)(0.898, 0.031)] [G loss: 0.682] [G acc: 0.703]\n",
      "214 [D loss: (0.694)(R 0.655, F 0.734)] [D acc: (0.453)(0.852, 0.055)] [G loss: 0.679] [G acc: 0.734]\n",
      "215 [D loss: (0.696)(R 0.640, F 0.752)] [D acc: (0.473)(0.914, 0.031)] [G loss: 0.673] [G acc: 0.805]\n",
      "216 [D loss: (0.713)(R 0.648, F 0.777)] [D acc: (0.453)(0.906, 0.000)] [G loss: 0.701] [G acc: 0.422]\n",
      "217 [D loss: (0.701)(R 0.663, F 0.738)] [D acc: (0.441)(0.836, 0.047)] [G loss: 0.681] [G acc: 0.742]\n",
      "218 [D loss: (0.700)(R 0.661, F 0.739)] [D acc: (0.477)(0.891, 0.062)] [G loss: 0.690] [G acc: 0.625]\n",
      "219 [D loss: (0.693)(R 0.654, F 0.731)] [D acc: (0.496)(0.930, 0.062)] [G loss: 0.681] [G acc: 0.750]\n",
      "220 [D loss: (0.696)(R 0.655, F 0.736)] [D acc: (0.480)(0.906, 0.055)] [G loss: 0.680] [G acc: 0.664]\n",
      "221 [D loss: (0.693)(R 0.651, F 0.736)] [D acc: (0.504)(0.938, 0.070)] [G loss: 0.682] [G acc: 0.695]\n",
      "222 [D loss: (0.698)(R 0.653, F 0.742)] [D acc: (0.516)(0.945, 0.086)] [G loss: 0.680] [G acc: 0.625]\n",
      "223 [D loss: (0.707)(R 0.658, F 0.757)] [D acc: (0.496)(0.898, 0.094)] [G loss: 0.715] [G acc: 0.336]\n",
      "224 [D loss: (0.685)(R 0.662, F 0.707)] [D acc: (0.547)(0.836, 0.258)] [G loss: 0.731] [G acc: 0.125]\n",
      "225 [D loss: (0.706)(R 0.674, F 0.738)] [D acc: (0.395)(0.664, 0.125)] [G loss: 0.794] [G acc: 0.039]\n",
      "226 [D loss: (0.727)(R 0.721, F 0.733)] [D acc: (0.199)(0.328, 0.070)] [G loss: 0.696] [G acc: 0.406]\n",
      "227 [D loss: (0.688)(R 0.638, F 0.738)] [D acc: (0.516)(0.906, 0.125)] [G loss: 0.691] [G acc: 0.516]\n",
      "228 [D loss: (0.695)(R 0.648, F 0.741)] [D acc: (0.520)(0.867, 0.172)] [G loss: 0.686] [G acc: 0.570]\n",
      "229 [D loss: (0.701)(R 0.633, F 0.769)] [D acc: (0.473)(0.883, 0.062)] [G loss: 0.676] [G acc: 0.641]\n",
      "230 [D loss: (0.706)(R 0.637, F 0.774)] [D acc: (0.461)(0.828, 0.094)] [G loss: 0.677] [G acc: 0.633]\n",
      "231 [D loss: (0.706)(R 0.623, F 0.789)] [D acc: (0.531)(0.906, 0.156)] [G loss: 0.702] [G acc: 0.500]\n",
      "232 [D loss: (0.702)(R 0.653, F 0.752)] [D acc: (0.449)(0.742, 0.156)] [G loss: 0.681] [G acc: 0.562]\n",
      "233 [D loss: (0.708)(R 0.632, F 0.783)] [D acc: (0.469)(0.867, 0.070)] [G loss: 0.691] [G acc: 0.477]\n",
      "234 [D loss: (0.701)(R 0.640, F 0.762)] [D acc: (0.504)(0.820, 0.188)] [G loss: 0.678] [G acc: 0.664]\n",
      "235 [D loss: (0.697)(R 0.625, F 0.770)] [D acc: (0.527)(0.914, 0.141)] [G loss: 0.766] [G acc: 0.086]\n",
      "236 [D loss: (0.715)(R 0.715, F 0.715)] [D acc: (0.344)(0.367, 0.320)] [G loss: 0.720] [G acc: 0.219]\n",
      "237 [D loss: (0.695)(R 0.630, F 0.760)] [D acc: (0.492)(0.859, 0.125)] [G loss: 0.726] [G acc: 0.320]\n",
      "238 [D loss: (0.700)(R 0.638, F 0.762)] [D acc: (0.449)(0.758, 0.141)] [G loss: 0.697] [G acc: 0.367]\n",
      "239 [D loss: (0.710)(R 0.642, F 0.779)] [D acc: (0.453)(0.805, 0.102)] [G loss: 0.692] [G acc: 0.555]\n",
      "240 [D loss: (0.686)(R 0.637, F 0.734)] [D acc: (0.559)(0.898, 0.219)] [G loss: 0.685] [G acc: 0.586]\n",
      "241 [D loss: (0.702)(R 0.638, F 0.766)] [D acc: (0.457)(0.867, 0.047)] [G loss: 0.700] [G acc: 0.406]\n",
      "242 [D loss: (0.695)(R 0.639, F 0.751)] [D acc: (0.484)(0.844, 0.125)] [G loss: 0.697] [G acc: 0.500]\n",
      "243 [D loss: (0.718)(R 0.631, F 0.804)] [D acc: (0.422)(0.781, 0.062)] [G loss: 0.699] [G acc: 0.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 [D loss: (0.695)(R 0.643, F 0.746)] [D acc: (0.441)(0.805, 0.078)] [G loss: 0.682] [G acc: 0.625]\n",
      "245 [D loss: (0.692)(R 0.636, F 0.748)] [D acc: (0.473)(0.852, 0.094)] [G loss: 0.693] [G acc: 0.453]\n",
      "246 [D loss: (0.697)(R 0.645, F 0.749)] [D acc: (0.445)(0.766, 0.125)] [G loss: 0.684] [G acc: 0.602]\n",
      "247 [D loss: (0.713)(R 0.617, F 0.810)] [D acc: (0.453)(0.883, 0.023)] [G loss: 0.695] [G acc: 0.500]\n",
      "248 [D loss: (0.713)(R 0.637, F 0.789)] [D acc: (0.449)(0.820, 0.078)] [G loss: 0.686] [G acc: 0.594]\n",
      "249 [D loss: (0.692)(R 0.634, F 0.750)] [D acc: (0.480)(0.852, 0.109)] [G loss: 0.693] [G acc: 0.445]\n",
      "250 [D loss: (0.692)(R 0.649, F 0.736)] [D acc: (0.465)(0.781, 0.148)] [G loss: 0.689] [G acc: 0.523]\n",
      "251 [D loss: (0.699)(R 0.636, F 0.761)] [D acc: (0.465)(0.859, 0.070)] [G loss: 0.686] [G acc: 0.602]\n",
      "252 [D loss: (0.694)(R 0.641, F 0.747)] [D acc: (0.484)(0.883, 0.086)] [G loss: 0.694] [G acc: 0.484]\n",
      "253 [D loss: (0.691)(R 0.639, F 0.743)] [D acc: (0.504)(0.828, 0.180)] [G loss: 0.677] [G acc: 0.609]\n",
      "254 [D loss: (0.696)(R 0.624, F 0.768)] [D acc: (0.520)(0.828, 0.211)] [G loss: 0.688] [G acc: 0.547]\n",
      "255 [D loss: (0.689)(R 0.641, F 0.737)] [D acc: (0.527)(0.820, 0.234)] [G loss: 0.692] [G acc: 0.453]\n",
      "256 [D loss: (0.702)(R 0.652, F 0.751)] [D acc: (0.402)(0.695, 0.109)] [G loss: 0.680] [G acc: 0.688]\n",
      "257 [D loss: (0.695)(R 0.644, F 0.747)] [D acc: (0.531)(0.891, 0.172)] [G loss: 0.689] [G acc: 0.562]\n",
      "258 [D loss: (0.694)(R 0.638, F 0.751)] [D acc: (0.527)(0.898, 0.156)] [G loss: 0.697] [G acc: 0.398]\n",
      "259 [D loss: (0.693)(R 0.657, F 0.728)] [D acc: (0.449)(0.727, 0.172)] [G loss: 0.693] [G acc: 0.484]\n",
      "260 [D loss: (0.686)(R 0.638, F 0.735)] [D acc: (0.531)(0.859, 0.203)] [G loss: 0.733] [G acc: 0.234]\n",
      "261 [D loss: (0.694)(R 0.650, F 0.739)] [D acc: (0.461)(0.734, 0.188)] [G loss: 0.689] [G acc: 0.492]\n",
      "262 [D loss: (0.744)(R 0.640, F 0.848)] [D acc: (0.496)(0.859, 0.133)] [G loss: 0.719] [G acc: 0.258]\n",
      "263 [D loss: (0.708)(R 0.634, F 0.782)] [D acc: (0.434)(0.789, 0.078)] [G loss: 0.685] [G acc: 0.594]\n",
      "264 [D loss: (0.707)(R 0.642, F 0.772)] [D acc: (0.492)(0.898, 0.086)] [G loss: 0.690] [G acc: 0.516]\n",
      "265 [D loss: (0.697)(R 0.637, F 0.757)] [D acc: (0.488)(0.805, 0.172)] [G loss: 0.697] [G acc: 0.406]\n",
      "266 [D loss: (0.728)(R 0.642, F 0.813)] [D acc: (0.422)(0.781, 0.062)] [G loss: 0.713] [G acc: 0.336]\n",
      "267 [D loss: (0.711)(R 0.645, F 0.777)] [D acc: (0.426)(0.789, 0.062)] [G loss: 0.722] [G acc: 0.328]\n",
      "268 [D loss: (0.752)(R 0.645, F 0.860)] [D acc: (0.387)(0.750, 0.023)] [G loss: 0.739] [G acc: 0.227]\n",
      "269 [D loss: (0.740)(R 0.660, F 0.821)] [D acc: (0.391)(0.734, 0.047)] [G loss: 0.714] [G acc: 0.359]\n",
      "270 [D loss: (0.811)(R 0.628, F 0.994)] [D acc: (0.355)(0.711, 0.000)] [G loss: 0.957] [G acc: 0.000]\n",
      "271 [D loss: (0.990)(R 0.933, F 1.048)] [D acc: (0.004)(0.008, 0.000)] [G loss: 0.795] [G acc: 0.023]\n",
      "272 [D loss: (0.784)(R 0.815, F 0.754)] [D acc: (0.137)(0.141, 0.133)] [G loss: 0.794] [G acc: 0.023]\n",
      "273 [D loss: (0.734)(R 0.620, F 0.848)] [D acc: (0.457)(0.852, 0.062)] [G loss: 0.695] [G acc: 0.461]\n",
      "274 [D loss: (0.711)(R 0.611, F 0.811)] [D acc: (0.445)(0.812, 0.078)] [G loss: 0.674] [G acc: 0.617]\n",
      "275 [D loss: (0.706)(R 0.612, F 0.801)] [D acc: (0.473)(0.867, 0.078)] [G loss: 0.684] [G acc: 0.602]\n",
      "276 [D loss: (0.730)(R 0.616, F 0.845)] [D acc: (0.453)(0.891, 0.016)] [G loss: 0.706] [G acc: 0.461]\n",
      "277 [D loss: (0.728)(R 0.637, F 0.820)] [D acc: (0.391)(0.766, 0.016)] [G loss: 0.683] [G acc: 0.562]\n",
      "278 [D loss: (0.723)(R 0.615, F 0.830)] [D acc: (0.461)(0.867, 0.055)] [G loss: 0.680] [G acc: 0.570]\n",
      "279 [D loss: (0.705)(R 0.628, F 0.782)] [D acc: (0.414)(0.773, 0.055)] [G loss: 0.695] [G acc: 0.445]\n",
      "280 [D loss: (0.731)(R 0.635, F 0.827)] [D acc: (0.375)(0.703, 0.047)] [G loss: 0.690] [G acc: 0.562]\n",
      "281 [D loss: (0.713)(R 0.614, F 0.812)] [D acc: (0.520)(0.930, 0.109)] [G loss: 0.681] [G acc: 0.594]\n",
      "282 [D loss: (0.717)(R 0.622, F 0.813)] [D acc: (0.461)(0.836, 0.086)] [G loss: 0.709] [G acc: 0.406]\n",
      "283 [D loss: (0.715)(R 0.616, F 0.815)] [D acc: (0.477)(0.883, 0.070)] [G loss: 0.691] [G acc: 0.500]\n",
      "284 [D loss: (0.697)(R 0.604, F 0.789)] [D acc: (0.496)(0.875, 0.117)] [G loss: 0.687] [G acc: 0.562]\n",
      "285 [D loss: (0.700)(R 0.606, F 0.794)] [D acc: (0.496)(0.914, 0.078)] [G loss: 0.696] [G acc: 0.508]\n",
      "286 [D loss: (0.698)(R 0.615, F 0.781)] [D acc: (0.480)(0.883, 0.078)] [G loss: 0.696] [G acc: 0.453]\n",
      "287 [D loss: (0.711)(R 0.610, F 0.813)] [D acc: (0.445)(0.852, 0.039)] [G loss: 0.677] [G acc: 0.609]\n",
      "288 [D loss: (0.698)(R 0.622, F 0.773)] [D acc: (0.461)(0.820, 0.102)] [G loss: 0.674] [G acc: 0.594]\n",
      "289 [D loss: (0.707)(R 0.618, F 0.797)] [D acc: (0.457)(0.844, 0.070)] [G loss: 0.728] [G acc: 0.281]\n",
      "290 [D loss: (0.740)(R 0.669, F 0.810)] [D acc: (0.328)(0.609, 0.047)] [G loss: 0.724] [G acc: 0.273]\n",
      "291 [D loss: (0.746)(R 0.645, F 0.848)] [D acc: (0.371)(0.711, 0.031)] [G loss: 0.845] [G acc: 0.062]\n",
      "292 [D loss: (0.991)(R 0.771, F 1.211)] [D acc: (0.160)(0.320, 0.000)] [G loss: 1.358] [G acc: 0.000]\n",
      "293 [D loss: (1.045)(R 1.470, F 0.621)] [D acc: (0.465)(0.000, 0.930)] [G loss: 0.801] [G acc: 0.148]\n",
      "294 [D loss: (0.856)(R 0.341, F 1.371)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.601] [G acc: 0.883]\n",
      "295 [D loss: (0.712)(R 0.495, F 0.928)] [D acc: (0.484)(0.953, 0.016)] [G loss: 0.596] [G acc: 0.906]\n",
      "296 [D loss: (0.750)(R 0.494, F 1.006)] [D acc: (0.484)(0.969, 0.000)] [G loss: 0.627] [G acc: 0.836]\n",
      "297 [D loss: (0.734)(R 0.533, F 0.935)] [D acc: (0.492)(0.984, 0.000)] [G loss: 0.696] [G acc: 0.469]\n",
      "298 [D loss: (0.709)(R 0.575, F 0.844)] [D acc: (0.473)(0.914, 0.031)] [G loss: 0.723] [G acc: 0.383]\n",
      "299 [D loss: (0.723)(R 0.587, F 0.859)] [D acc: (0.465)(0.859, 0.070)] [G loss: 0.670] [G acc: 0.578]\n",
      "300 [D loss: (0.696)(R 0.552, F 0.841)] [D acc: (0.500)(0.914, 0.086)] [G loss: 0.669] [G acc: 0.594]\n",
      "301 [D loss: (0.709)(R 0.562, F 0.857)] [D acc: (0.488)(0.898, 0.078)] [G loss: 0.704] [G acc: 0.430]\n",
      "302 [D loss: (0.736)(R 0.612, F 0.861)] [D acc: (0.453)(0.820, 0.086)] [G loss: 0.691] [G acc: 0.500]\n",
      "303 [D loss: (0.720)(R 0.595, F 0.845)] [D acc: (0.465)(0.875, 0.055)] [G loss: 0.723] [G acc: 0.305]\n",
      "304 [D loss: (0.733)(R 0.613, F 0.853)] [D acc: (0.410)(0.727, 0.094)] [G loss: 0.701] [G acc: 0.469]\n",
      "305 [D loss: (0.723)(R 0.618, F 0.829)] [D acc: (0.469)(0.812, 0.125)] [G loss: 0.726] [G acc: 0.289]\n",
      "306 [D loss: (0.746)(R 0.640, F 0.852)] [D acc: (0.410)(0.734, 0.086)] [G loss: 0.727] [G acc: 0.336]\n",
      "307 [D loss: (0.750)(R 0.634, F 0.867)] [D acc: (0.371)(0.711, 0.031)] [G loss: 0.742] [G acc: 0.289]\n",
      "308 [D loss: (0.691)(R 0.623, F 0.759)] [D acc: (0.496)(0.719, 0.273)] [G loss: 0.783] [G acc: 0.195]\n",
      "309 [D loss: (0.774)(R 0.670, F 0.878)] [D acc: (0.324)(0.570, 0.078)] [G loss: 0.904] [G acc: 0.102]\n",
      "310 [D loss: (0.912)(R 0.787, F 1.037)] [D acc: (0.133)(0.250, 0.016)] [G loss: 0.830] [G acc: 0.117]\n",
      "311 [D loss: (1.314)(R 0.757, F 1.871)] [D acc: (0.199)(0.398, 0.000)] [G loss: 3.541] [G acc: 0.000]\n",
      "312 [D loss: (8.446)(R 3.953, F 12.938)] [D acc: (0.000)(0.000, 0.000)] [G loss: 12.669] [G acc: 0.000]\n",
      "313 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 7.192] [G acc: 0.000]\n",
      "314 [D loss: (8.061)(R 16.118, F 0.003)] [D acc: (0.500)(0.000, 1.000)] [G loss: 5.057] [G acc: 0.000]\n",
      "315 [D loss: (8.068)(R 16.118, F 0.017)] [D acc: (0.500)(0.000, 1.000)] [G loss: 4.011] [G acc: 0.000]\n",
      "316 [D loss: (8.069)(R 16.118, F 0.021)] [D acc: (0.500)(0.000, 1.000)] [G loss: 3.341] [G acc: 0.000]\n",
      "317 [D loss: (8.082)(R 16.118, F 0.046)] [D acc: (0.500)(0.000, 1.000)] [G loss: 3.036] [G acc: 0.000]\n",
      "318 [D loss: (8.114)(R 16.118, F 0.110)] [D acc: (0.500)(0.000, 1.000)] [G loss: 5.411] [G acc: 0.000]\n",
      "319 [D loss: (8.084)(R 16.118, F 0.049)] [D acc: (0.500)(0.000, 1.000)] [G loss: 4.672] [G acc: 0.000]\n",
      "320 [D loss: (8.071)(R 16.118, F 0.023)] [D acc: (0.500)(0.000, 1.000)] [G loss: 6.307] [G acc: 0.000]\n",
      "321 [D loss: (8.085)(R 16.118, F 0.052)] [D acc: (0.500)(0.000, 1.000)] [G loss: 4.925] [G acc: 0.000]\n",
      "322 [D loss: (8.062)(R 16.118, F 0.006)] [D acc: (0.500)(0.000, 1.000)] [G loss: 7.832] [G acc: 0.000]\n",
      "323 [D loss: (8.069)(R 16.118, F 0.020)] [D acc: (0.500)(0.000, 1.000)] [G loss: 8.213] [G acc: 0.000]\n",
      "324 [D loss: (8.065)(R 16.118, F 0.012)] [D acc: (0.500)(0.000, 1.000)] [G loss: 6.570] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325 [D loss: (8.062)(R 16.118, F 0.006)] [D acc: (0.500)(0.000, 1.000)] [G loss: 7.559] [G acc: 0.000]\n",
      "326 [D loss: (8.059)(R 16.118, F 0.001)] [D acc: (0.500)(0.000, 1.000)] [G loss: 9.768] [G acc: 0.000]\n",
      "327 [D loss: (8.065)(R 16.118, F 0.011)] [D acc: (0.500)(0.000, 1.000)] [G loss: 6.577] [G acc: 0.000]\n",
      "328 [D loss: (8.060)(R 16.118, F 0.001)] [D acc: (0.500)(0.000, 1.000)] [G loss: 4.269] [G acc: 0.000]\n",
      "329 [D loss: (8.062)(R 16.118, F 0.006)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.739] [G acc: 0.000]\n",
      "330 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 9.689] [G acc: 0.000]\n",
      "331 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 8.826] [G acc: 0.000]\n",
      "332 [D loss: (8.060)(R 16.118, F 0.002)] [D acc: (0.500)(0.000, 1.000)] [G loss: 7.129] [G acc: 0.000]\n",
      "333 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 10.633] [G acc: 0.000]\n",
      "334 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 9.878] [G acc: 0.000]\n",
      "335 [D loss: (8.060)(R 16.118, F 0.001)] [D acc: (0.500)(0.000, 1.000)] [G loss: 8.219] [G acc: 0.000]\n",
      "336 [D loss: (8.060)(R 16.118, F 0.002)] [D acc: (0.500)(0.000, 1.000)] [G loss: 7.785] [G acc: 0.000]\n",
      "337 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.092] [G acc: 0.000]\n",
      "338 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 10.699] [G acc: 0.000]\n",
      "339 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 10.187] [G acc: 0.000]\n",
      "340 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 9.214] [G acc: 0.000]\n",
      "341 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 9.125] [G acc: 0.000]\n",
      "342 [D loss: (8.059)(R 16.118, F 0.001)] [D acc: (0.500)(0.000, 1.000)] [G loss: 10.296] [G acc: 0.000]\n",
      "343 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.759] [G acc: 0.000]\n",
      "344 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.499] [G acc: 0.000]\n",
      "345 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.474] [G acc: 0.000]\n",
      "346 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.664] [G acc: 0.000]\n",
      "347 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.856] [G acc: 0.000]\n",
      "348 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.049] [G acc: 0.000]\n",
      "349 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.358] [G acc: 0.000]\n",
      "350 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.639] [G acc: 0.000]\n",
      "351 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.410] [G acc: 0.000]\n",
      "352 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.311] [G acc: 0.000]\n",
      "353 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.897] [G acc: 0.000]\n",
      "354 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 12.069] [G acc: 0.000]\n",
      "355 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.893] [G acc: 0.000]\n",
      "356 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 12.284] [G acc: 0.000]\n",
      "357 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 12.888] [G acc: 0.000]\n",
      "358 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.012] [G acc: 0.000]\n",
      "359 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 12.935] [G acc: 0.000]\n",
      "360 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.201] [G acc: 0.000]\n",
      "361 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.528] [G acc: 0.000]\n",
      "362 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.520] [G acc: 0.000]\n",
      "363 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.561] [G acc: 0.000]\n",
      "364 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.071] [G acc: 0.000]\n",
      "365 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.648] [G acc: 0.000]\n",
      "366 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.895] [G acc: 0.000]\n",
      "367 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.019] [G acc: 0.000]\n",
      "368 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.878] [G acc: 0.000]\n",
      "369 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.001] [G acc: 0.000]\n",
      "370 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.268] [G acc: 0.000]\n",
      "371 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.389] [G acc: 0.000]\n",
      "372 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.300] [G acc: 0.000]\n",
      "373 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.711] [G acc: 0.000]\n",
      "374 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.645] [G acc: 0.000]\n",
      "375 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.732] [G acc: 0.000]\n",
      "376 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.055] [G acc: 0.000]\n",
      "377 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.863] [G acc: 0.000]\n",
      "378 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.067] [G acc: 0.000]\n",
      "379 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.039] [G acc: 0.000]\n",
      "380 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.302] [G acc: 0.000]\n",
      "381 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.364] [G acc: 0.000]\n",
      "382 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.512] [G acc: 0.000]\n",
      "383 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.418] [G acc: 0.000]\n",
      "384 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.499] [G acc: 0.000]\n",
      "385 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.653] [G acc: 0.000]\n",
      "386 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.799] [G acc: 0.000]\n",
      "387 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.867] [G acc: 0.000]\n",
      "388 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.893] [G acc: 0.000]\n",
      "389 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.976] [G acc: 0.000]\n",
      "390 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.039] [G acc: 0.000]\n",
      "391 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "392 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "393 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n"
     ]
    }
   ],
   "source": [
    "gan.train(     \n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = 20000\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot([x[3] for x in gan.d_losses], color='green', linewidth=0.5)\n",
    "plt.plot([x[4] for x in gan.d_losses], color='red', linewidth=0.5)\n",
    "\n",
    "plt.plot([x[1] for x in gan.g_losses], color='orange', linewidth=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
