{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.GAN import GAN\n",
    "from utils.loaders import load_safari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = '0035'\n",
    "RUN_FOLDER = os.path.join(\"./run\", RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = load_safari('camel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13f42da20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAD3BJREFUeJzt3X2sFGWWx/HfkYH4AooCEnREZtGMIWJEry/J4oZ1l9Elo0g0BjQG0YgxmDhRwxpXI2Fj1I3DRolimIjDLOBgAighZpkRJosYHAVU3lwGhYtALiDxZSARETj7xy02d5B66tJv1XC+n+Tmdtfpp+rQ+rtV3dVdj7m7AMRzStkNACgH4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/ENRPGrkxM+PjhECdubt15nFV7fnN7EYz22hmn5nZY9WsC0BjWaWf7TezLpL+Imm4pO2SPpQ0xt03JMaw5wfqrBF7/qslfebum939gKTfSxpZxfoANFA14T9f0rYO97dny/6GmY03s5VmtrKKbQGosbq/4efu0yVNlzjsB5pJNXv+HZIu6HD/p9kyACeAasL/oaSLzexnZtZN0mhJC2vTFoB6q/iw390PmtmDkhZL6iJphruvr1lnAOqq4lN9FW2M1/xA3TXkQz4ATlyEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFXxFN2SZGatkvZKOiTpoLu31KIpAPVXVfgz/+jue2qwHgANxGE/EFS14XdJfzCzVWY2vhYNAWiMag/7h7r7DjM7V9Ifzex/3X1ZxwdkfxT4wwA0GXP32qzIbJKkfe7+fOIxtdkYgFzubp15XMWH/WZ2hpn1OHJb0i8krat0fQAaq5rD/r6SFpjZkfXMcff/rklXAOquZof9ndpYHQ/7Bw4cmKzPmTMnWe/evXuyvmnTptzayy+/nBy7dOnSZP3gwYPJOnA86n7YD+DERviBoAg/EBThB4Ii/EBQhB8IqqlO9XXt2jU5/tVXX82t3XHHHcmxXbp0Sdbrae7cucn66NGjG9QJIuBUH4Akwg8ERfiBoAg/EBThB4Ii/EBQhB8IqqnO8995553J8bNmzcqtPffcc8mxzz+fe4EhSdKePekLEPfo0aOiviTp5ptvTtavu+66ZH358uXJenZNhWO68MILk2PPPPPMZH3NmjXJ+umnn56s9+rVK7fW1taWHMtXnSvDeX4ASYQfCIrwA0ERfiAowg8ERfiBoAg/EFQtZumtmXPPPbfisZMmTUrW9+/fX/G6JWnv3r25tfXr1yfHFp3nHzx4cLJedJ4/dZ2DcePGJccWSV2yXCr+HEG3bt1ya4cOHUqO3bJlS7I+ZcqUZP2VV17JrTXy8y3Nij0/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRVeJ7fzGZI+qWk3e5+abbsHElzJQ2Q1Crpdnf/utpmtm3bVvHYadOmJesPPPBAsl7N5wDWrl1b8VhJGjRoULJ+zTXXJOt33313bi11rluStm7dmqw/88wzyfqbb76ZrM+fPz+31r9//+TYouscFE2Nfsop+fu2l156KTk2gs7s+X8r6cajlj0maYm7XyxpSXYfwAmkMPzuvkzSV0ctHilpZnZ7pqRbatwXgDqr9DV/X3c/cg2mnZL61qgfAA1S9Wf73d1T1+Yzs/GSxle7HQC1Vemef5eZ9ZOk7PfuvAe6+3R3b3H3lgq3BaAOKg3/Qkljs9tjJb1Vm3YANEph+M3sdUkrJP3czLab2b2SnpU03Mw2Sfrn7D6AE0hTXbc/df15SZo4cWJu7emnn06O/e6775L1AwcOJOv79u3LrfXp0yc5tmvXrsn6u+++m6yfdtppyfp5552XW7vkkkuSY4uu279z585k/aGHHkrWX3zxxWS9GkuXLk3We/bsmVu74oorat1O0+C6/QCSCD8QFOEHgiL8QFCEHwiK8ANBNdWlu4tOO6am4V6xYkVy7IgRI5L1U089NVm/8sorc2tFX02dOXNmsn7XXXcl66mvpkrSmDFjcmtFpziLLp/9zTffJOu7du1K1utp1apVyfp9993XoE5OTOz5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCopjrPX41ly5ZVVS9y//3359aGDh2aHPvJJ58k62PHjk3W33///WR97ty5yXpK0VeZ+/ZNX56xaHw9bd68OVk/66yzcmvvvPNOcmzRZ07WrVuXrBdNGf/tt98m643Anh8IivADQRF+ICjCDwRF+IGgCD8QFOEHgjppzvPX20cffVTx2Keeeqqqbc+ePTtZr+fl18s8j1+k6Dx/Supy55LU1taWrE+YMCFZv+2225L11DUYli9fnhxbK+z5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCowvP8ZjZD0i8l7Xb3S7NlkyTdJ+nL7GGPu/vb9WqyGXzwwQe5tbffTv/Ti+YMKNKjR4+qxlcj9Z14SbrnnnuS9alTp+bWDh48WFFPR2zcuDFZT805UPScbtiwIVm/9dZbk/UXXnghWZ88eXJu7frrr0+OrZXO7Pl/K+nGYyz/T3e/PPs5qYMPnIwKw+/uyyR91YBeADRQNa/5HzSzNWY2w8zOrllHABqi0vBPkzRQ0uWS2iT9Ou+BZjbezFaa2coKtwWgDioKv7vvcvdD7n5Y0m8kXZ147HR3b3H3lkqbBFB7FYXfzPp1uDtKUvpSpgCaTmdO9b0uaZik3ma2XdJTkoaZ2eWSXFKrpPzrWgNoSlbP74L/aGNmjdtYE5k/f36yPmrUqGS96BrvqXPOK1asSI79/vvvk/WbbropWV+wYEGyPmzYsNxat27dkmOHDBmSrD/xxBPJepcuXXJrRf9Niv7dP/zwQ7Je9J38G264IbfWs2fP5Niiayy4uyUfkOETfkBQhB8IivADQRF+ICjCDwRF+IGguHR3A6xevTpZLzqttHXr1mR98eLFubX169cnx1500UXJ+rZt25L1IkuWLMmtpU7FdcYbb7yRrD/66KO5taJ/V//+/ZP19957L1kfOXJksm6WfzbuqquuqmrbncWeHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeC4jx/AxRd/nr//v3J+vDhw5P1LVu25NYuu+yy5NgiAwcOrGp8Nefyiy4L/tprr1W87iJffPFFsj5nzpxkfeLEiRVvu6UlfdErzvMDqArhB4Ii/EBQhB8IivADQRF+ICjCDwTFef4GKJrC+5FHHknWU99Ll9LfDZ83b15ybNFU0ytXpmdZu/baa5P1ahRddrxM9ext586ddVt3R+z5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCowim6zewCSb+T1FeSS5ru7i+Y2TmS5koaIKlV0u3u/nXBukJO0V1kxowZyfq4ceOS9dSUzUXfiZ81a1ayXuTzzz9P1lPXA0hdh6BorCQ1cnr5o/Xp0ydZb21tTdYPHz6cWxs8eHBV667lFN0HJT3i7oMkXStpgpkNkvSYpCXufrGkJdl9ACeIwvC7e5u7r85u75X0qaTzJY2UNDN72ExJt9SrSQC1d1yv+c1sgKQhkv4sqa+7t2WlnWp/WQDgBNHpz/abWXdJ8yT9yt3/2vHz5O7uea/nzWy8pPHVNgqgtjq15zezrmoP/mx3n58t3mVm/bJ6P0m7jzXW3ae7e4u7p69KCKChCsNv7bv4VyV96u5TOpQWShqb3R4r6a3atwegXjpz2P/3ku6StNbMPs6WPS7pWUlvmNm9krZKur0+LZ78Hn744WT966+TZ1CTU4AvWrQoObao3qtXr2R98uTJyfqTTz6ZW5s6dWpybJmn8op8+eWXyXrv3r2T9dTp2UOHDlXU0/EqDL+7L5eUd97wn2rbDoBG4RN+QFCEHwiK8ANBEX4gKMIPBEX4gaAKv9Jb043xlV6g7mr5lV4AJyHCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IqjD8ZnaBmf3JzDaY2XozeyhbPsnMdpjZx9nPiPq3C6BWCiftMLN+kvq5+2oz6yFplaRbJN0uaZ+7P9/pjTFpB1B3nZ204yedWFGbpLbs9l4z+1TS+dW1B6Bsx/Wa38wGSBoi6c/ZogfNbI2ZzTCzs3PGjDezlWa2sqpOAdRUp+fqM7Pukv5H0tPuPt/M+kraI8kl/bvaXxrcU7AODvuBOuvsYX+nwm9mXSUtkrTY3accoz5A0iJ3v7RgPYQfqLOaTdRpZibpVUmfdgx+9kbgEaMkrTveJgGUpzPv9g+V9K6ktZIOZ4sflzRG0uVqP+xvlXR/9uZgal3s+YE6q+lhf60QfqD+anbYD+DkRPiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiq8AKeNbZH0tYO93tny5pRs/bWrH1J9FapWvZ2YWcf2NDv8/9o42Yr3b2ltAYSmrW3Zu1LordKldUbh/1AUIQfCKrs8E8vefspzdpbs/Yl0VulSumt1Nf8AMpT9p4fQElKCb+Z3WhmG83sMzN7rIwe8phZq5mtzWYeLnWKsWwatN1mtq7DsnPM7I9mtin7fcxp0krqrSlmbk7MLF3qc9dsM143/LDfzLpI+ouk4ZK2S/pQ0hh339DQRnKYWaukFncv/Zywmf2DpH2SfndkNiQz+w9JX7n7s9kfzrPd/V+bpLdJOs6Zm+vUW97M0nerxOeuljNe10IZe/6rJX3m7pvd/YCk30saWUIfTc/dl0n66qjFIyXNzG7PVPv/PA2X01tTcPc2d1+d3d4r6cjM0qU+d4m+SlFG+M+XtK3D/e1qrim/XdIfzGyVmY0vu5lj6NthZqSdkvqW2cwxFM7c3EhHzSzdNM9dJTNe1xpv+P3YUHe/QtK/SJqQHd42JW9/zdZMp2umSRqo9mnc2iT9usxmspml50n6lbv/tWOtzOfuGH2V8ryVEf4dki7ocP+n2bKm4O47st+7JS1Q+8uUZrLryCSp2e/dJffz/9x9l7sfcvfDkn6jEp+7bGbpeZJmu/v8bHHpz92x+irreSsj/B9KutjMfmZm3SSNlrSwhD5+xMzOyN6IkZmdIekXar7ZhxdKGpvdHivprRJ7+RvNMnNz3szSKvm5a7oZr9294T+SRqj9Hf/PJf1bGT3k9PV3kj7JftaX3Zuk19V+GPiD2t8buVdSL0lLJG2S9I6kc5qot/9S+2zOa9QetH4l9TZU7Yf0ayR9nP2MKPu5S/RVyvPGJ/yAoHjDDwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUP8HRD4Gf/ZvhpYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[200,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(input_dim = (28,28,1)\n",
    "        , discriminator_conv_filters = [64,64,128,128]\n",
    "        , discriminator_conv_kernel_size = [5,5,5,5]\n",
    "        , discriminator_conv_strides = [2,2,2,1]\n",
    "        , discriminator_conv_padding = 'same'\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'relu'\n",
    "        , discriminator_dropout_rate = None\n",
    "        , discriminator_learning_rate = 0.0008\n",
    "        , generator_initial_dense_layer_size = (7, 7, 64)\n",
    "        , generator_upsample = [2,2, 1, 1]\n",
    "        , generator_conv_filters = [128,64, 64,1]\n",
    "        , generator_conv_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_strides = [1,1, 1, 1]\n",
    "        , generator_conv_padding = 'same'\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0004\n",
    "        , optimiser = 'rmsprop'\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "gan.save(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 720,833\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_0 (Conv2DTran (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_1 (Conv2DTran (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_2 (Conv2DTran (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_3 (Conv2DTran (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 844,161\n",
      "Trainable params: 837,377\n",
      "Non-trainable params: 6,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 6000\n",
    "PRINT_EVERY_N_BATCHES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfoster/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (0.728)(R 0.690, F 0.767)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.676] [G acc: 1.000]\n",
      "1 [D loss: (1.750)(R 0.642, F 2.858)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.671] [G acc: 1.000]\n",
      "2 [D loss: (0.692)(R 0.668, F 0.716)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.661] [G acc: 1.000]\n",
      "3 [D loss: (0.705)(R 0.662, F 0.748)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.664] [G acc: 1.000]\n",
      "4 [D loss: (0.699)(R 0.666, F 0.733)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.666] [G acc: 1.000]\n",
      "5 [D loss: (0.699)(R 0.668, F 0.731)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.670] [G acc: 1.000]\n",
      "6 [D loss: (0.699)(R 0.670, F 0.727)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.673] [G acc: 1.000]\n",
      "7 [D loss: (0.698)(R 0.672, F 0.724)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.675] [G acc: 1.000]\n",
      "8 [D loss: (0.698)(R 0.674, F 0.722)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.677] [G acc: 1.000]\n",
      "9 [D loss: (0.697)(R 0.676, F 0.718)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.680] [G acc: 1.000]\n",
      "10 [D loss: (0.696)(R 0.677, F 0.715)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.683] [G acc: 1.000]\n",
      "11 [D loss: (0.699)(R 0.677, F 0.721)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.683] [G acc: 1.000]\n",
      "12 [D loss: (0.695)(R 0.679, F 0.711)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.685] [G acc: 1.000]\n",
      "13 [D loss: (0.697)(R 0.679, F 0.716)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.686] [G acc: 0.992]\n",
      "14 [D loss: (0.694)(R 0.682, F 0.707)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.687] [G acc: 1.000]\n",
      "15 [D loss: (0.698)(R 0.681, F 0.715)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.685] [G acc: 1.000]\n",
      "16 [D loss: (0.700)(R 0.684, F 0.717)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.689] [G acc: 0.969]\n",
      "17 [D loss: (0.694)(R 0.685, F 0.703)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.688] [G acc: 1.000]\n",
      "18 [D loss: (0.695)(R 0.686, F 0.705)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.694] [G acc: 0.414]\n",
      "19 [D loss: (0.694)(R 0.688, F 0.701)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.691] [G acc: 0.930]\n",
      "20 [D loss: (0.693)(R 0.687, F 0.699)] [D acc: (0.500)(0.992, 0.008)] [G loss: 0.693] [G acc: 0.555]\n",
      "21 [D loss: (0.696)(R 0.688, F 0.703)] [D acc: (0.484)(0.969, 0.000)] [G loss: 0.692] [G acc: 0.875]\n",
      "22 [D loss: (0.694)(R 0.688, F 0.699)] [D acc: (0.496)(0.984, 0.008)] [G loss: 0.693] [G acc: 0.625]\n",
      "23 [D loss: (0.694)(R 0.690, F 0.699)] [D acc: (0.473)(0.945, 0.000)] [G loss: 0.694] [G acc: 0.312]\n",
      "24 [D loss: (0.694)(R 0.689, F 0.698)] [D acc: (0.488)(0.945, 0.031)] [G loss: 0.694] [G acc: 0.453]\n",
      "25 [D loss: (0.695)(R 0.690, F 0.699)] [D acc: (0.438)(0.875, 0.000)] [G loss: 0.695] [G acc: 0.125]\n",
      "26 [D loss: (0.694)(R 0.691, F 0.698)] [D acc: (0.430)(0.820, 0.039)] [G loss: 0.693] [G acc: 0.414]\n",
      "27 [D loss: (0.694)(R 0.690, F 0.698)] [D acc: (0.461)(0.914, 0.008)] [G loss: 0.694] [G acc: 0.180]\n",
      "28 [D loss: (0.694)(R 0.690, F 0.698)] [D acc: (0.453)(0.875, 0.031)] [G loss: 0.694] [G acc: 0.266]\n",
      "29 [D loss: (0.694)(R 0.691, F 0.696)] [D acc: (0.453)(0.883, 0.023)] [G loss: 0.695] [G acc: 0.070]\n",
      "30 [D loss: (0.694)(R 0.691, F 0.696)] [D acc: (0.418)(0.781, 0.055)] [G loss: 0.694] [G acc: 0.242]\n",
      "31 [D loss: (0.694)(R 0.692, F 0.696)] [D acc: (0.469)(0.875, 0.062)] [G loss: 0.694] [G acc: 0.219]\n",
      "32 [D loss: (0.693)(R 0.691, F 0.695)] [D acc: (0.484)(0.867, 0.102)] [G loss: 0.694] [G acc: 0.281]\n",
      "33 [D loss: (0.707)(R 0.691, F 0.723)] [D acc: (0.418)(0.836, 0.000)] [G loss: 0.694] [G acc: 0.398]\n",
      "34 [D loss: (0.695)(R 0.692, F 0.698)] [D acc: (0.316)(0.594, 0.039)] [G loss: 0.695] [G acc: 0.188]\n",
      "35 [D loss: (0.698)(R 0.690, F 0.706)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.693] [G acc: 0.883]\n",
      "36 [D loss: (0.694)(R 0.691, F 0.696)] [D acc: (0.465)(0.930, 0.000)] [G loss: 0.693] [G acc: 0.789]\n",
      "37 [D loss: (0.694)(R 0.692, F 0.696)] [D acc: (0.434)(0.867, 0.000)] [G loss: 0.693] [G acc: 0.773]\n",
      "38 [D loss: (0.694)(R 0.692, F 0.696)] [D acc: (0.465)(0.930, 0.000)] [G loss: 0.693] [G acc: 0.500]\n",
      "39 [D loss: (0.694)(R 0.692, F 0.696)] [D acc: (0.426)(0.852, 0.000)] [G loss: 0.693] [G acc: 0.617]\n",
      "40 [D loss: (0.693)(R 0.692, F 0.695)] [D acc: (0.453)(0.898, 0.008)] [G loss: 0.693] [G acc: 0.383]\n",
      "41 [D loss: (0.693)(R 0.692, F 0.695)] [D acc: (0.438)(0.875, 0.000)] [G loss: 0.694] [G acc: 0.219]\n",
      "42 [D loss: (0.694)(R 0.692, F 0.695)] [D acc: (0.379)(0.758, 0.000)] [G loss: 0.693] [G acc: 0.344]\n",
      "43 [D loss: (0.694)(R 0.692, F 0.695)] [D acc: (0.426)(0.844, 0.008)] [G loss: 0.694] [G acc: 0.125]\n",
      "44 [D loss: (0.693)(R 0.692, F 0.695)] [D acc: (0.398)(0.781, 0.016)] [G loss: 0.693] [G acc: 0.141]\n",
      "45 [D loss: (0.693)(R 0.692, F 0.694)] [D acc: (0.438)(0.805, 0.070)] [G loss: 0.694] [G acc: 0.055]\n",
      "46 [D loss: (0.693)(R 0.692, F 0.695)] [D acc: (0.434)(0.852, 0.016)] [G loss: 0.694] [G acc: 0.039]\n",
      "47 [D loss: (0.693)(R 0.693, F 0.694)] [D acc: (0.402)(0.758, 0.047)] [G loss: 0.694] [G acc: 0.078]\n",
      "48 [D loss: (0.694)(R 0.691, F 0.697)] [D acc: (0.461)(0.922, 0.000)] [G loss: 0.701] [G acc: 0.000]\n",
      "49 [D loss: (0.697)(R 0.701, F 0.694)] [D acc: (0.027)(0.000, 0.055)] [G loss: 0.694] [G acc: 0.008]\n",
      "50 [D loss: (0.693)(R 0.692, F 0.695)] [D acc: (0.449)(0.773, 0.125)] [G loss: 0.694] [G acc: 0.039]\n",
      "51 [D loss: (0.695)(R 0.689, F 0.702)] [D acc: (0.512)(0.906, 0.117)] [G loss: 0.700] [G acc: 0.000]\n",
      "52 [D loss: (0.697)(R 0.697, F 0.697)] [D acc: (0.113)(0.203, 0.023)] [G loss: 0.697] [G acc: 0.000]\n",
      "53 [D loss: (0.698)(R 0.691, F 0.706)] [D acc: (0.359)(0.719, 0.000)] [G loss: 0.705] [G acc: 0.039]\n",
      "54 [D loss: (0.691)(R 0.682, F 0.700)] [D acc: (0.473)(0.930, 0.016)] [G loss: 0.709] [G acc: 0.016]\n",
      "55 [D loss: (0.781)(R 0.681, F 0.882)] [D acc: (0.355)(0.711, 0.000)] [G loss: 0.782] [G acc: 0.000]\n",
      "56 [D loss: (0.750)(R 0.789, F 0.711)] [D acc: (0.000)(0.000, 0.000)] [G loss: 0.700] [G acc: 0.000]\n",
      "57 [D loss: (0.701)(R 0.694, F 0.707)] [D acc: (0.215)(0.430, 0.000)] [G loss: 0.693] [G acc: 0.383]\n",
      "58 [D loss: (0.696)(R 0.686, F 0.705)] [D acc: (0.477)(0.953, 0.000)] [G loss: 0.691] [G acc: 0.914]\n",
      "59 [D loss: (0.695)(R 0.685, F 0.706)] [D acc: (0.477)(0.945, 0.008)] [G loss: 0.691] [G acc: 0.953]\n",
      "60 [D loss: (0.695)(R 0.683, F 0.706)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.691] [G acc: 0.945]\n",
      "61 [D loss: (0.694)(R 0.683, F 0.705)] [D acc: (0.480)(0.961, 0.000)] [G loss: 0.690] [G acc: 0.984]\n",
      "62 [D loss: (0.695)(R 0.682, F 0.708)] [D acc: (0.477)(0.953, 0.000)] [G loss: 0.690] [G acc: 1.000]\n",
      "63 [D loss: (0.694)(R 0.684, F 0.705)] [D acc: (0.480)(0.961, 0.000)] [G loss: 0.690] [G acc: 0.984]\n",
      "64 [D loss: (0.695)(R 0.683, F 0.707)] [D acc: (0.492)(0.984, 0.000)] [G loss: 0.690] [G acc: 1.000]\n",
      "65 [D loss: (0.694)(R 0.686, F 0.703)] [D acc: (0.492)(0.984, 0.000)] [G loss: 0.690] [G acc: 0.977]\n",
      "66 [D loss: (0.694)(R 0.685, F 0.703)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.691] [G acc: 1.000]\n",
      "67 [D loss: (0.693)(R 0.686, F 0.701)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.691] [G acc: 0.930]\n",
      "68 [D loss: (0.694)(R 0.685, F 0.703)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.692] [G acc: 0.789]\n",
      "69 [D loss: (0.694)(R 0.686, F 0.702)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.692] [G acc: 0.719]\n",
      "70 [D loss: (0.693)(R 0.686, F 0.700)] [D acc: (0.492)(0.984, 0.000)] [G loss: 0.693] [G acc: 0.422]\n",
      "71 [D loss: (0.694)(R 0.686, F 0.702)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.693] [G acc: 0.469]\n",
      "72 [D loss: (0.694)(R 0.686, F 0.701)] [D acc: (0.488)(0.977, 0.000)] [G loss: 0.694] [G acc: 0.227]\n",
      "73 [D loss: (0.695)(R 0.687, F 0.702)] [D acc: (0.453)(0.906, 0.000)] [G loss: 0.693] [G acc: 0.469]\n",
      "74 [D loss: (0.692)(R 0.685, F 0.700)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.694] [G acc: 0.234]\n",
      "75 [D loss: (0.693)(R 0.686, F 0.700)] [D acc: (0.488)(0.961, 0.016)] [G loss: 0.695] [G acc: 0.211]\n",
      "76 [D loss: (0.694)(R 0.684, F 0.704)] [D acc: (0.500)(0.984, 0.016)] [G loss: 0.694] [G acc: 0.344]\n",
      "77 [D loss: (0.692)(R 0.685, F 0.699)] [D acc: (0.531)(0.984, 0.078)] [G loss: 0.696] [G acc: 0.062]\n",
      "78 [D loss: (0.694)(R 0.686, F 0.702)] [D acc: (0.441)(0.867, 0.016)] [G loss: 0.699] [G acc: 0.000]\n",
      "79 [D loss: (0.700)(R 0.680, F 0.720)] [D acc: (0.504)(1.000, 0.008)] [G loss: 0.702] [G acc: 0.008]\n",
      "80 [D loss: (0.696)(R 0.690, F 0.701)] [D acc: (0.344)(0.570, 0.117)] [G loss: 0.699] [G acc: 0.023]\n",
      "81 [D loss: (0.698)(R 0.685, F 0.711)] [D acc: (0.434)(0.859, 0.008)] [G loss: 0.695] [G acc: 0.102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 [D loss: (0.696)(R 0.685, F 0.707)] [D acc: (0.422)(0.844, 0.000)] [G loss: 0.694] [G acc: 0.281]\n",
      "83 [D loss: (0.692)(R 0.683, F 0.700)] [D acc: (0.500)(0.969, 0.031)] [G loss: 0.694] [G acc: 0.398]\n",
      "84 [D loss: (0.694)(R 0.681, F 0.706)] [D acc: (0.465)(0.914, 0.016)] [G loss: 0.693] [G acc: 0.406]\n",
      "85 [D loss: (0.693)(R 0.681, F 0.705)] [D acc: (0.480)(0.945, 0.016)] [G loss: 0.695] [G acc: 0.219]\n",
      "86 [D loss: (0.694)(R 0.680, F 0.708)] [D acc: (0.465)(0.930, 0.000)] [G loss: 0.694] [G acc: 0.289]\n",
      "87 [D loss: (0.693)(R 0.681, F 0.706)] [D acc: (0.484)(0.945, 0.023)] [G loss: 0.696] [G acc: 0.242]\n",
      "88 [D loss: (0.695)(R 0.682, F 0.709)] [D acc: (0.453)(0.891, 0.016)] [G loss: 0.695] [G acc: 0.180]\n",
      "89 [D loss: (0.690)(R 0.682, F 0.697)] [D acc: (0.562)(0.922, 0.203)] [G loss: 0.693] [G acc: 0.414]\n",
      "90 [D loss: (0.707)(R 0.672, F 0.741)] [D acc: (0.453)(0.898, 0.008)] [G loss: 0.735] [G acc: 0.000]\n",
      "91 [D loss: (0.703)(R 0.707, F 0.698)] [D acc: (0.066)(0.070, 0.062)] [G loss: 0.692] [G acc: 0.781]\n",
      "92 [D loss: (0.690)(R 0.685, F 0.695)] [D acc: (0.707)(0.984, 0.430)] [G loss: 0.692] [G acc: 0.547]\n",
      "93 [D loss: (0.699)(R 0.678, F 0.721)] [D acc: (0.453)(0.883, 0.023)] [G loss: 0.693] [G acc: 0.523]\n",
      "94 [D loss: (0.695)(R 0.684, F 0.706)] [D acc: (0.395)(0.773, 0.016)] [G loss: 0.691] [G acc: 0.641]\n",
      "95 [D loss: (0.700)(R 0.681, F 0.719)] [D acc: (0.414)(0.820, 0.008)] [G loss: 0.693] [G acc: 0.531]\n",
      "96 [D loss: (0.695)(R 0.683, F 0.706)] [D acc: (0.422)(0.766, 0.078)] [G loss: 0.689] [G acc: 0.773]\n",
      "97 [D loss: (0.699)(R 0.683, F 0.715)] [D acc: (0.434)(0.867, 0.000)] [G loss: 0.696] [G acc: 0.336]\n",
      "98 [D loss: (0.726)(R 0.688, F 0.764)] [D acc: (0.414)(0.828, 0.000)] [G loss: 0.703] [G acc: 0.062]\n",
      "99 [D loss: (0.689)(R 0.681, F 0.696)] [D acc: (0.625)(0.898, 0.352)] [G loss: 0.698] [G acc: 0.227]\n",
      "100 [D loss: (0.696)(R 0.678, F 0.714)] [D acc: (0.453)(0.789, 0.117)] [G loss: 0.776] [G acc: 0.000]\n",
      "101 [D loss: (0.713)(R 0.717, F 0.709)] [D acc: (0.102)(0.109, 0.094)] [G loss: 0.692] [G acc: 0.578]\n",
      "102 [D loss: (0.697)(R 0.677, F 0.717)] [D acc: (0.488)(0.938, 0.039)] [G loss: 0.693] [G acc: 0.422]\n",
      "103 [D loss: (0.694)(R 0.681, F 0.708)] [D acc: (0.457)(0.859, 0.055)] [G loss: 0.691] [G acc: 0.664]\n",
      "104 [D loss: (0.693)(R 0.677, F 0.708)] [D acc: (0.547)(0.969, 0.125)] [G loss: 0.691] [G acc: 0.539]\n",
      "105 [D loss: (0.702)(R 0.679, F 0.725)] [D acc: (0.375)(0.750, 0.000)] [G loss: 0.700] [G acc: 0.219]\n",
      "106 [D loss: (0.693)(R 0.682, F 0.704)] [D acc: (0.461)(0.742, 0.180)] [G loss: 0.685] [G acc: 0.766]\n",
      "107 [D loss: (0.706)(R 0.678, F 0.735)] [D acc: (0.426)(0.836, 0.016)] [G loss: 0.692] [G acc: 0.547]\n",
      "108 [D loss: (0.697)(R 0.685, F 0.710)] [D acc: (0.473)(0.844, 0.102)] [G loss: 0.687] [G acc: 0.797]\n",
      "109 [D loss: (0.701)(R 0.685, F 0.717)] [D acc: (0.410)(0.789, 0.031)] [G loss: 0.718] [G acc: 0.008]\n",
      "110 [D loss: (0.693)(R 0.696, F 0.690)] [D acc: (0.562)(0.359, 0.766)] [G loss: 0.705] [G acc: 0.109]\n",
      "111 [D loss: (0.699)(R 0.688, F 0.711)] [D acc: (0.441)(0.625, 0.258)] [G loss: 0.726] [G acc: 0.000]\n",
      "112 [D loss: (0.700)(R 0.701, F 0.700)] [D acc: (0.352)(0.320, 0.383)] [G loss: 0.697] [G acc: 0.328]\n",
      "113 [D loss: (0.700)(R 0.685, F 0.714)] [D acc: (0.402)(0.805, 0.000)] [G loss: 0.696] [G acc: 0.391]\n",
      "114 [D loss: (0.696)(R 0.687, F 0.705)] [D acc: (0.426)(0.812, 0.039)] [G loss: 0.689] [G acc: 0.805]\n",
      "115 [D loss: (0.695)(R 0.685, F 0.705)] [D acc: (0.461)(0.883, 0.039)] [G loss: 0.689] [G acc: 0.828]\n",
      "116 [D loss: (0.695)(R 0.685, F 0.704)] [D acc: (0.469)(0.906, 0.031)] [G loss: 0.690] [G acc: 0.750]\n",
      "117 [D loss: (0.696)(R 0.686, F 0.705)] [D acc: (0.449)(0.898, 0.000)] [G loss: 0.691] [G acc: 0.695]\n",
      "118 [D loss: (0.695)(R 0.686, F 0.703)] [D acc: (0.441)(0.852, 0.031)] [G loss: 0.691] [G acc: 0.656]\n",
      "119 [D loss: (0.695)(R 0.686, F 0.703)] [D acc: (0.449)(0.867, 0.031)] [G loss: 0.692] [G acc: 0.641]\n",
      "120 [D loss: (0.695)(R 0.687, F 0.702)] [D acc: (0.453)(0.859, 0.047)] [G loss: 0.692] [G acc: 0.648]\n",
      "121 [D loss: (0.693)(R 0.686, F 0.700)] [D acc: (0.426)(0.812, 0.039)] [G loss: 0.694] [G acc: 0.438]\n",
      "122 [D loss: (0.696)(R 0.685, F 0.707)] [D acc: (0.523)(0.852, 0.195)] [G loss: 0.693] [G acc: 0.484]\n",
      "123 [D loss: (0.695)(R 0.688, F 0.702)] [D acc: (0.469)(0.820, 0.117)] [G loss: 0.692] [G acc: 0.547]\n",
      "124 [D loss: (0.694)(R 0.688, F 0.701)] [D acc: (0.461)(0.797, 0.125)] [G loss: 0.693] [G acc: 0.484]\n",
      "125 [D loss: (0.693)(R 0.688, F 0.699)] [D acc: (0.488)(0.844, 0.133)] [G loss: 0.693] [G acc: 0.531]\n",
      "126 [D loss: (0.694)(R 0.688, F 0.699)] [D acc: (0.469)(0.766, 0.172)] [G loss: 0.693] [G acc: 0.445]\n",
      "127 [D loss: (0.696)(R 0.687, F 0.704)] [D acc: (0.492)(0.836, 0.148)] [G loss: 0.695] [G acc: 0.359]\n",
      "128 [D loss: (0.695)(R 0.689, F 0.701)] [D acc: (0.473)(0.797, 0.148)] [G loss: 0.697] [G acc: 0.234]\n",
      "129 [D loss: (0.693)(R 0.687, F 0.699)] [D acc: (0.441)(0.711, 0.172)] [G loss: 0.692] [G acc: 0.570]\n",
      "130 [D loss: (0.696)(R 0.688, F 0.704)] [D acc: (0.496)(0.859, 0.133)] [G loss: 0.693] [G acc: 0.477]\n",
      "131 [D loss: (0.696)(R 0.685, F 0.706)] [D acc: (0.469)(0.875, 0.062)] [G loss: 0.695] [G acc: 0.383]\n",
      "132 [D loss: (0.694)(R 0.688, F 0.699)] [D acc: (0.477)(0.789, 0.164)] [G loss: 0.693] [G acc: 0.445]\n",
      "133 [D loss: (0.694)(R 0.687, F 0.701)] [D acc: (0.500)(0.867, 0.133)] [G loss: 0.698] [G acc: 0.219]\n",
      "134 [D loss: (0.695)(R 0.688, F 0.702)] [D acc: (0.383)(0.727, 0.039)] [G loss: 0.692] [G acc: 0.555]\n",
      "135 [D loss: (0.695)(R 0.686, F 0.704)] [D acc: (0.477)(0.867, 0.086)] [G loss: 0.702] [G acc: 0.164]\n",
      "136 [D loss: (0.696)(R 0.690, F 0.702)] [D acc: (0.332)(0.594, 0.070)] [G loss: 0.693] [G acc: 0.453]\n",
      "137 [D loss: (0.711)(R 0.684, F 0.737)] [D acc: (0.449)(0.852, 0.047)] [G loss: 0.690] [G acc: 0.586]\n",
      "138 [D loss: (0.723)(R 0.678, F 0.767)] [D acc: (0.488)(0.914, 0.062)] [G loss: 0.691] [G acc: 0.641]\n",
      "139 [D loss: (0.697)(R 0.686, F 0.708)] [D acc: (0.410)(0.773, 0.047)] [G loss: 0.699] [G acc: 0.125]\n",
      "140 [D loss: (0.695)(R 0.689, F 0.701)] [D acc: (0.418)(0.766, 0.070)] [G loss: 0.692] [G acc: 0.641]\n",
      "141 [D loss: (0.693)(R 0.686, F 0.700)] [D acc: (0.488)(0.906, 0.070)] [G loss: 0.692] [G acc: 0.508]\n",
      "142 [D loss: (0.694)(R 0.685, F 0.704)] [D acc: (0.453)(0.844, 0.062)] [G loss: 0.691] [G acc: 0.688]\n",
      "143 [D loss: (0.697)(R 0.686, F 0.707)] [D acc: (0.414)(0.773, 0.055)] [G loss: 0.694] [G acc: 0.492]\n",
      "144 [D loss: (0.695)(R 0.686, F 0.703)] [D acc: (0.426)(0.805, 0.047)] [G loss: 0.692] [G acc: 0.586]\n",
      "145 [D loss: (0.694)(R 0.685, F 0.704)] [D acc: (0.461)(0.852, 0.070)] [G loss: 0.696] [G acc: 0.281]\n",
      "146 [D loss: (0.694)(R 0.688, F 0.699)] [D acc: (0.395)(0.695, 0.094)] [G loss: 0.694] [G acc: 0.398]\n",
      "147 [D loss: (0.695)(R 0.685, F 0.704)] [D acc: (0.453)(0.875, 0.031)] [G loss: 0.693] [G acc: 0.578]\n",
      "148 [D loss: (0.694)(R 0.684, F 0.704)] [D acc: (0.484)(0.922, 0.047)] [G loss: 0.691] [G acc: 0.719]\n",
      "149 [D loss: (0.695)(R 0.684, F 0.705)] [D acc: (0.477)(0.891, 0.062)] [G loss: 0.695] [G acc: 0.352]\n",
      "150 [D loss: (0.698)(R 0.686, F 0.710)] [D acc: (0.418)(0.766, 0.070)] [G loss: 0.694] [G acc: 0.461]\n",
      "151 [D loss: (0.698)(R 0.681, F 0.716)] [D acc: (0.480)(0.859, 0.102)] [G loss: 0.695] [G acc: 0.453]\n",
      "152 [D loss: (0.694)(R 0.683, F 0.704)] [D acc: (0.430)(0.758, 0.102)] [G loss: 0.696] [G acc: 0.281]\n",
      "153 [D loss: (0.702)(R 0.682, F 0.723)] [D acc: (0.434)(0.742, 0.125)] [G loss: 0.707] [G acc: 0.219]\n",
      "154 [D loss: (0.692)(R 0.689, F 0.696)] [D acc: (0.434)(0.555, 0.312)] [G loss: 0.716] [G acc: 0.000]\n",
      "155 [D loss: (0.725)(R 0.669, F 0.781)] [D acc: (0.457)(0.891, 0.023)] [G loss: 0.697] [G acc: 0.375]\n",
      "156 [D loss: (0.696)(R 0.681, F 0.712)] [D acc: (0.469)(0.688, 0.250)] [G loss: 0.730] [G acc: 0.023]\n",
      "157 [D loss: (0.702)(R 0.701, F 0.702)] [D acc: (0.383)(0.398, 0.367)] [G loss: 0.862] [G acc: 0.000]\n",
      "158 [D loss: (0.725)(R 0.727, F 0.723)] [D acc: (0.160)(0.266, 0.055)] [G loss: 0.694] [G acc: 0.391]\n",
      "159 [D loss: (0.697)(R 0.672, F 0.722)] [D acc: (0.453)(0.844, 0.062)] [G loss: 0.694] [G acc: 0.414]\n",
      "160 [D loss: (0.687)(R 0.666, F 0.709)] [D acc: (0.590)(0.930, 0.250)] [G loss: 0.700] [G acc: 0.297]\n",
      "161 [D loss: (0.693)(R 0.673, F 0.712)] [D acc: (0.441)(0.688, 0.195)] [G loss: 0.688] [G acc: 0.516]\n",
      "162 [D loss: (0.704)(R 0.654, F 0.753)] [D acc: (0.508)(0.883, 0.133)] [G loss: 0.687] [G acc: 0.555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 [D loss: (0.688)(R 0.664, F 0.711)] [D acc: (0.617)(0.875, 0.359)] [G loss: 0.682] [G acc: 0.633]\n",
      "164 [D loss: (0.705)(R 0.655, F 0.754)] [D acc: (0.488)(0.789, 0.188)] [G loss: 0.696] [G acc: 0.344]\n",
      "165 [D loss: (0.700)(R 0.665, F 0.734)] [D acc: (0.473)(0.773, 0.172)] [G loss: 0.686] [G acc: 0.633]\n",
      "166 [D loss: (0.699)(R 0.665, F 0.732)] [D acc: (0.504)(0.867, 0.141)] [G loss: 0.726] [G acc: 0.281]\n",
      "167 [D loss: (0.695)(R 0.683, F 0.706)] [D acc: (0.402)(0.648, 0.156)] [G loss: 0.696] [G acc: 0.312]\n",
      "168 [D loss: (0.696)(R 0.668, F 0.724)] [D acc: (0.477)(0.844, 0.109)] [G loss: 0.689] [G acc: 0.633]\n",
      "169 [D loss: (0.692)(R 0.670, F 0.714)] [D acc: (0.488)(0.852, 0.125)] [G loss: 0.687] [G acc: 0.664]\n",
      "170 [D loss: (0.698)(R 0.670, F 0.725)] [D acc: (0.469)(0.820, 0.117)] [G loss: 0.689] [G acc: 0.625]\n",
      "171 [D loss: (0.697)(R 0.676, F 0.718)] [D acc: (0.422)(0.758, 0.086)] [G loss: 0.687] [G acc: 0.617]\n",
      "172 [D loss: (0.698)(R 0.671, F 0.725)] [D acc: (0.434)(0.805, 0.062)] [G loss: 0.685] [G acc: 0.828]\n",
      "173 [D loss: (0.695)(R 0.673, F 0.717)] [D acc: (0.504)(0.922, 0.086)] [G loss: 0.687] [G acc: 0.727]\n",
      "174 [D loss: (0.696)(R 0.676, F 0.715)] [D acc: (0.465)(0.859, 0.070)] [G loss: 0.689] [G acc: 0.617]\n",
      "175 [D loss: (0.695)(R 0.675, F 0.715)] [D acc: (0.465)(0.898, 0.031)] [G loss: 0.688] [G acc: 0.703]\n",
      "176 [D loss: (0.694)(R 0.676, F 0.711)] [D acc: (0.551)(0.922, 0.180)] [G loss: 0.692] [G acc: 0.531]\n",
      "177 [D loss: (0.697)(R 0.680, F 0.715)] [D acc: (0.453)(0.812, 0.094)] [G loss: 0.692] [G acc: 0.523]\n",
      "178 [D loss: (0.695)(R 0.672, F 0.718)] [D acc: (0.477)(0.844, 0.109)] [G loss: 0.690] [G acc: 0.602]\n",
      "179 [D loss: (0.691)(R 0.674, F 0.708)] [D acc: (0.527)(0.867, 0.188)] [G loss: 0.693] [G acc: 0.469]\n",
      "180 [D loss: (0.691)(R 0.674, F 0.709)] [D acc: (0.520)(0.844, 0.195)] [G loss: 0.694] [G acc: 0.422]\n",
      "181 [D loss: (0.693)(R 0.674, F 0.713)] [D acc: (0.477)(0.758, 0.195)] [G loss: 0.689] [G acc: 0.633]\n",
      "182 [D loss: (0.696)(R 0.672, F 0.720)] [D acc: (0.488)(0.867, 0.109)] [G loss: 0.694] [G acc: 0.516]\n",
      "183 [D loss: (0.690)(R 0.669, F 0.712)] [D acc: (0.504)(0.859, 0.148)] [G loss: 0.692] [G acc: 0.531]\n",
      "184 [D loss: (0.692)(R 0.668, F 0.715)] [D acc: (0.477)(0.812, 0.141)] [G loss: 0.689] [G acc: 0.555]\n",
      "185 [D loss: (0.697)(R 0.668, F 0.725)] [D acc: (0.492)(0.883, 0.102)] [G loss: 0.696] [G acc: 0.445]\n",
      "186 [D loss: (0.716)(R 0.666, F 0.765)] [D acc: (0.387)(0.703, 0.070)] [G loss: 0.689] [G acc: 0.531]\n",
      "187 [D loss: (0.717)(R 0.662, F 0.773)] [D acc: (0.445)(0.852, 0.039)] [G loss: 0.759] [G acc: 0.055]\n",
      "188 [D loss: (0.722)(R 0.745, F 0.699)] [D acc: (0.277)(0.141, 0.414)] [G loss: 0.730] [G acc: 0.070]\n",
      "189 [D loss: (0.755)(R 0.626, F 0.885)] [D acc: (0.477)(0.953, 0.000)] [G loss: 0.701] [G acc: 0.414]\n",
      "190 [D loss: (0.694)(R 0.653, F 0.735)] [D acc: (0.473)(0.859, 0.086)] [G loss: 0.717] [G acc: 0.195]\n",
      "191 [D loss: (0.705)(R 0.666, F 0.743)] [D acc: (0.410)(0.734, 0.086)] [G loss: 0.751] [G acc: 0.133]\n",
      "192 [D loss: (0.708)(R 0.656, F 0.760)] [D acc: (0.406)(0.797, 0.016)] [G loss: 0.693] [G acc: 0.547]\n",
      "193 [D loss: (0.707)(R 0.654, F 0.759)] [D acc: (0.402)(0.766, 0.039)] [G loss: 0.697] [G acc: 0.438]\n",
      "194 [D loss: (0.694)(R 0.656, F 0.731)] [D acc: (0.457)(0.844, 0.070)] [G loss: 0.691] [G acc: 0.508]\n",
      "195 [D loss: (0.705)(R 0.650, F 0.759)] [D acc: (0.453)(0.852, 0.055)] [G loss: 0.676] [G acc: 0.617]\n",
      "196 [D loss: (0.696)(R 0.638, F 0.753)] [D acc: (0.488)(0.875, 0.102)] [G loss: 0.679] [G acc: 0.625]\n",
      "197 [D loss: (0.714)(R 0.660, F 0.768)] [D acc: (0.406)(0.766, 0.047)] [G loss: 0.691] [G acc: 0.531]\n",
      "198 [D loss: (0.691)(R 0.651, F 0.731)] [D acc: (0.488)(0.875, 0.102)] [G loss: 0.678] [G acc: 0.805]\n",
      "199 [D loss: (0.698)(R 0.652, F 0.744)] [D acc: (0.500)(0.930, 0.070)] [G loss: 0.681] [G acc: 0.734]\n",
      "200 [D loss: (0.696)(R 0.657, F 0.734)] [D acc: (0.473)(0.914, 0.031)] [G loss: 0.682] [G acc: 0.633]\n",
      "201 [D loss: (0.702)(R 0.628, F 0.777)] [D acc: (0.477)(0.875, 0.078)] [G loss: 0.679] [G acc: 0.656]\n",
      "202 [D loss: (0.700)(R 0.648, F 0.751)] [D acc: (0.449)(0.844, 0.055)] [G loss: 0.703] [G acc: 0.430]\n",
      "203 [D loss: (0.692)(R 0.655, F 0.728)] [D acc: (0.438)(0.812, 0.062)] [G loss: 0.698] [G acc: 0.398]\n",
      "204 [D loss: (0.898)(R 0.640, F 1.156)] [D acc: (0.445)(0.867, 0.023)] [G loss: 0.678] [G acc: 0.664]\n",
      "205 [D loss: (0.715)(R 0.652, F 0.778)] [D acc: (0.457)(0.875, 0.039)] [G loss: 0.679] [G acc: 0.672]\n",
      "206 [D loss: (0.688)(R 0.652, F 0.724)] [D acc: (0.527)(0.852, 0.203)] [G loss: 0.667] [G acc: 0.844]\n",
      "207 [D loss: (0.701)(R 0.644, F 0.758)] [D acc: (0.473)(0.891, 0.055)] [G loss: 0.678] [G acc: 0.727]\n",
      "208 [D loss: (0.694)(R 0.649, F 0.739)] [D acc: (0.473)(0.891, 0.055)] [G loss: 0.664] [G acc: 0.805]\n",
      "209 [D loss: (0.705)(R 0.643, F 0.767)] [D acc: (0.430)(0.820, 0.039)] [G loss: 0.734] [G acc: 0.133]\n",
      "210 [D loss: (0.707)(R 0.685, F 0.730)] [D acc: (0.320)(0.570, 0.070)] [G loss: 0.685] [G acc: 0.664]\n",
      "211 [D loss: (0.689)(R 0.625, F 0.754)] [D acc: (0.512)(0.969, 0.055)] [G loss: 0.675] [G acc: 0.766]\n",
      "212 [D loss: (0.699)(R 0.645, F 0.753)] [D acc: (0.477)(0.891, 0.062)] [G loss: 0.682] [G acc: 0.711]\n",
      "213 [D loss: (0.698)(R 0.649, F 0.748)] [D acc: (0.465)(0.898, 0.031)] [G loss: 0.682] [G acc: 0.703]\n",
      "214 [D loss: (0.694)(R 0.655, F 0.734)] [D acc: (0.453)(0.852, 0.055)] [G loss: 0.679] [G acc: 0.734]\n",
      "215 [D loss: (0.696)(R 0.640, F 0.752)] [D acc: (0.473)(0.914, 0.031)] [G loss: 0.673] [G acc: 0.805]\n",
      "216 [D loss: (0.713)(R 0.648, F 0.777)] [D acc: (0.453)(0.906, 0.000)] [G loss: 0.701] [G acc: 0.422]\n",
      "217 [D loss: (0.701)(R 0.663, F 0.738)] [D acc: (0.441)(0.836, 0.047)] [G loss: 0.681] [G acc: 0.742]\n",
      "218 [D loss: (0.700)(R 0.661, F 0.739)] [D acc: (0.477)(0.891, 0.062)] [G loss: 0.690] [G acc: 0.625]\n",
      "219 [D loss: (0.693)(R 0.654, F 0.731)] [D acc: (0.496)(0.930, 0.062)] [G loss: 0.681] [G acc: 0.750]\n",
      "220 [D loss: (0.696)(R 0.655, F 0.736)] [D acc: (0.480)(0.906, 0.055)] [G loss: 0.680] [G acc: 0.664]\n",
      "221 [D loss: (0.693)(R 0.651, F 0.736)] [D acc: (0.504)(0.938, 0.070)] [G loss: 0.682] [G acc: 0.695]\n",
      "222 [D loss: (0.698)(R 0.653, F 0.742)] [D acc: (0.516)(0.945, 0.086)] [G loss: 0.680] [G acc: 0.625]\n",
      "223 [D loss: (0.707)(R 0.658, F 0.757)] [D acc: (0.496)(0.898, 0.094)] [G loss: 0.715] [G acc: 0.336]\n",
      "224 [D loss: (0.685)(R 0.662, F 0.707)] [D acc: (0.547)(0.836, 0.258)] [G loss: 0.731] [G acc: 0.125]\n",
      "225 [D loss: (0.706)(R 0.674, F 0.738)] [D acc: (0.395)(0.664, 0.125)] [G loss: 0.794] [G acc: 0.039]\n",
      "226 [D loss: (0.727)(R 0.721, F 0.733)] [D acc: (0.199)(0.328, 0.070)] [G loss: 0.696] [G acc: 0.406]\n",
      "227 [D loss: (0.688)(R 0.638, F 0.738)] [D acc: (0.516)(0.906, 0.125)] [G loss: 0.691] [G acc: 0.516]\n",
      "228 [D loss: (0.695)(R 0.648, F 0.741)] [D acc: (0.520)(0.867, 0.172)] [G loss: 0.686] [G acc: 0.570]\n",
      "229 [D loss: (0.701)(R 0.633, F 0.769)] [D acc: (0.473)(0.883, 0.062)] [G loss: 0.676] [G acc: 0.641]\n",
      "230 [D loss: (0.706)(R 0.637, F 0.774)] [D acc: (0.461)(0.828, 0.094)] [G loss: 0.677] [G acc: 0.633]\n",
      "231 [D loss: (0.706)(R 0.623, F 0.789)] [D acc: (0.531)(0.906, 0.156)] [G loss: 0.702] [G acc: 0.500]\n",
      "232 [D loss: (0.702)(R 0.653, F 0.752)] [D acc: (0.449)(0.742, 0.156)] [G loss: 0.681] [G acc: 0.562]\n",
      "233 [D loss: (0.708)(R 0.632, F 0.783)] [D acc: (0.469)(0.867, 0.070)] [G loss: 0.691] [G acc: 0.477]\n",
      "234 [D loss: (0.701)(R 0.640, F 0.762)] [D acc: (0.504)(0.820, 0.188)] [G loss: 0.678] [G acc: 0.664]\n",
      "235 [D loss: (0.697)(R 0.625, F 0.770)] [D acc: (0.527)(0.914, 0.141)] [G loss: 0.766] [G acc: 0.086]\n",
      "236 [D loss: (0.715)(R 0.715, F 0.715)] [D acc: (0.344)(0.367, 0.320)] [G loss: 0.720] [G acc: 0.219]\n",
      "237 [D loss: (0.695)(R 0.630, F 0.760)] [D acc: (0.492)(0.859, 0.125)] [G loss: 0.726] [G acc: 0.320]\n",
      "238 [D loss: (0.700)(R 0.638, F 0.762)] [D acc: (0.449)(0.758, 0.141)] [G loss: 0.697] [G acc: 0.367]\n",
      "239 [D loss: (0.710)(R 0.642, F 0.779)] [D acc: (0.453)(0.805, 0.102)] [G loss: 0.692] [G acc: 0.555]\n",
      "240 [D loss: (0.686)(R 0.637, F 0.734)] [D acc: (0.559)(0.898, 0.219)] [G loss: 0.685] [G acc: 0.586]\n",
      "241 [D loss: (0.702)(R 0.638, F 0.766)] [D acc: (0.457)(0.867, 0.047)] [G loss: 0.700] [G acc: 0.406]\n",
      "242 [D loss: (0.695)(R 0.639, F 0.751)] [D acc: (0.484)(0.844, 0.125)] [G loss: 0.697] [G acc: 0.500]\n",
      "243 [D loss: (0.718)(R 0.631, F 0.804)] [D acc: (0.422)(0.781, 0.062)] [G loss: 0.699] [G acc: 0.469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 [D loss: (0.695)(R 0.643, F 0.746)] [D acc: (0.441)(0.805, 0.078)] [G loss: 0.682] [G acc: 0.625]\n",
      "245 [D loss: (0.692)(R 0.636, F 0.748)] [D acc: (0.473)(0.852, 0.094)] [G loss: 0.693] [G acc: 0.453]\n",
      "246 [D loss: (0.697)(R 0.645, F 0.749)] [D acc: (0.445)(0.766, 0.125)] [G loss: 0.684] [G acc: 0.602]\n",
      "247 [D loss: (0.713)(R 0.617, F 0.810)] [D acc: (0.453)(0.883, 0.023)] [G loss: 0.695] [G acc: 0.500]\n",
      "248 [D loss: (0.713)(R 0.637, F 0.789)] [D acc: (0.449)(0.820, 0.078)] [G loss: 0.686] [G acc: 0.594]\n",
      "249 [D loss: (0.692)(R 0.634, F 0.750)] [D acc: (0.480)(0.852, 0.109)] [G loss: 0.693] [G acc: 0.445]\n",
      "250 [D loss: (0.692)(R 0.649, F 0.736)] [D acc: (0.465)(0.781, 0.148)] [G loss: 0.689] [G acc: 0.523]\n",
      "251 [D loss: (0.699)(R 0.636, F 0.761)] [D acc: (0.465)(0.859, 0.070)] [G loss: 0.686] [G acc: 0.602]\n",
      "252 [D loss: (0.694)(R 0.641, F 0.747)] [D acc: (0.484)(0.883, 0.086)] [G loss: 0.694] [G acc: 0.484]\n",
      "253 [D loss: (0.691)(R 0.639, F 0.743)] [D acc: (0.504)(0.828, 0.180)] [G loss: 0.677] [G acc: 0.609]\n",
      "254 [D loss: (0.696)(R 0.624, F 0.768)] [D acc: (0.520)(0.828, 0.211)] [G loss: 0.688] [G acc: 0.547]\n",
      "255 [D loss: (0.689)(R 0.641, F 0.737)] [D acc: (0.527)(0.820, 0.234)] [G loss: 0.692] [G acc: 0.453]\n",
      "256 [D loss: (0.702)(R 0.652, F 0.751)] [D acc: (0.402)(0.695, 0.109)] [G loss: 0.680] [G acc: 0.688]\n",
      "257 [D loss: (0.695)(R 0.644, F 0.747)] [D acc: (0.531)(0.891, 0.172)] [G loss: 0.689] [G acc: 0.562]\n",
      "258 [D loss: (0.694)(R 0.638, F 0.751)] [D acc: (0.527)(0.898, 0.156)] [G loss: 0.697] [G acc: 0.398]\n",
      "259 [D loss: (0.693)(R 0.657, F 0.728)] [D acc: (0.449)(0.727, 0.172)] [G loss: 0.693] [G acc: 0.484]\n",
      "260 [D loss: (0.686)(R 0.638, F 0.735)] [D acc: (0.531)(0.859, 0.203)] [G loss: 0.733] [G acc: 0.234]\n",
      "261 [D loss: (0.694)(R 0.650, F 0.739)] [D acc: (0.461)(0.734, 0.188)] [G loss: 0.689] [G acc: 0.492]\n",
      "262 [D loss: (0.744)(R 0.640, F 0.848)] [D acc: (0.496)(0.859, 0.133)] [G loss: 0.719] [G acc: 0.258]\n",
      "263 [D loss: (0.708)(R 0.634, F 0.782)] [D acc: (0.434)(0.789, 0.078)] [G loss: 0.685] [G acc: 0.594]\n",
      "264 [D loss: (0.707)(R 0.642, F 0.772)] [D acc: (0.492)(0.898, 0.086)] [G loss: 0.690] [G acc: 0.516]\n",
      "265 [D loss: (0.697)(R 0.637, F 0.757)] [D acc: (0.488)(0.805, 0.172)] [G loss: 0.697] [G acc: 0.406]\n",
      "266 [D loss: (0.728)(R 0.642, F 0.813)] [D acc: (0.422)(0.781, 0.062)] [G loss: 0.713] [G acc: 0.336]\n",
      "267 [D loss: (0.711)(R 0.645, F 0.777)] [D acc: (0.426)(0.789, 0.062)] [G loss: 0.722] [G acc: 0.328]\n",
      "268 [D loss: (0.752)(R 0.645, F 0.860)] [D acc: (0.387)(0.750, 0.023)] [G loss: 0.739] [G acc: 0.227]\n",
      "269 [D loss: (0.740)(R 0.660, F 0.821)] [D acc: (0.391)(0.734, 0.047)] [G loss: 0.714] [G acc: 0.359]\n",
      "270 [D loss: (0.811)(R 0.628, F 0.994)] [D acc: (0.355)(0.711, 0.000)] [G loss: 0.957] [G acc: 0.000]\n",
      "271 [D loss: (0.990)(R 0.933, F 1.048)] [D acc: (0.004)(0.008, 0.000)] [G loss: 0.795] [G acc: 0.023]\n",
      "272 [D loss: (0.784)(R 0.815, F 0.754)] [D acc: (0.137)(0.141, 0.133)] [G loss: 0.794] [G acc: 0.023]\n",
      "273 [D loss: (0.734)(R 0.620, F 0.848)] [D acc: (0.457)(0.852, 0.062)] [G loss: 0.695] [G acc: 0.461]\n",
      "274 [D loss: (0.711)(R 0.611, F 0.811)] [D acc: (0.445)(0.812, 0.078)] [G loss: 0.674] [G acc: 0.617]\n",
      "275 [D loss: (0.706)(R 0.612, F 0.801)] [D acc: (0.473)(0.867, 0.078)] [G loss: 0.684] [G acc: 0.602]\n",
      "276 [D loss: (0.730)(R 0.616, F 0.845)] [D acc: (0.453)(0.891, 0.016)] [G loss: 0.706] [G acc: 0.461]\n",
      "277 [D loss: (0.728)(R 0.637, F 0.820)] [D acc: (0.391)(0.766, 0.016)] [G loss: 0.683] [G acc: 0.562]\n",
      "278 [D loss: (0.723)(R 0.615, F 0.830)] [D acc: (0.461)(0.867, 0.055)] [G loss: 0.680] [G acc: 0.570]\n",
      "279 [D loss: (0.705)(R 0.628, F 0.782)] [D acc: (0.414)(0.773, 0.055)] [G loss: 0.695] [G acc: 0.445]\n",
      "280 [D loss: (0.731)(R 0.635, F 0.827)] [D acc: (0.375)(0.703, 0.047)] [G loss: 0.690] [G acc: 0.562]\n",
      "281 [D loss: (0.713)(R 0.614, F 0.812)] [D acc: (0.520)(0.930, 0.109)] [G loss: 0.681] [G acc: 0.594]\n",
      "282 [D loss: (0.717)(R 0.622, F 0.813)] [D acc: (0.461)(0.836, 0.086)] [G loss: 0.709] [G acc: 0.406]\n",
      "283 [D loss: (0.715)(R 0.616, F 0.815)] [D acc: (0.477)(0.883, 0.070)] [G loss: 0.691] [G acc: 0.500]\n",
      "284 [D loss: (0.697)(R 0.604, F 0.789)] [D acc: (0.496)(0.875, 0.117)] [G loss: 0.687] [G acc: 0.562]\n",
      "285 [D loss: (0.700)(R 0.606, F 0.794)] [D acc: (0.496)(0.914, 0.078)] [G loss: 0.696] [G acc: 0.508]\n",
      "286 [D loss: (0.698)(R 0.615, F 0.781)] [D acc: (0.480)(0.883, 0.078)] [G loss: 0.696] [G acc: 0.453]\n",
      "287 [D loss: (0.711)(R 0.610, F 0.813)] [D acc: (0.445)(0.852, 0.039)] [G loss: 0.677] [G acc: 0.609]\n",
      "288 [D loss: (0.698)(R 0.622, F 0.773)] [D acc: (0.461)(0.820, 0.102)] [G loss: 0.674] [G acc: 0.594]\n",
      "289 [D loss: (0.707)(R 0.618, F 0.797)] [D acc: (0.457)(0.844, 0.070)] [G loss: 0.728] [G acc: 0.281]\n",
      "290 [D loss: (0.740)(R 0.669, F 0.810)] [D acc: (0.328)(0.609, 0.047)] [G loss: 0.724] [G acc: 0.273]\n",
      "291 [D loss: (0.746)(R 0.645, F 0.848)] [D acc: (0.371)(0.711, 0.031)] [G loss: 0.845] [G acc: 0.062]\n",
      "292 [D loss: (0.991)(R 0.771, F 1.211)] [D acc: (0.160)(0.320, 0.000)] [G loss: 1.358] [G acc: 0.000]\n",
      "293 [D loss: (1.045)(R 1.470, F 0.621)] [D acc: (0.465)(0.000, 0.930)] [G loss: 0.801] [G acc: 0.148]\n",
      "294 [D loss: (0.856)(R 0.341, F 1.371)] [D acc: (0.496)(0.992, 0.000)] [G loss: 0.601] [G acc: 0.883]\n",
      "295 [D loss: (0.712)(R 0.495, F 0.928)] [D acc: (0.484)(0.953, 0.016)] [G loss: 0.596] [G acc: 0.906]\n",
      "296 [D loss: (0.750)(R 0.494, F 1.006)] [D acc: (0.484)(0.969, 0.000)] [G loss: 0.627] [G acc: 0.836]\n",
      "297 [D loss: (0.734)(R 0.533, F 0.935)] [D acc: (0.492)(0.984, 0.000)] [G loss: 0.696] [G acc: 0.469]\n",
      "298 [D loss: (0.709)(R 0.575, F 0.844)] [D acc: (0.473)(0.914, 0.031)] [G loss: 0.723] [G acc: 0.383]\n",
      "299 [D loss: (0.723)(R 0.587, F 0.859)] [D acc: (0.465)(0.859, 0.070)] [G loss: 0.670] [G acc: 0.578]\n",
      "300 [D loss: (0.696)(R 0.552, F 0.841)] [D acc: (0.500)(0.914, 0.086)] [G loss: 0.669] [G acc: 0.594]\n",
      "301 [D loss: (0.709)(R 0.562, F 0.857)] [D acc: (0.488)(0.898, 0.078)] [G loss: 0.704] [G acc: 0.430]\n",
      "302 [D loss: (0.736)(R 0.612, F 0.861)] [D acc: (0.453)(0.820, 0.086)] [G loss: 0.691] [G acc: 0.500]\n",
      "303 [D loss: (0.720)(R 0.595, F 0.845)] [D acc: (0.465)(0.875, 0.055)] [G loss: 0.723] [G acc: 0.305]\n",
      "304 [D loss: (0.733)(R 0.613, F 0.853)] [D acc: (0.410)(0.727, 0.094)] [G loss: 0.701] [G acc: 0.469]\n",
      "305 [D loss: (0.723)(R 0.618, F 0.829)] [D acc: (0.469)(0.812, 0.125)] [G loss: 0.726] [G acc: 0.289]\n",
      "306 [D loss: (0.746)(R 0.640, F 0.852)] [D acc: (0.410)(0.734, 0.086)] [G loss: 0.727] [G acc: 0.336]\n",
      "307 [D loss: (0.750)(R 0.634, F 0.867)] [D acc: (0.371)(0.711, 0.031)] [G loss: 0.742] [G acc: 0.289]\n",
      "308 [D loss: (0.691)(R 0.623, F 0.759)] [D acc: (0.496)(0.719, 0.273)] [G loss: 0.783] [G acc: 0.195]\n",
      "309 [D loss: (0.774)(R 0.670, F 0.878)] [D acc: (0.324)(0.570, 0.078)] [G loss: 0.904] [G acc: 0.102]\n",
      "310 [D loss: (0.912)(R 0.787, F 1.037)] [D acc: (0.133)(0.250, 0.016)] [G loss: 0.830] [G acc: 0.117]\n",
      "311 [D loss: (1.314)(R 0.757, F 1.871)] [D acc: (0.199)(0.398, 0.000)] [G loss: 3.541] [G acc: 0.000]\n",
      "312 [D loss: (8.446)(R 3.953, F 12.938)] [D acc: (0.000)(0.000, 0.000)] [G loss: 12.669] [G acc: 0.000]\n",
      "313 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 7.192] [G acc: 0.000]\n",
      "314 [D loss: (8.061)(R 16.118, F 0.003)] [D acc: (0.500)(0.000, 1.000)] [G loss: 5.057] [G acc: 0.000]\n",
      "315 [D loss: (8.068)(R 16.118, F 0.017)] [D acc: (0.500)(0.000, 1.000)] [G loss: 4.011] [G acc: 0.000]\n",
      "316 [D loss: (8.069)(R 16.118, F 0.021)] [D acc: (0.500)(0.000, 1.000)] [G loss: 3.341] [G acc: 0.000]\n",
      "317 [D loss: (8.082)(R 16.118, F 0.046)] [D acc: (0.500)(0.000, 1.000)] [G loss: 3.036] [G acc: 0.000]\n",
      "318 [D loss: (8.114)(R 16.118, F 0.110)] [D acc: (0.500)(0.000, 1.000)] [G loss: 5.411] [G acc: 0.000]\n",
      "319 [D loss: (8.084)(R 16.118, F 0.049)] [D acc: (0.500)(0.000, 1.000)] [G loss: 4.672] [G acc: 0.000]\n",
      "320 [D loss: (8.071)(R 16.118, F 0.023)] [D acc: (0.500)(0.000, 1.000)] [G loss: 6.307] [G acc: 0.000]\n",
      "321 [D loss: (8.085)(R 16.118, F 0.052)] [D acc: (0.500)(0.000, 1.000)] [G loss: 4.925] [G acc: 0.000]\n",
      "322 [D loss: (8.062)(R 16.118, F 0.006)] [D acc: (0.500)(0.000, 1.000)] [G loss: 7.832] [G acc: 0.000]\n",
      "323 [D loss: (8.069)(R 16.118, F 0.020)] [D acc: (0.500)(0.000, 1.000)] [G loss: 8.213] [G acc: 0.000]\n",
      "324 [D loss: (8.065)(R 16.118, F 0.012)] [D acc: (0.500)(0.000, 1.000)] [G loss: 6.570] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325 [D loss: (8.062)(R 16.118, F 0.006)] [D acc: (0.500)(0.000, 1.000)] [G loss: 7.559] [G acc: 0.000]\n",
      "326 [D loss: (8.059)(R 16.118, F 0.001)] [D acc: (0.500)(0.000, 1.000)] [G loss: 9.768] [G acc: 0.000]\n",
      "327 [D loss: (8.065)(R 16.118, F 0.011)] [D acc: (0.500)(0.000, 1.000)] [G loss: 6.577] [G acc: 0.000]\n",
      "328 [D loss: (8.060)(R 16.118, F 0.001)] [D acc: (0.500)(0.000, 1.000)] [G loss: 4.269] [G acc: 0.000]\n",
      "329 [D loss: (8.062)(R 16.118, F 0.006)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.739] [G acc: 0.000]\n",
      "330 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 9.689] [G acc: 0.000]\n",
      "331 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 8.826] [G acc: 0.000]\n",
      "332 [D loss: (8.060)(R 16.118, F 0.002)] [D acc: (0.500)(0.000, 1.000)] [G loss: 7.129] [G acc: 0.000]\n",
      "333 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 10.633] [G acc: 0.000]\n",
      "334 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 9.878] [G acc: 0.000]\n",
      "335 [D loss: (8.060)(R 16.118, F 0.001)] [D acc: (0.500)(0.000, 1.000)] [G loss: 8.219] [G acc: 0.000]\n",
      "336 [D loss: (8.060)(R 16.118, F 0.002)] [D acc: (0.500)(0.000, 1.000)] [G loss: 7.785] [G acc: 0.000]\n",
      "337 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.092] [G acc: 0.000]\n",
      "338 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 10.699] [G acc: 0.000]\n",
      "339 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 10.187] [G acc: 0.000]\n",
      "340 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 9.214] [G acc: 0.000]\n",
      "341 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 9.125] [G acc: 0.000]\n",
      "342 [D loss: (8.059)(R 16.118, F 0.001)] [D acc: (0.500)(0.000, 1.000)] [G loss: 10.296] [G acc: 0.000]\n",
      "343 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.759] [G acc: 0.000]\n",
      "344 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.499] [G acc: 0.000]\n",
      "345 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.474] [G acc: 0.000]\n",
      "346 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.664] [G acc: 0.000]\n",
      "347 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.856] [G acc: 0.000]\n",
      "348 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.049] [G acc: 0.000]\n",
      "349 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.358] [G acc: 0.000]\n",
      "350 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.639] [G acc: 0.000]\n",
      "351 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.410] [G acc: 0.000]\n",
      "352 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.311] [G acc: 0.000]\n",
      "353 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.897] [G acc: 0.000]\n",
      "354 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 12.069] [G acc: 0.000]\n",
      "355 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 11.893] [G acc: 0.000]\n",
      "356 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 12.284] [G acc: 0.000]\n",
      "357 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 12.888] [G acc: 0.000]\n",
      "358 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.012] [G acc: 0.000]\n",
      "359 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 12.935] [G acc: 0.000]\n",
      "360 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.201] [G acc: 0.000]\n",
      "361 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.528] [G acc: 0.000]\n",
      "362 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.520] [G acc: 0.000]\n",
      "363 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.561] [G acc: 0.000]\n",
      "364 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.071] [G acc: 0.000]\n",
      "365 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.648] [G acc: 0.000]\n",
      "366 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.895] [G acc: 0.000]\n",
      "367 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.019] [G acc: 0.000]\n",
      "368 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 13.878] [G acc: 0.000]\n",
      "369 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.001] [G acc: 0.000]\n",
      "370 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.268] [G acc: 0.000]\n",
      "371 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.389] [G acc: 0.000]\n",
      "372 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.300] [G acc: 0.000]\n",
      "373 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.711] [G acc: 0.000]\n",
      "374 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.645] [G acc: 0.000]\n",
      "375 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.732] [G acc: 0.000]\n",
      "376 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.055] [G acc: 0.000]\n",
      "377 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 14.863] [G acc: 0.000]\n",
      "378 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.067] [G acc: 0.000]\n",
      "379 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.039] [G acc: 0.000]\n",
      "380 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.302] [G acc: 0.000]\n",
      "381 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.364] [G acc: 0.000]\n",
      "382 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.512] [G acc: 0.000]\n",
      "383 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.418] [G acc: 0.000]\n",
      "384 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.499] [G acc: 0.000]\n",
      "385 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.653] [G acc: 0.000]\n",
      "386 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.799] [G acc: 0.000]\n",
      "387 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.867] [G acc: 0.000]\n",
      "388 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.893] [G acc: 0.000]\n",
      "389 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 15.976] [G acc: 0.000]\n",
      "390 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.039] [G acc: 0.000]\n",
      "391 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "392 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "393 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "394 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.111] [G acc: 0.000]\n",
      "395 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "396 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "397 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "398 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "399 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "400 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "401 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "402 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "403 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "405 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "406 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "407 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "408 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "409 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "410 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "411 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "412 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "413 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "414 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "415 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "416 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "417 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "418 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "419 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "420 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "421 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "422 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "423 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "424 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "425 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "426 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "427 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "428 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "429 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "430 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "431 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "432 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "433 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "434 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "435 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "436 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "437 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "438 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "439 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "440 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "441 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "442 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "443 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "444 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "445 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "446 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "447 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "448 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "449 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "450 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "451 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "452 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "453 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "454 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "455 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "456 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "457 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "458 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "459 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "460 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "461 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "462 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "463 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "464 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "465 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "466 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "467 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "468 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "469 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "470 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "471 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "472 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "473 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "474 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "475 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "476 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "477 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "478 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "479 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "480 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "481 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "482 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "484 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "485 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "486 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "487 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "488 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "489 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "490 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "491 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "492 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "493 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "494 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "495 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "496 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "497 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "498 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "499 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "500 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "501 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "502 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "503 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "504 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "505 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "506 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "507 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "508 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "509 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "510 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "511 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "512 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "513 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "514 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "515 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "516 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "517 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "518 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "519 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "520 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "521 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "522 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "523 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "524 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "525 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "526 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "527 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "528 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "529 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "530 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "531 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "532 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "533 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "534 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "535 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "536 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "537 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "538 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "539 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "540 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "541 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "542 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "543 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "544 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "545 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "546 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "547 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "548 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "549 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "550 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "551 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "552 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "553 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "554 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "555 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "556 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "557 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "558 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "559 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "560 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n",
      "561 [D loss: (8.059)(R 16.118, F 0.000)] [D acc: (0.500)(0.000, 1.000)] [G loss: 16.118] [G acc: 0.000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-17df8a8afeb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mrun_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRUN_FOLDER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m,\u001b[0m \u001b[0mprint_every_n_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPRINT_EVERY_N_BATCHES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/Git/Personal/GDL/generative_deep_learning_code/models/GAN.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, batch_size, epochs, run_folder, print_every_n_batches, using_generator)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musing_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/Personal/GDL/generative_deep_learning_code/models/GAN.py\u001b[0m in \u001b[0;36mtrain_discriminator\u001b[0;34m(self, x_train, batch_size, using_generator)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan.train(     \n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = 20000\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17e8383c8>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.plot([x[1] for x in gan.d_losses], color='green', linewidth=0.5)\n",
    "plt.plot([x[2] for x in gan.d_losses], color='red', linewidth=0.5)\n",
    "\n",
    "plt.plot([x[0] for x in gan.g_losses], color='orange', linewidth=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
