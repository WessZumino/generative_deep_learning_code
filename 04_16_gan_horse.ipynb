{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.GAN import GAN\n",
    "from utils.loaders import load_safari, load_cifar10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = '0016'\n",
    "RUN_FOLDER = os.path.join(\"./run\", RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 7\n",
    "(x_train, y_train) = load_cifar10(label)\n",
    "# (x_train, y_train) = load_safari('elephant')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x131dda278>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(x_train))\n",
    "print(np.max(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works\n",
    "\n",
    "gan = GAN(input_dim = (32,32,3)\n",
    "        , discriminator_conv_filters = [64,64,128,128]\n",
    "        , discriminator_conv_kernel_size = [5,5,5,5]\n",
    "        , discriminator_conv_strides = [2,2,2,1]\n",
    "        , discriminator_conv_padding = 'same'\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'leaky_relu'\n",
    "        , discriminator_dropout_rate = None\n",
    "        , discriminator_learning_rate = 0.001\n",
    "        , generator_initial_dense_layer_size = (8, 8, 128)\n",
    "        , generator_use_upsampling = [True,True, False,False]\n",
    "        , generator_conv_t_filters = [128,64, 64,3]\n",
    "        , generator_conv_t_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_t_strides = [1,1,1,1]\n",
    "        , generator_conv_t_padding = 'same'\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'leaky_relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0001\n",
    "        , optimiser = 'rmsprop'\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "#testing\n",
    "\n",
    "\n",
    "\n",
    "gan = GAN(input_dim = (32,32,3)\n",
    "        , discriminator_conv_filters = [64,64,128,128]\n",
    "        , discriminator_conv_kernel_size = [5,5,5,5]\n",
    "        , discriminator_conv_strides = [2,2,2,1]\n",
    "        , discriminator_conv_padding = 'same'\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'leaky_relu'\n",
    "        , discriminator_dropout_rate = None\n",
    "        , discriminator_learning_rate = 0.0008\n",
    "        , generator_initial_dense_layer_size = (8, 8, 64)\n",
    "        , generator_use_upsampling = [True,True, False,False]\n",
    "        , generator_conv_t_filters = [128,64, 64,3]\n",
    "        , generator_conv_t_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_t_strides = [1,1,1,1]\n",
    "        , generator_conv_t_padding = 'same'\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'leaky_relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0004\n",
    "        , optimiser = 'rmsprop'\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "gan.save(RUN_FOLDER)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 16, 16, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 8, 8, 64)          102464    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 1,448,066\n",
      "Trainable params: 724,033\n",
      "Non-trainable params: 724,033\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfoster/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8192)              827392    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 8192)              32768     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_0 (Conv2DTr (None, 16, 16, 128)       409728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_1 (Conv2DTr (None, 32, 32, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_2 (Conv2DTr (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_3 (Conv2DTr (None, 32, 32, 3)         4803      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 1,583,043\n",
      "Trainable params: 1,566,147\n",
      "Non-trainable params: 16,896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.697934] [D acc: 0.437500] [G loss: 0.098444] [G acc: 1.000000]\n",
      "1 [D loss: 2.610443] [D acc: 0.500000] [G loss: 2.406914] [G acc: 0.000000]\n",
      "2 [D loss: 0.571848] [D acc: 0.753906] [G loss: 0.841384] [G acc: 0.390625]\n",
      "3 [D loss: 6.148867] [D acc: 0.480469] [G loss: 2.515951] [G acc: 0.000000]\n",
      "4 [D loss: 0.722893] [D acc: 0.605469] [G loss: 3.741087] [G acc: 0.000000]\n",
      "5 [D loss: 2.285067] [D acc: 0.359375] [G loss: 1.329217] [G acc: 0.171875]\n",
      "6 [D loss: 2.407302] [D acc: 0.273438] [G loss: 0.584004] [G acc: 1.000000]\n",
      "7 [D loss: 0.684219] [D acc: 0.500000] [G loss: 0.672019] [G acc: 0.835938]\n",
      "8 [D loss: 0.623252] [D acc: 0.558594] [G loss: 0.707142] [G acc: 0.351562]\n",
      "9 [D loss: 0.561203] [D acc: 0.671875] [G loss: 0.894926] [G acc: 0.000000]\n",
      "10 [D loss: 0.470784] [D acc: 0.945312] [G loss: 1.053017] [G acc: 0.000000]\n",
      "11 [D loss: 0.475186] [D acc: 0.714844] [G loss: 1.612737] [G acc: 0.000000]\n",
      "12 [D loss: 0.505667] [D acc: 0.722656] [G loss: 1.287866] [G acc: 0.000000]\n",
      "13 [D loss: 2.009515] [D acc: 0.453125] [G loss: 1.490286] [G acc: 0.000000]\n",
      "14 [D loss: 0.692612] [D acc: 0.500000] [G loss: 1.295501] [G acc: 0.000000]\n",
      "15 [D loss: 0.626432] [D acc: 0.550781] [G loss: 1.301309] [G acc: 0.000000]\n",
      "16 [D loss: 0.607514] [D acc: 0.648438] [G loss: 1.316845] [G acc: 0.000000]\n",
      "17 [D loss: 0.567272] [D acc: 0.699219] [G loss: 1.366715] [G acc: 0.000000]\n",
      "18 [D loss: 0.552419] [D acc: 0.730469] [G loss: 1.410901] [G acc: 0.000000]\n",
      "19 [D loss: 0.502257] [D acc: 0.769531] [G loss: 1.495506] [G acc: 0.000000]\n",
      "20 [D loss: 0.475914] [D acc: 0.792969] [G loss: 1.439355] [G acc: 0.000000]\n",
      "21 [D loss: 0.380574] [D acc: 0.886719] [G loss: 1.729792] [G acc: 0.000000]\n",
      "22 [D loss: 0.323646] [D acc: 0.875000] [G loss: 1.747404] [G acc: 0.000000]\n",
      "23 [D loss: 1.022312] [D acc: 0.539062] [G loss: 2.291164] [G acc: 0.000000]\n",
      "24 [D loss: 1.713343] [D acc: 0.425781] [G loss: 1.230627] [G acc: 0.000000]\n",
      "25 [D loss: 0.466379] [D acc: 0.816406] [G loss: 1.656178] [G acc: 0.000000]\n",
      "26 [D loss: 0.427517] [D acc: 0.816406] [G loss: 1.668314] [G acc: 0.000000]\n",
      "27 [D loss: 0.737123] [D acc: 0.554688] [G loss: 1.304845] [G acc: 0.000000]\n",
      "28 [D loss: 0.626549] [D acc: 0.652344] [G loss: 1.158270] [G acc: 0.000000]\n",
      "29 [D loss: 0.582819] [D acc: 0.714844] [G loss: 1.082009] [G acc: 0.000000]\n",
      "30 [D loss: 0.585460] [D acc: 0.714844] [G loss: 1.101499] [G acc: 0.000000]\n",
      "31 [D loss: 0.581469] [D acc: 0.699219] [G loss: 1.306668] [G acc: 0.000000]\n",
      "32 [D loss: 0.520716] [D acc: 0.750000] [G loss: 1.421471] [G acc: 0.015625]\n",
      "33 [D loss: 0.644020] [D acc: 0.632812] [G loss: 1.550239] [G acc: 0.000000]\n",
      "34 [D loss: 0.643465] [D acc: 0.582031] [G loss: 1.173925] [G acc: 0.117188]\n",
      "35 [D loss: 0.576010] [D acc: 0.667969] [G loss: 1.242493] [G acc: 0.218750]\n",
      "36 [D loss: 0.757442] [D acc: 0.539062] [G loss: 1.368184] [G acc: 0.007812]\n",
      "37 [D loss: 0.663856] [D acc: 0.566406] [G loss: 1.143698] [G acc: 0.007812]\n",
      "38 [D loss: 0.630759] [D acc: 0.562500] [G loss: 1.139954] [G acc: 0.000000]\n",
      "39 [D loss: 0.639113] [D acc: 0.593750] [G loss: 1.036968] [G acc: 0.046875]\n",
      "40 [D loss: 0.630894] [D acc: 0.625000] [G loss: 1.108546] [G acc: 0.156250]\n",
      "41 [D loss: 0.570487] [D acc: 0.687500] [G loss: 1.312486] [G acc: 0.125000]\n",
      "42 [D loss: 0.626424] [D acc: 0.664062] [G loss: 0.917853] [G acc: 0.242188]\n",
      "43 [D loss: 0.594799] [D acc: 0.718750] [G loss: 1.226305] [G acc: 0.070312]\n",
      "44 [D loss: 0.657054] [D acc: 0.585938] [G loss: 0.980792] [G acc: 0.132812]\n",
      "45 [D loss: 0.608916] [D acc: 0.667969] [G loss: 1.123877] [G acc: 0.140625]\n",
      "46 [D loss: 0.594475] [D acc: 0.664062] [G loss: 0.901702] [G acc: 0.320312]\n",
      "47 [D loss: 0.699393] [D acc: 0.566406] [G loss: 1.913320] [G acc: 0.000000]\n",
      "48 [D loss: 0.714824] [D acc: 0.500000] [G loss: 0.665460] [G acc: 0.656250]\n",
      "49 [D loss: 0.695983] [D acc: 0.527344] [G loss: 0.904385] [G acc: 0.007812]\n",
      "50 [D loss: 0.637129] [D acc: 0.628906] [G loss: 0.995332] [G acc: 0.015625]\n",
      "51 [D loss: 0.617484] [D acc: 0.613281] [G loss: 0.974715] [G acc: 0.179688]\n",
      "52 [D loss: 0.563879] [D acc: 0.742188] [G loss: 0.799335] [G acc: 0.398438]\n",
      "53 [D loss: 0.553638] [D acc: 0.824219] [G loss: 1.680545] [G acc: 0.000000]\n",
      "54 [D loss: 0.755919] [D acc: 0.542969] [G loss: 0.466779] [G acc: 0.984375]\n",
      "55 [D loss: 0.758653] [D acc: 0.519531] [G loss: 1.884387] [G acc: 0.000000]\n",
      "56 [D loss: 0.630811] [D acc: 0.625000] [G loss: 0.409798] [G acc: 0.945312]\n",
      "57 [D loss: 0.994372] [D acc: 0.468750] [G loss: 0.968656] [G acc: 0.070312]\n",
      "58 [D loss: 0.539665] [D acc: 0.777344] [G loss: 1.390256] [G acc: 0.164062]\n",
      "59 [D loss: 0.686886] [D acc: 0.640625] [G loss: 1.408696] [G acc: 0.000000]\n",
      "60 [D loss: 0.683764] [D acc: 0.621094] [G loss: 0.872916] [G acc: 0.117188]\n",
      "61 [D loss: 0.621258] [D acc: 0.671875] [G loss: 1.201538] [G acc: 0.000000]\n",
      "62 [D loss: 0.599496] [D acc: 0.664062] [G loss: 0.653793] [G acc: 0.585938]\n",
      "63 [D loss: 0.757823] [D acc: 0.460938] [G loss: 1.443032] [G acc: 0.007812]\n",
      "64 [D loss: 0.728278] [D acc: 0.542969] [G loss: 0.847074] [G acc: 0.000000]\n",
      "65 [D loss: 0.630020] [D acc: 0.691406] [G loss: 0.896297] [G acc: 0.000000]\n",
      "66 [D loss: 0.594354] [D acc: 0.773438] [G loss: 0.891091] [G acc: 0.039062]\n",
      "67 [D loss: 0.544659] [D acc: 0.929688] [G loss: 1.131849] [G acc: 0.000000]\n",
      "68 [D loss: 0.492959] [D acc: 0.890625] [G loss: 0.984970] [G acc: 0.187500]\n",
      "69 [D loss: 0.590398] [D acc: 0.699219] [G loss: 2.717293] [G acc: 0.000000]\n",
      "70 [D loss: 0.952574] [D acc: 0.507812] [G loss: 0.554175] [G acc: 0.789062]\n",
      "71 [D loss: 0.699646] [D acc: 0.558594] [G loss: 0.838784] [G acc: 0.000000]\n",
      "72 [D loss: 0.614785] [D acc: 0.699219] [G loss: 0.827953] [G acc: 0.000000]\n",
      "73 [D loss: 0.574840] [D acc: 0.792969] [G loss: 0.781054] [G acc: 0.226562]\n",
      "74 [D loss: 0.525749] [D acc: 0.851562] [G loss: 0.820531] [G acc: 0.273438]\n",
      "75 [D loss: 0.508338] [D acc: 0.800781] [G loss: 0.780055] [G acc: 0.320312]\n",
      "76 [D loss: 0.478633] [D acc: 0.832031] [G loss: 1.240095] [G acc: 0.039062]\n",
      "77 [D loss: 0.578998] [D acc: 0.707031] [G loss: 1.056017] [G acc: 0.429688]\n",
      "78 [D loss: 1.819451] [D acc: 0.363281] [G loss: 1.489938] [G acc: 0.031250]\n",
      "79 [D loss: 0.662852] [D acc: 0.640625] [G loss: 0.759335] [G acc: 0.414062]\n",
      "80 [D loss: 0.674011] [D acc: 0.570312] [G loss: 0.876462] [G acc: 0.078125]\n",
      "81 [D loss: 0.661588] [D acc: 0.597656] [G loss: 0.789665] [G acc: 0.218750]\n",
      "82 [D loss: 0.655133] [D acc: 0.644531] [G loss: 0.813480] [G acc: 0.109375]\n",
      "83 [D loss: 0.631081] [D acc: 0.734375] [G loss: 0.848268] [G acc: 0.117188]\n",
      "84 [D loss: 0.619305] [D acc: 0.730469] [G loss: 0.850963] [G acc: 0.148438]\n",
      "85 [D loss: 0.557785] [D acc: 0.832031] [G loss: 0.746309] [G acc: 0.429688]\n",
      "86 [D loss: 0.455008] [D acc: 0.859375] [G loss: 0.480239] [G acc: 0.773438]\n",
      "87 [D loss: 0.506900] [D acc: 0.804688] [G loss: 0.203437] [G acc: 0.992188]\n",
      "88 [D loss: 0.518025] [D acc: 0.726562] [G loss: 1.272590] [G acc: 0.140625]\n",
      "89 [D loss: 1.077103] [D acc: 0.226562] [G loss: 1.450257] [G acc: 0.031250]\n",
      "90 [D loss: 0.591768] [D acc: 0.773438] [G loss: 1.104447] [G acc: 0.054688]\n",
      "91 [D loss: 0.555755] [D acc: 0.812500] [G loss: 1.314167] [G acc: 0.054688]\n",
      "92 [D loss: 0.633185] [D acc: 0.679688] [G loss: 0.934789] [G acc: 0.078125]\n",
      "93 [D loss: 0.586717] [D acc: 0.773438] [G loss: 1.106707] [G acc: 0.000000]\n",
      "94 [D loss: 0.535356] [D acc: 0.867188] [G loss: 0.745962] [G acc: 0.375000]\n",
      "95 [D loss: 0.603615] [D acc: 0.683594] [G loss: 2.696730] [G acc: 0.000000]\n",
      "96 [D loss: 0.863125] [D acc: 0.582031] [G loss: 0.526071] [G acc: 0.906250]\n",
      "97 [D loss: 0.556351] [D acc: 0.660156] [G loss: 0.735220] [G acc: 0.406250]\n",
      "98 [D loss: 0.498785] [D acc: 0.839844] [G loss: 0.513635] [G acc: 0.757812]\n",
      "99 [D loss: 0.426701] [D acc: 0.886719] [G loss: 0.579917] [G acc: 0.640625]\n",
      "100 [D loss: 0.477181] [D acc: 0.781250] [G loss: 0.578429] [G acc: 0.703125]\n",
      "101 [D loss: 0.589452] [D acc: 0.640625] [G loss: 1.842096] [G acc: 0.015625]\n",
      "102 [D loss: 0.678589] [D acc: 0.601562] [G loss: 0.605211] [G acc: 0.625000]\n",
      "103 [D loss: 0.669625] [D acc: 0.585938] [G loss: 1.563360] [G acc: 0.000000]\n",
      "104 [D loss: 0.868611] [D acc: 0.523438] [G loss: 0.723015] [G acc: 0.421875]\n",
      "105 [D loss: 0.531317] [D acc: 0.777344] [G loss: 0.863320] [G acc: 0.234375]\n",
      "106 [D loss: 0.544919] [D acc: 0.628906] [G loss: 0.989392] [G acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 [D loss: 0.505546] [D acc: 0.898438] [G loss: 0.868263] [G acc: 0.156250]\n",
      "108 [D loss: 0.454988] [D acc: 0.781250] [G loss: 1.266657] [G acc: 0.031250]\n",
      "109 [D loss: 0.523580] [D acc: 0.777344] [G loss: 0.700643] [G acc: 0.468750]\n",
      "110 [D loss: 0.394341] [D acc: 0.824219] [G loss: 1.382184] [G acc: 0.023438]\n",
      "111 [D loss: 0.469690] [D acc: 0.796875] [G loss: 0.055006] [G acc: 1.000000]\n",
      "112 [D loss: 0.496523] [D acc: 0.707031] [G loss: 1.665652] [G acc: 0.007812]\n",
      "113 [D loss: 0.863097] [D acc: 0.570312] [G loss: 0.361677] [G acc: 0.882812]\n",
      "114 [D loss: 0.638652] [D acc: 0.554688] [G loss: 1.055856] [G acc: 0.007812]\n",
      "115 [D loss: 0.624733] [D acc: 0.675781] [G loss: 0.834083] [G acc: 0.242188]\n",
      "116 [D loss: 0.577207] [D acc: 0.687500] [G loss: 1.017167] [G acc: 0.015625]\n",
      "117 [D loss: 0.565500] [D acc: 0.785156] [G loss: 0.457264] [G acc: 0.875000]\n",
      "118 [D loss: 0.622664] [D acc: 0.546875] [G loss: 1.496791] [G acc: 0.031250]\n",
      "119 [D loss: 0.769681] [D acc: 0.613281] [G loss: 0.544417] [G acc: 0.898438]\n",
      "120 [D loss: 0.682308] [D acc: 0.503906] [G loss: 0.935636] [G acc: 0.117188]\n",
      "121 [D loss: 0.664589] [D acc: 0.613281] [G loss: 0.924101] [G acc: 0.070312]\n",
      "122 [D loss: 0.661684] [D acc: 0.601562] [G loss: 0.921789] [G acc: 0.007812]\n",
      "123 [D loss: 0.591276] [D acc: 0.824219] [G loss: 1.075297] [G acc: 0.015625]\n",
      "124 [D loss: 0.545448] [D acc: 0.804688] [G loss: 1.566866] [G acc: 0.015625]\n",
      "125 [D loss: 0.532925] [D acc: 0.769531] [G loss: 0.877304] [G acc: 0.226562]\n",
      "126 [D loss: 0.526868] [D acc: 0.691406] [G loss: 1.699210] [G acc: 0.000000]\n",
      "127 [D loss: 0.670730] [D acc: 0.621094] [G loss: 0.867868] [G acc: 0.234375]\n",
      "128 [D loss: 0.662002] [D acc: 0.664062] [G loss: 1.253739] [G acc: 0.039062]\n",
      "129 [D loss: 0.532277] [D acc: 0.734375] [G loss: 0.967211] [G acc: 0.257812]\n",
      "130 [D loss: 0.545159] [D acc: 0.710938] [G loss: 1.177649] [G acc: 0.007812]\n",
      "131 [D loss: 0.664327] [D acc: 0.628906] [G loss: 1.031392] [G acc: 0.382812]\n",
      "132 [D loss: 0.910651] [D acc: 0.484375] [G loss: 0.903052] [G acc: 0.023438]\n",
      "133 [D loss: 0.643151] [D acc: 0.628906] [G loss: 0.827567] [G acc: 0.125000]\n",
      "134 [D loss: 0.587348] [D acc: 0.789062] [G loss: 0.846468] [G acc: 0.140625]\n",
      "135 [D loss: 0.540976] [D acc: 0.824219] [G loss: 0.701413] [G acc: 0.531250]\n",
      "136 [D loss: 0.447386] [D acc: 0.968750] [G loss: 0.913822] [G acc: 0.203125]\n",
      "137 [D loss: 0.412872] [D acc: 0.910156] [G loss: 1.298724] [G acc: 0.117188]\n",
      "138 [D loss: 0.420993] [D acc: 0.808594] [G loss: 4.515443] [G acc: 0.007812]\n",
      "139 [D loss: 2.668413] [D acc: 0.312500] [G loss: 1.082040] [G acc: 0.187500]\n",
      "140 [D loss: 0.480675] [D acc: 0.750000] [G loss: 0.646473] [G acc: 0.632812]\n",
      "141 [D loss: 0.513376] [D acc: 0.769531] [G loss: 0.747628] [G acc: 0.507812]\n",
      "142 [D loss: 0.510285] [D acc: 0.777344] [G loss: 0.820166] [G acc: 0.390625]\n",
      "143 [D loss: 0.658652] [D acc: 0.675781] [G loss: 1.020338] [G acc: 0.109375]\n",
      "144 [D loss: 0.536569] [D acc: 0.722656] [G loss: 1.222831] [G acc: 0.140625]\n",
      "145 [D loss: 0.454807] [D acc: 0.828125] [G loss: 1.158525] [G acc: 0.250000]\n",
      "146 [D loss: 0.420621] [D acc: 0.816406] [G loss: 0.595526] [G acc: 0.671875]\n",
      "147 [D loss: 0.413858] [D acc: 0.847656] [G loss: 0.600415] [G acc: 0.726562]\n",
      "148 [D loss: 0.338580] [D acc: 0.914062] [G loss: 0.324222] [G acc: 0.882812]\n",
      "149 [D loss: 0.613221] [D acc: 0.617188] [G loss: 2.565311] [G acc: 0.046875]\n",
      "150 [D loss: 0.834155] [D acc: 0.640625] [G loss: 0.381171] [G acc: 0.828125]\n",
      "151 [D loss: 0.635949] [D acc: 0.687500] [G loss: 1.478533] [G acc: 0.007812]\n",
      "152 [D loss: 0.461828] [D acc: 0.804688] [G loss: 1.050118] [G acc: 0.078125]\n",
      "153 [D loss: 0.537348] [D acc: 0.730469] [G loss: 0.789717] [G acc: 0.382812]\n",
      "154 [D loss: 0.476046] [D acc: 0.863281] [G loss: 0.782274] [G acc: 0.414062]\n",
      "155 [D loss: 0.454591] [D acc: 0.828125] [G loss: 0.191376] [G acc: 1.000000]\n",
      "156 [D loss: 0.447093] [D acc: 0.816406] [G loss: 1.652857] [G acc: 0.078125]\n",
      "157 [D loss: 0.680937] [D acc: 0.640625] [G loss: 0.793522] [G acc: 0.421875]\n",
      "158 [D loss: 0.597246] [D acc: 0.609375] [G loss: 1.252010] [G acc: 0.007812]\n",
      "159 [D loss: 0.521912] [D acc: 0.765625] [G loss: 0.509806] [G acc: 0.781250]\n",
      "160 [D loss: 0.519396] [D acc: 0.726562] [G loss: 1.648940] [G acc: 0.023438]\n",
      "161 [D loss: 0.543503] [D acc: 0.753906] [G loss: 0.735418] [G acc: 0.476562]\n",
      "162 [D loss: 0.449624] [D acc: 0.835938] [G loss: 1.825950] [G acc: 0.000000]\n",
      "163 [D loss: 0.470096] [D acc: 0.808594] [G loss: 0.331546] [G acc: 0.882812]\n",
      "164 [D loss: 0.580406] [D acc: 0.589844] [G loss: 3.293290] [G acc: 0.000000]\n",
      "165 [D loss: 0.587285] [D acc: 0.703125] [G loss: 1.001518] [G acc: 0.046875]\n",
      "166 [D loss: 0.487402] [D acc: 0.800781] [G loss: 1.381325] [G acc: 0.023438]\n",
      "167 [D loss: 0.419429] [D acc: 0.835938] [G loss: 1.377211] [G acc: 0.062500]\n",
      "168 [D loss: 0.367912] [D acc: 0.847656] [G loss: 0.771677] [G acc: 0.601562]\n",
      "169 [D loss: 0.433590] [D acc: 0.808594] [G loss: 0.467188] [G acc: 0.812500]\n",
      "170 [D loss: 0.437909] [D acc: 0.781250] [G loss: 1.434405] [G acc: 0.335938]\n",
      "171 [D loss: 1.013271] [D acc: 0.496094] [G loss: 1.283135] [G acc: 0.093750]\n",
      "172 [D loss: 0.605174] [D acc: 0.621094] [G loss: 1.291806] [G acc: 0.023438]\n",
      "173 [D loss: 0.541443] [D acc: 0.742188] [G loss: 1.321965] [G acc: 0.007812]\n",
      "174 [D loss: 0.524237] [D acc: 0.722656] [G loss: 1.834050] [G acc: 0.000000]\n",
      "175 [D loss: 0.485398] [D acc: 0.847656] [G loss: 1.297422] [G acc: 0.109375]\n",
      "176 [D loss: 0.577425] [D acc: 0.628906] [G loss: 2.566590] [G acc: 0.000000]\n",
      "177 [D loss: 0.704719] [D acc: 0.621094] [G loss: 0.539518] [G acc: 0.828125]\n",
      "178 [D loss: 0.474131] [D acc: 0.835938] [G loss: 0.754975] [G acc: 0.484375]\n",
      "179 [D loss: 0.503728] [D acc: 0.765625] [G loss: 0.865801] [G acc: 0.328125]\n",
      "180 [D loss: 0.475609] [D acc: 0.789062] [G loss: 1.072433] [G acc: 0.257812]\n",
      "181 [D loss: 0.459048] [D acc: 0.824219] [G loss: 0.750024] [G acc: 0.468750]\n",
      "182 [D loss: 0.357239] [D acc: 0.890625] [G loss: 0.082369] [G acc: 0.992188]\n",
      "183 [D loss: 0.414253] [D acc: 0.792969] [G loss: 2.062261] [G acc: 0.085938]\n",
      "184 [D loss: 0.822338] [D acc: 0.613281] [G loss: 1.501473] [G acc: 0.054688]\n",
      "185 [D loss: 0.538383] [D acc: 0.722656] [G loss: 1.211036] [G acc: 0.093750]\n",
      "186 [D loss: 0.439271] [D acc: 0.843750] [G loss: 0.944505] [G acc: 0.335938]\n",
      "187 [D loss: 0.362711] [D acc: 0.878906] [G loss: 1.290233] [G acc: 0.171875]\n",
      "188 [D loss: 0.329277] [D acc: 0.937500] [G loss: 3.719779] [G acc: 0.000000]\n",
      "189 [D loss: 0.385831] [D acc: 0.820312] [G loss: 2.086548] [G acc: 0.164062]\n",
      "190 [D loss: 0.351928] [D acc: 0.886719] [G loss: 0.226041] [G acc: 0.976562]\n",
      "191 [D loss: 0.387192] [D acc: 0.781250] [G loss: 3.460630] [G acc: 0.000000]\n",
      "192 [D loss: 0.654950] [D acc: 0.714844] [G loss: 0.303656] [G acc: 0.960938]\n",
      "193 [D loss: 0.338260] [D acc: 0.878906] [G loss: 0.461754] [G acc: 0.804688]\n",
      "194 [D loss: 0.272926] [D acc: 0.957031] [G loss: 0.739444] [G acc: 0.539062]\n",
      "195 [D loss: 0.299137] [D acc: 0.914062] [G loss: 2.322727] [G acc: 0.000000]\n",
      "196 [D loss: 0.366359] [D acc: 0.847656] [G loss: 0.607837] [G acc: 0.625000]\n",
      "197 [D loss: 1.167326] [D acc: 0.433594] [G loss: 6.313797] [G acc: 0.000000]\n",
      "198 [D loss: 1.456211] [D acc: 0.585938] [G loss: 0.900803] [G acc: 0.101562]\n",
      "199 [D loss: 0.521172] [D acc: 0.796875] [G loss: 1.154737] [G acc: 0.031250]\n",
      "200 [D loss: 0.492480] [D acc: 0.781250] [G loss: 1.312134] [G acc: 0.039062]\n",
      "201 [D loss: 0.457725] [D acc: 0.812500] [G loss: 1.484473] [G acc: 0.078125]\n",
      "202 [D loss: 0.462166] [D acc: 0.792969] [G loss: 1.515055] [G acc: 0.062500]\n",
      "203 [D loss: 0.495216] [D acc: 0.777344] [G loss: 1.567815] [G acc: 0.085938]\n",
      "204 [D loss: 0.444480] [D acc: 0.781250] [G loss: 1.045086] [G acc: 0.250000]\n",
      "205 [D loss: 0.541511] [D acc: 0.742188] [G loss: 1.305300] [G acc: 0.039062]\n",
      "206 [D loss: 0.487319] [D acc: 0.765625] [G loss: 0.402791] [G acc: 0.929688]\n",
      "207 [D loss: 0.340102] [D acc: 0.906250] [G loss: 1.124692] [G acc: 0.304688]\n",
      "208 [D loss: 0.352470] [D acc: 0.863281] [G loss: 0.094691] [G acc: 1.000000]\n",
      "209 [D loss: 0.406635] [D acc: 0.789062] [G loss: 1.217227] [G acc: 0.210938]\n",
      "210 [D loss: 0.393521] [D acc: 0.812500] [G loss: 0.357769] [G acc: 0.859375]\n",
      "211 [D loss: 0.295709] [D acc: 0.921875] [G loss: 0.386630] [G acc: 0.820312]\n",
      "212 [D loss: 0.216462] [D acc: 0.968750] [G loss: 0.417293] [G acc: 0.812500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213 [D loss: 0.214431] [D acc: 0.917969] [G loss: 0.038530] [G acc: 1.000000]\n",
      "214 [D loss: 0.226576] [D acc: 0.910156] [G loss: 1.066945] [G acc: 0.375000]\n",
      "215 [D loss: 0.303083] [D acc: 0.867188] [G loss: 0.181660] [G acc: 0.953125]\n",
      "216 [D loss: 0.817964] [D acc: 0.589844] [G loss: 1.634877] [G acc: 0.109375]\n",
      "217 [D loss: 0.893197] [D acc: 0.609375] [G loss: 0.796394] [G acc: 0.390625]\n",
      "218 [D loss: 0.580624] [D acc: 0.695312] [G loss: 2.392367] [G acc: 0.000000]\n",
      "219 [D loss: 0.502289] [D acc: 0.812500] [G loss: 2.623125] [G acc: 0.000000]\n",
      "220 [D loss: 0.447661] [D acc: 0.828125] [G loss: 1.221624] [G acc: 0.226562]\n",
      "221 [D loss: 0.408206] [D acc: 0.800781] [G loss: 1.861094] [G acc: 0.140625]\n",
      "222 [D loss: 0.544509] [D acc: 0.738281] [G loss: 0.187615] [G acc: 0.929688]\n",
      "223 [D loss: 0.405946] [D acc: 0.835938] [G loss: 0.408015] [G acc: 0.796875]\n",
      "224 [D loss: 0.357771] [D acc: 0.855469] [G loss: 0.182491] [G acc: 0.921875]\n",
      "225 [D loss: 0.418945] [D acc: 0.816406] [G loss: 1.477808] [G acc: 0.242188]\n",
      "226 [D loss: 0.560048] [D acc: 0.722656] [G loss: 0.863392] [G acc: 0.453125]\n",
      "227 [D loss: 0.556005] [D acc: 0.699219] [G loss: 2.462966] [G acc: 0.039062]\n",
      "228 [D loss: 0.643551] [D acc: 0.660156] [G loss: 1.973711] [G acc: 0.000000]\n",
      "229 [D loss: 0.433130] [D acc: 0.859375] [G loss: 2.888677] [G acc: 0.000000]\n",
      "230 [D loss: 0.306895] [D acc: 0.941406] [G loss: 3.046721] [G acc: 0.000000]\n",
      "231 [D loss: 0.271911] [D acc: 0.929688] [G loss: 3.392790] [G acc: 0.000000]\n",
      "232 [D loss: 0.235028] [D acc: 0.914062] [G loss: 2.639666] [G acc: 0.132812]\n",
      "233 [D loss: 1.112675] [D acc: 0.625000] [G loss: 0.948879] [G acc: 0.640625]\n",
      "234 [D loss: 0.830633] [D acc: 0.652344] [G loss: 1.044932] [G acc: 0.312500]\n",
      "235 [D loss: 0.544724] [D acc: 0.699219] [G loss: 1.108801] [G acc: 0.070312]\n",
      "236 [D loss: 0.467711] [D acc: 0.832031] [G loss: 1.012829] [G acc: 0.195312]\n",
      "237 [D loss: 0.350217] [D acc: 0.898438] [G loss: 0.737100] [G acc: 0.500000]\n",
      "238 [D loss: 0.212665] [D acc: 0.917969] [G loss: 0.217374] [G acc: 1.000000]\n",
      "239 [D loss: 0.197836] [D acc: 0.953125] [G loss: 0.692960] [G acc: 0.609375]\n",
      "240 [D loss: 0.416826] [D acc: 0.839844] [G loss: 0.060382] [G acc: 1.000000]\n",
      "241 [D loss: 0.801870] [D acc: 0.527344] [G loss: 0.547568] [G acc: 0.710938]\n",
      "242 [D loss: 0.272390] [D acc: 0.898438] [G loss: 0.257012] [G acc: 0.953125]\n",
      "243 [D loss: 0.262596] [D acc: 0.929688] [G loss: 0.222765] [G acc: 0.953125]\n",
      "244 [D loss: 0.263393] [D acc: 0.929688] [G loss: 0.527946] [G acc: 0.734375]\n",
      "245 [D loss: 0.363745] [D acc: 0.832031] [G loss: 0.811342] [G acc: 0.484375]\n",
      "246 [D loss: 0.519264] [D acc: 0.738281] [G loss: 0.830049] [G acc: 0.421875]\n",
      "247 [D loss: 0.432527] [D acc: 0.863281] [G loss: 0.723881] [G acc: 0.562500]\n",
      "248 [D loss: 0.358282] [D acc: 0.816406] [G loss: 3.142804] [G acc: 0.007812]\n",
      "249 [D loss: 0.496262] [D acc: 0.777344] [G loss: 1.077644] [G acc: 0.382812]\n",
      "250 [D loss: 0.245312] [D acc: 0.929688] [G loss: 0.937773] [G acc: 0.531250]\n",
      "251 [D loss: 0.291269] [D acc: 0.875000] [G loss: 4.073112] [G acc: 0.000000]\n",
      "252 [D loss: 0.399158] [D acc: 0.828125] [G loss: 0.118981] [G acc: 0.992188]\n",
      "253 [D loss: 0.516098] [D acc: 0.660156] [G loss: 3.599724] [G acc: 0.000000]\n",
      "254 [D loss: 0.379148] [D acc: 0.855469] [G loss: 1.051653] [G acc: 0.375000]\n",
      "255 [D loss: 0.281421] [D acc: 0.882812] [G loss: 3.761011] [G acc: 0.000000]\n",
      "256 [D loss: 0.261595] [D acc: 0.890625] [G loss: 0.769912] [G acc: 0.585938]\n",
      "257 [D loss: 0.240164] [D acc: 0.910156] [G loss: 1.810259] [G acc: 0.054688]\n",
      "258 [D loss: 0.255280] [D acc: 0.917969] [G loss: 0.664139] [G acc: 0.609375]\n",
      "259 [D loss: 0.230220] [D acc: 0.941406] [G loss: 1.096335] [G acc: 0.460938]\n",
      "260 [D loss: 0.231880] [D acc: 0.910156] [G loss: 0.019157] [G acc: 1.000000]\n",
      "261 [D loss: 0.449952] [D acc: 0.750000] [G loss: 3.249483] [G acc: 0.093750]\n",
      "262 [D loss: 1.132462] [D acc: 0.582031] [G loss: 0.618893] [G acc: 0.695312]\n",
      "263 [D loss: 0.544008] [D acc: 0.714844] [G loss: 0.649474] [G acc: 0.601562]\n",
      "264 [D loss: 0.469460] [D acc: 0.800781] [G loss: 1.412889] [G acc: 0.031250]\n",
      "265 [D loss: 0.339201] [D acc: 0.886719] [G loss: 0.657057] [G acc: 0.632812]\n",
      "266 [D loss: 0.262087] [D acc: 0.910156] [G loss: 0.330392] [G acc: 0.867188]\n",
      "267 [D loss: 0.192216] [D acc: 0.957031] [G loss: 0.313626] [G acc: 0.867188]\n",
      "268 [D loss: 0.186111] [D acc: 0.929688] [G loss: 0.001263] [G acc: 1.000000]\n",
      "269 [D loss: 0.613820] [D acc: 0.671875] [G loss: 3.875249] [G acc: 0.093750]\n",
      "270 [D loss: 1.143958] [D acc: 0.609375] [G loss: 1.510396] [G acc: 0.335938]\n",
      "271 [D loss: 0.677832] [D acc: 0.644531] [G loss: 1.016828] [G acc: 0.375000]\n",
      "272 [D loss: 0.424040] [D acc: 0.847656] [G loss: 1.310068] [G acc: 0.296875]\n",
      "273 [D loss: 0.350449] [D acc: 0.878906] [G loss: 1.403371] [G acc: 0.125000]\n",
      "274 [D loss: 0.327672] [D acc: 0.898438] [G loss: 2.592144] [G acc: 0.046875]\n",
      "275 [D loss: 0.329810] [D acc: 0.851562] [G loss: 2.072992] [G acc: 0.101562]\n",
      "276 [D loss: 0.357164] [D acc: 0.835938] [G loss: 2.414659] [G acc: 0.078125]\n",
      "277 [D loss: 0.325560] [D acc: 0.894531] [G loss: 1.388140] [G acc: 0.156250]\n",
      "278 [D loss: 0.389381] [D acc: 0.832031] [G loss: 3.062339] [G acc: 0.203125]\n",
      "279 [D loss: 0.507732] [D acc: 0.824219] [G loss: 0.284569] [G acc: 1.000000]\n",
      "280 [D loss: 0.598098] [D acc: 0.648438] [G loss: 1.440667] [G acc: 0.187500]\n",
      "281 [D loss: 0.366534] [D acc: 0.851562] [G loss: 0.340432] [G acc: 0.921875]\n",
      "282 [D loss: 0.380389] [D acc: 0.824219] [G loss: 0.934188] [G acc: 0.250000]\n",
      "283 [D loss: 0.260853] [D acc: 0.906250] [G loss: 0.566495] [G acc: 0.742188]\n",
      "284 [D loss: 0.239856] [D acc: 0.933594] [G loss: 0.348640] [G acc: 0.929688]\n",
      "285 [D loss: 0.348192] [D acc: 0.835938] [G loss: 1.679954] [G acc: 0.382812]\n",
      "286 [D loss: 0.704725] [D acc: 0.738281] [G loss: 1.169260] [G acc: 0.117188]\n",
      "287 [D loss: 0.419542] [D acc: 0.824219] [G loss: 0.809424] [G acc: 0.414062]\n",
      "288 [D loss: 0.458119] [D acc: 0.765625] [G loss: 3.806769] [G acc: 0.000000]\n",
      "289 [D loss: 0.379115] [D acc: 0.863281] [G loss: 2.390514] [G acc: 0.007812]\n",
      "290 [D loss: 0.477087] [D acc: 0.718750] [G loss: 3.509612] [G acc: 0.000000]\n",
      "291 [D loss: 0.494251] [D acc: 0.773438] [G loss: 1.634290] [G acc: 0.085938]\n",
      "292 [D loss: 0.406819] [D acc: 0.820312] [G loss: 2.249697] [G acc: 0.000000]\n",
      "293 [D loss: 0.253210] [D acc: 0.910156] [G loss: 0.930723] [G acc: 0.320312]\n",
      "294 [D loss: 0.165189] [D acc: 0.968750] [G loss: 0.830828] [G acc: 0.484375]\n",
      "295 [D loss: 0.147682] [D acc: 0.949219] [G loss: 0.052657] [G acc: 0.992188]\n",
      "296 [D loss: 0.171481] [D acc: 0.937500] [G loss: 1.101299] [G acc: 0.453125]\n",
      "297 [D loss: 0.262509] [D acc: 0.890625] [G loss: 0.025881] [G acc: 1.000000]\n",
      "298 [D loss: 0.208879] [D acc: 0.941406] [G loss: 0.677906] [G acc: 0.640625]\n",
      "299 [D loss: 0.448805] [D acc: 0.812500] [G loss: 3.383574] [G acc: 0.054688]\n",
      "300 [D loss: 0.556329] [D acc: 0.765625] [G loss: 1.484533] [G acc: 0.148438]\n",
      "301 [D loss: 0.393844] [D acc: 0.839844] [G loss: 0.328819] [G acc: 0.843750]\n",
      "302 [D loss: 0.208400] [D acc: 0.941406] [G loss: 0.490653] [G acc: 0.750000]\n",
      "303 [D loss: 0.187359] [D acc: 0.949219] [G loss: 0.492127] [G acc: 0.750000]\n",
      "304 [D loss: 0.147624] [D acc: 0.972656] [G loss: 2.399686] [G acc: 0.125000]\n",
      "305 [D loss: 0.248379] [D acc: 0.894531] [G loss: 0.883081] [G acc: 0.679688]\n",
      "306 [D loss: 0.569412] [D acc: 0.730469] [G loss: 6.426158] [G acc: 0.000000]\n",
      "307 [D loss: 0.644603] [D acc: 0.750000] [G loss: 0.544674] [G acc: 0.640625]\n",
      "308 [D loss: 0.974986] [D acc: 0.609375] [G loss: 2.489577] [G acc: 0.000000]\n",
      "309 [D loss: 0.387938] [D acc: 0.839844] [G loss: 1.631483] [G acc: 0.062500]\n",
      "310 [D loss: 0.440722] [D acc: 0.773438] [G loss: 2.493039] [G acc: 0.000000]\n",
      "311 [D loss: 0.564356] [D acc: 0.718750] [G loss: 1.806246] [G acc: 0.046875]\n",
      "312 [D loss: 0.480925] [D acc: 0.781250] [G loss: 1.308053] [G acc: 0.242188]\n",
      "313 [D loss: 0.620902] [D acc: 0.675781] [G loss: 1.774080] [G acc: 0.046875]\n",
      "314 [D loss: 0.540782] [D acc: 0.742188] [G loss: 0.634454] [G acc: 0.617188]\n",
      "315 [D loss: 0.395392] [D acc: 0.867188] [G loss: 2.177895] [G acc: 0.000000]\n",
      "316 [D loss: 0.316019] [D acc: 0.898438] [G loss: 0.977950] [G acc: 0.382812]\n",
      "317 [D loss: 0.257139] [D acc: 0.914062] [G loss: 5.222457] [G acc: 0.000000]\n",
      "318 [D loss: 0.687676] [D acc: 0.718750] [G loss: 0.226575] [G acc: 0.968750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319 [D loss: 1.001462] [D acc: 0.558594] [G loss: 2.870215] [G acc: 0.007812]\n",
      "320 [D loss: 0.378907] [D acc: 0.839844] [G loss: 1.282198] [G acc: 0.210938]\n",
      "321 [D loss: 0.293276] [D acc: 0.910156] [G loss: 2.309218] [G acc: 0.000000]\n",
      "322 [D loss: 0.319772] [D acc: 0.859375] [G loss: 1.055961] [G acc: 0.445312]\n",
      "323 [D loss: 0.353332] [D acc: 0.843750] [G loss: 1.817982] [G acc: 0.109375]\n",
      "324 [D loss: 0.351565] [D acc: 0.851562] [G loss: 0.604175] [G acc: 0.718750]\n",
      "325 [D loss: 0.240682] [D acc: 0.941406] [G loss: 1.302918] [G acc: 0.250000]\n",
      "326 [D loss: 0.160443] [D acc: 0.949219] [G loss: 0.353829] [G acc: 0.851562]\n",
      "327 [D loss: 0.264002] [D acc: 0.894531] [G loss: 6.241112] [G acc: 0.000000]\n",
      "328 [D loss: 0.614136] [D acc: 0.765625] [G loss: 0.792200] [G acc: 0.507812]\n",
      "329 [D loss: 0.760728] [D acc: 0.582031] [G loss: 3.691324] [G acc: 0.000000]\n",
      "330 [D loss: 0.300875] [D acc: 0.859375] [G loss: 2.348533] [G acc: 0.000000]\n",
      "331 [D loss: 0.216351] [D acc: 0.925781] [G loss: 1.872432] [G acc: 0.109375]\n",
      "332 [D loss: 0.150404] [D acc: 0.957031] [G loss: 0.720119] [G acc: 0.601562]\n",
      "333 [D loss: 0.182316] [D acc: 0.941406] [G loss: 0.131208] [G acc: 0.960938]\n",
      "334 [D loss: 0.191660] [D acc: 0.925781] [G loss: 0.081141] [G acc: 0.992188]\n",
      "335 [D loss: 0.141383] [D acc: 0.941406] [G loss: 0.150240] [G acc: 0.968750]\n",
      "336 [D loss: 0.135194] [D acc: 0.945312] [G loss: 0.006155] [G acc: 1.000000]\n",
      "337 [D loss: 0.220683] [D acc: 0.902344] [G loss: 2.038448] [G acc: 0.226562]\n",
      "338 [D loss: 0.752033] [D acc: 0.667969] [G loss: 1.302478] [G acc: 0.421875]\n",
      "339 [D loss: 0.747597] [D acc: 0.632812] [G loss: 1.593097] [G acc: 0.156250]\n",
      "340 [D loss: 0.334874] [D acc: 0.894531] [G loss: 0.566953] [G acc: 0.617188]\n",
      "341 [D loss: 0.218635] [D acc: 0.921875] [G loss: 0.136744] [G acc: 0.992188]\n",
      "342 [D loss: 0.152707] [D acc: 0.957031] [G loss: 0.332967] [G acc: 0.812500]\n",
      "343 [D loss: 0.149265] [D acc: 0.957031] [G loss: 0.188693] [G acc: 0.921875]\n",
      "344 [D loss: 0.136232] [D acc: 0.972656] [G loss: 1.594940] [G acc: 0.335938]\n",
      "345 [D loss: 0.186901] [D acc: 0.933594] [G loss: 0.279807] [G acc: 0.898438]\n",
      "346 [D loss: 0.091972] [D acc: 0.980469] [G loss: 0.612005] [G acc: 0.648438]\n",
      "347 [D loss: 0.408806] [D acc: 0.812500] [G loss: 7.346283] [G acc: 0.000000]\n",
      "348 [D loss: 1.169619] [D acc: 0.644531] [G loss: 1.246368] [G acc: 0.281250]\n",
      "349 [D loss: 0.567942] [D acc: 0.656250] [G loss: 1.570504] [G acc: 0.218750]\n",
      "350 [D loss: 0.364575] [D acc: 0.867188] [G loss: 2.768174] [G acc: 0.000000]\n",
      "351 [D loss: 0.302967] [D acc: 0.906250] [G loss: 2.190212] [G acc: 0.039062]\n",
      "352 [D loss: 0.350890] [D acc: 0.839844] [G loss: 2.109656] [G acc: 0.046875]\n",
      "353 [D loss: 0.504012] [D acc: 0.781250] [G loss: 1.715934] [G acc: 0.031250]\n",
      "354 [D loss: 0.392710] [D acc: 0.828125] [G loss: 1.531105] [G acc: 0.070312]\n",
      "355 [D loss: 0.326535] [D acc: 0.886719] [G loss: 1.052837] [G acc: 0.273438]\n",
      "356 [D loss: 0.298633] [D acc: 0.886719] [G loss: 1.203875] [G acc: 0.390625]\n",
      "357 [D loss: 1.212066] [D acc: 0.609375] [G loss: 1.528677] [G acc: 0.132812]\n",
      "358 [D loss: 0.553901] [D acc: 0.730469] [G loss: 0.132140] [G acc: 1.000000]\n",
      "359 [D loss: 0.591390] [D acc: 0.597656] [G loss: 1.820308] [G acc: 0.000000]\n",
      "360 [D loss: 0.394232] [D acc: 0.796875] [G loss: 0.505653] [G acc: 0.734375]\n",
      "361 [D loss: 0.271306] [D acc: 0.941406] [G loss: 0.894344] [G acc: 0.476562]\n",
      "362 [D loss: 0.202284] [D acc: 0.937500] [G loss: 0.409206] [G acc: 0.851562]\n",
      "363 [D loss: 0.165811] [D acc: 0.972656] [G loss: 1.220575] [G acc: 0.398438]\n",
      "364 [D loss: 0.233368] [D acc: 0.917969] [G loss: 0.021148] [G acc: 1.000000]\n",
      "365 [D loss: 0.712594] [D acc: 0.605469] [G loss: 4.448046] [G acc: 0.000000]\n",
      "366 [D loss: 0.685646] [D acc: 0.730469] [G loss: 0.563073] [G acc: 0.710938]\n",
      "367 [D loss: 1.221364] [D acc: 0.679688] [G loss: 1.648214] [G acc: 0.031250]\n",
      "368 [D loss: 0.719397] [D acc: 0.605469] [G loss: 0.750983] [G acc: 0.484375]\n",
      "369 [D loss: 0.608877] [D acc: 0.699219] [G loss: 0.992133] [G acc: 0.210938]\n",
      "370 [D loss: 0.459363] [D acc: 0.839844] [G loss: 1.798389] [G acc: 0.015625]\n",
      "371 [D loss: 0.468942] [D acc: 0.800781] [G loss: 1.920510] [G acc: 0.039062]\n",
      "372 [D loss: 0.413813] [D acc: 0.832031] [G loss: 2.193417] [G acc: 0.023438]\n",
      "373 [D loss: 0.288331] [D acc: 0.910156] [G loss: 4.070454] [G acc: 0.000000]\n",
      "374 [D loss: 0.282184] [D acc: 0.878906] [G loss: 1.299374] [G acc: 0.187500]\n",
      "375 [D loss: 0.318423] [D acc: 0.898438] [G loss: 6.126540] [G acc: 0.000000]\n",
      "376 [D loss: 0.448632] [D acc: 0.859375] [G loss: 1.782058] [G acc: 0.140625]\n",
      "377 [D loss: 0.428145] [D acc: 0.781250] [G loss: 3.249687] [G acc: 0.007812]\n",
      "378 [D loss: 0.400563] [D acc: 0.875000] [G loss: 1.519341] [G acc: 0.109375]\n",
      "379 [D loss: 0.814796] [D acc: 0.640625] [G loss: 2.576690] [G acc: 0.000000]\n",
      "380 [D loss: 0.385873] [D acc: 0.804688] [G loss: 0.621445] [G acc: 0.679688]\n",
      "381 [D loss: 0.252166] [D acc: 0.976562] [G loss: 0.848838] [G acc: 0.429688]\n",
      "382 [D loss: 0.175643] [D acc: 0.933594] [G loss: 0.302203] [G acc: 0.921875]\n",
      "383 [D loss: 0.229125] [D acc: 0.929688] [G loss: 1.866116] [G acc: 0.156250]\n",
      "384 [D loss: 0.192403] [D acc: 0.925781] [G loss: 0.335724] [G acc: 0.835938]\n",
      "385 [D loss: 0.163353] [D acc: 0.976562] [G loss: 1.117272] [G acc: 0.320312]\n",
      "386 [D loss: 0.151234] [D acc: 0.941406] [G loss: 0.083063] [G acc: 1.000000]\n",
      "387 [D loss: 0.169835] [D acc: 0.957031] [G loss: 1.030563] [G acc: 0.484375]\n",
      "388 [D loss: 0.177584] [D acc: 0.949219] [G loss: 0.535926] [G acc: 0.757812]\n",
      "389 [D loss: 0.187995] [D acc: 0.945312] [G loss: 2.289787] [G acc: 0.210938]\n",
      "390 [D loss: 0.251063] [D acc: 0.898438] [G loss: 0.206611] [G acc: 0.929688]\n",
      "391 [D loss: 0.516641] [D acc: 0.746094] [G loss: 6.066998] [G acc: 0.000000]\n",
      "392 [D loss: 1.089448] [D acc: 0.632812] [G loss: 0.433158] [G acc: 0.781250]\n",
      "393 [D loss: 0.691188] [D acc: 0.585938] [G loss: 1.281227] [G acc: 0.085938]\n",
      "394 [D loss: 0.427847] [D acc: 0.859375] [G loss: 1.224955] [G acc: 0.195312]\n",
      "395 [D loss: 0.397490] [D acc: 0.851562] [G loss: 1.470564] [G acc: 0.148438]\n",
      "396 [D loss: 0.378826] [D acc: 0.851562] [G loss: 1.300335] [G acc: 0.218750]\n",
      "397 [D loss: 0.324317] [D acc: 0.867188] [G loss: 1.513165] [G acc: 0.148438]\n",
      "398 [D loss: 0.247147] [D acc: 0.929688] [G loss: 0.487531] [G acc: 0.718750]\n",
      "399 [D loss: 0.185068] [D acc: 0.949219] [G loss: 1.105409] [G acc: 0.390625]\n",
      "400 [D loss: 0.373158] [D acc: 0.832031] [G loss: 1.675438] [G acc: 0.390625]\n",
      "401 [D loss: 2.132408] [D acc: 0.585938] [G loss: 1.337998] [G acc: 0.195312]\n",
      "402 [D loss: 0.680170] [D acc: 0.710938] [G loss: 0.220168] [G acc: 0.945312]\n",
      "403 [D loss: 0.608375] [D acc: 0.644531] [G loss: 1.426659] [G acc: 0.117188]\n",
      "404 [D loss: 0.507131] [D acc: 0.753906] [G loss: 0.494213] [G acc: 0.757812]\n",
      "405 [D loss: 0.366495] [D acc: 0.882812] [G loss: 0.905197] [G acc: 0.492188]\n",
      "406 [D loss: 0.335022] [D acc: 0.878906] [G loss: 0.255865] [G acc: 0.890625]\n",
      "407 [D loss: 0.276803] [D acc: 0.941406] [G loss: 1.230940] [G acc: 0.328125]\n",
      "408 [D loss: 0.198140] [D acc: 0.941406] [G loss: 0.307474] [G acc: 0.859375]\n",
      "409 [D loss: 0.209448] [D acc: 0.937500] [G loss: 1.393671] [G acc: 0.218750]\n",
      "410 [D loss: 0.300305] [D acc: 0.875000] [G loss: 0.279760] [G acc: 0.882812]\n",
      "411 [D loss: 0.162984] [D acc: 0.949219] [G loss: 3.091467] [G acc: 0.000000]\n",
      "412 [D loss: 0.164998] [D acc: 0.929688] [G loss: 0.267138] [G acc: 0.882812]\n",
      "413 [D loss: 0.170014] [D acc: 0.949219] [G loss: 2.787829] [G acc: 0.007812]\n",
      "414 [D loss: 0.135330] [D acc: 0.941406] [G loss: 1.023975] [G acc: 0.500000]\n",
      "415 [D loss: 0.274739] [D acc: 0.878906] [G loss: 5.883595] [G acc: 0.000000]\n",
      "416 [D loss: 0.592800] [D acc: 0.820312] [G loss: 1.843885] [G acc: 0.195312]\n",
      "417 [D loss: 0.140204] [D acc: 0.964844] [G loss: 1.105993] [G acc: 0.414062]\n",
      "418 [D loss: 0.178505] [D acc: 0.941406] [G loss: 0.247550] [G acc: 0.890625]\n",
      "419 [D loss: 0.093799] [D acc: 0.972656] [G loss: 0.456631] [G acc: 0.851562]\n",
      "420 [D loss: 0.242710] [D acc: 0.894531] [G loss: 5.947358] [G acc: 0.000000]\n",
      "421 [D loss: 0.499235] [D acc: 0.832031] [G loss: 1.489933] [G acc: 0.242188]\n",
      "422 [D loss: 0.504108] [D acc: 0.785156] [G loss: 3.157947] [G acc: 0.000000]\n",
      "423 [D loss: 0.541140] [D acc: 0.792969] [G loss: 1.414779] [G acc: 0.125000]\n",
      "424 [D loss: 0.368213] [D acc: 0.847656] [G loss: 1.375878] [G acc: 0.171875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425 [D loss: 0.345954] [D acc: 0.847656] [G loss: 1.027698] [G acc: 0.320312]\n",
      "426 [D loss: 0.236171] [D acc: 0.906250] [G loss: 0.448572] [G acc: 0.742188]\n",
      "427 [D loss: 0.149729] [D acc: 0.960938] [G loss: 0.518831] [G acc: 0.718750]\n",
      "428 [D loss: 0.105336] [D acc: 0.964844] [G loss: 0.475542] [G acc: 0.781250]\n",
      "429 [D loss: 0.074312] [D acc: 0.972656] [G loss: 0.066338] [G acc: 0.984375]\n",
      "430 [D loss: 0.065198] [D acc: 0.980469] [G loss: 0.325186] [G acc: 0.898438]\n",
      "431 [D loss: 0.091200] [D acc: 0.976562] [G loss: 0.531951] [G acc: 0.742188]\n",
      "432 [D loss: 0.089016] [D acc: 0.972656] [G loss: 0.020336] [G acc: 1.000000]\n",
      "433 [D loss: 0.084389] [D acc: 0.984375] [G loss: 0.514958] [G acc: 0.773438]\n",
      "434 [D loss: 0.170712] [D acc: 0.949219] [G loss: 0.287795] [G acc: 0.875000]\n",
      "435 [D loss: 0.133006] [D acc: 0.960938] [G loss: 0.198962] [G acc: 0.914062]\n",
      "436 [D loss: 0.163617] [D acc: 0.937500] [G loss: 1.157506] [G acc: 0.476562]\n",
      "437 [D loss: 0.427610] [D acc: 0.812500] [G loss: 6.832118] [G acc: 0.000000]\n",
      "438 [D loss: 0.613563] [D acc: 0.800781] [G loss: 0.064193] [G acc: 0.968750]\n",
      "439 [D loss: 0.560604] [D acc: 0.703125] [G loss: 1.327252] [G acc: 0.257812]\n",
      "440 [D loss: 0.335766] [D acc: 0.878906] [G loss: 1.066416] [G acc: 0.406250]\n",
      "441 [D loss: 0.103285] [D acc: 0.980469] [G loss: 0.655169] [G acc: 0.609375]\n",
      "442 [D loss: 0.294971] [D acc: 0.847656] [G loss: 3.092899] [G acc: 0.023438]\n",
      "443 [D loss: 0.489556] [D acc: 0.835938] [G loss: 0.857234] [G acc: 0.335938]\n",
      "444 [D loss: 0.223050] [D acc: 0.921875] [G loss: 1.375888] [G acc: 0.023438]\n",
      "445 [D loss: 0.184980] [D acc: 0.933594] [G loss: 0.427429] [G acc: 0.781250]\n",
      "446 [D loss: 0.554628] [D acc: 0.765625] [G loss: 3.783848] [G acc: 0.000000]\n",
      "447 [D loss: 0.601655] [D acc: 0.742188] [G loss: 1.292002] [G acc: 0.039062]\n",
      "448 [D loss: 0.198619] [D acc: 0.960938] [G loss: 1.512761] [G acc: 0.031250]\n",
      "449 [D loss: 0.155664] [D acc: 0.980469] [G loss: 2.003149] [G acc: 0.015625]\n",
      "450 [D loss: 0.132679] [D acc: 0.960938] [G loss: 1.799273] [G acc: 0.078125]\n",
      "451 [D loss: 0.107278] [D acc: 0.964844] [G loss: 0.539507] [G acc: 0.726562]\n",
      "452 [D loss: 0.240201] [D acc: 0.921875] [G loss: 4.603037] [G acc: 0.031250]\n",
      "453 [D loss: 0.588568] [D acc: 0.789062] [G loss: 1.006396] [G acc: 0.328125]\n",
      "454 [D loss: 0.748685] [D acc: 0.675781] [G loss: 4.005258] [G acc: 0.054688]\n",
      "455 [D loss: 1.047850] [D acc: 0.695312] [G loss: 0.273041] [G acc: 0.835938]\n",
      "456 [D loss: 0.405860] [D acc: 0.828125] [G loss: 0.449880] [G acc: 0.742188]\n",
      "457 [D loss: 0.303110] [D acc: 0.894531] [G loss: 0.631142] [G acc: 0.640625]\n",
      "458 [D loss: 0.269970] [D acc: 0.894531] [G loss: 0.063624] [G acc: 0.984375]\n",
      "459 [D loss: 0.278299] [D acc: 0.882812] [G loss: 3.332154] [G acc: 0.023438]\n",
      "460 [D loss: 0.657472] [D acc: 0.734375] [G loss: 0.131077] [G acc: 0.953125]\n",
      "461 [D loss: 0.281105] [D acc: 0.957031] [G loss: 0.136368] [G acc: 0.953125]\n",
      "462 [D loss: 0.420793] [D acc: 0.820312] [G loss: 2.570466] [G acc: 0.070312]\n",
      "463 [D loss: 0.376732] [D acc: 0.828125] [G loss: 0.547000] [G acc: 0.742188]\n",
      "464 [D loss: 0.274605] [D acc: 0.898438] [G loss: 1.762474] [G acc: 0.226562]\n",
      "465 [D loss: 0.312885] [D acc: 0.875000] [G loss: 0.214774] [G acc: 0.929688]\n",
      "466 [D loss: 0.616616] [D acc: 0.687500] [G loss: 3.447138] [G acc: 0.000000]\n",
      "467 [D loss: 0.474155] [D acc: 0.777344] [G loss: 2.181191] [G acc: 0.015625]\n",
      "468 [D loss: 0.515362] [D acc: 0.769531] [G loss: 3.148597] [G acc: 0.000000]\n",
      "469 [D loss: 0.369746] [D acc: 0.832031] [G loss: 2.375455] [G acc: 0.015625]\n",
      "470 [D loss: 0.318617] [D acc: 0.878906] [G loss: 2.262100] [G acc: 0.031250]\n",
      "471 [D loss: 0.248138] [D acc: 0.929688] [G loss: 1.287970] [G acc: 0.140625]\n",
      "472 [D loss: 0.230623] [D acc: 0.921875] [G loss: 1.904619] [G acc: 0.125000]\n",
      "473 [D loss: 0.215919] [D acc: 0.941406] [G loss: 0.381059] [G acc: 0.828125]\n",
      "474 [D loss: 0.208091] [D acc: 0.933594] [G loss: 1.433676] [G acc: 0.250000]\n",
      "475 [D loss: 0.426355] [D acc: 0.835938] [G loss: 0.049011] [G acc: 0.992188]\n",
      "476 [D loss: 0.420733] [D acc: 0.765625] [G loss: 2.577915] [G acc: 0.031250]\n",
      "477 [D loss: 0.380144] [D acc: 0.824219] [G loss: 0.090169] [G acc: 1.000000]\n",
      "478 [D loss: 0.242283] [D acc: 0.921875] [G loss: 0.258433] [G acc: 0.914062]\n",
      "479 [D loss: 0.310845] [D acc: 0.863281] [G loss: 1.076818] [G acc: 0.398438]\n",
      "480 [D loss: 0.272104] [D acc: 0.910156] [G loss: 0.234022] [G acc: 0.937500]\n",
      "481 [D loss: 0.268411] [D acc: 0.929688] [G loss: 3.179647] [G acc: 0.023438]\n",
      "482 [D loss: 0.283936] [D acc: 0.882812] [G loss: 0.335688] [G acc: 0.859375]\n",
      "483 [D loss: 0.305894] [D acc: 0.875000] [G loss: 6.319691] [G acc: 0.000000]\n",
      "484 [D loss: 0.893561] [D acc: 0.632812] [G loss: 0.713889] [G acc: 0.632812]\n",
      "485 [D loss: 0.558630] [D acc: 0.699219] [G loss: 2.330375] [G acc: 0.054688]\n",
      "486 [D loss: 0.422829] [D acc: 0.812500] [G loss: 1.108856] [G acc: 0.328125]\n",
      "487 [D loss: 0.282396] [D acc: 0.914062] [G loss: 3.611685] [G acc: 0.000000]\n",
      "488 [D loss: 0.184656] [D acc: 0.937500] [G loss: 1.638771] [G acc: 0.226562]\n",
      "489 [D loss: 0.164347] [D acc: 0.949219] [G loss: 2.693544] [G acc: 0.039062]\n",
      "490 [D loss: 0.221107] [D acc: 0.921875] [G loss: 2.739684] [G acc: 0.054688]\n",
      "491 [D loss: 0.188865] [D acc: 0.937500] [G loss: 2.374898] [G acc: 0.132812]\n",
      "492 [D loss: 0.419644] [D acc: 0.804688] [G loss: 6.644737] [G acc: 0.000000]\n",
      "493 [D loss: 1.102078] [D acc: 0.621094] [G loss: 0.843861] [G acc: 0.421875]\n",
      "494 [D loss: 0.417216] [D acc: 0.808594] [G loss: 3.027624] [G acc: 0.000000]\n",
      "495 [D loss: 0.227708] [D acc: 0.929688] [G loss: 2.339223] [G acc: 0.007812]\n",
      "496 [D loss: 0.207775] [D acc: 0.945312] [G loss: 3.563249] [G acc: 0.000000]\n",
      "497 [D loss: 0.174201] [D acc: 0.929688] [G loss: 1.918318] [G acc: 0.109375]\n",
      "498 [D loss: 0.149341] [D acc: 0.957031] [G loss: 2.811523] [G acc: 0.015625]\n",
      "499 [D loss: 0.152650] [D acc: 0.945312] [G loss: 1.726751] [G acc: 0.203125]\n",
      "500 [D loss: 0.153184] [D acc: 0.941406] [G loss: 3.730077] [G acc: 0.000000]\n",
      "501 [D loss: 0.292209] [D acc: 0.867188] [G loss: 0.066783] [G acc: 0.992188]\n",
      "502 [D loss: 1.858038] [D acc: 0.523438] [G loss: 2.401845] [G acc: 0.093750]\n",
      "503 [D loss: 0.502335] [D acc: 0.785156] [G loss: 0.599596] [G acc: 0.632812]\n",
      "504 [D loss: 0.430115] [D acc: 0.800781] [G loss: 2.848375] [G acc: 0.000000]\n",
      "505 [D loss: 0.497671] [D acc: 0.757812] [G loss: 0.092745] [G acc: 0.968750]\n",
      "506 [D loss: 0.326482] [D acc: 0.894531] [G loss: 0.529358] [G acc: 0.703125]\n",
      "507 [D loss: 0.300194] [D acc: 0.859375] [G loss: 0.078817] [G acc: 1.000000]\n",
      "508 [D loss: 0.263176] [D acc: 0.933594] [G loss: 2.093274] [G acc: 0.015625]\n",
      "509 [D loss: 0.354912] [D acc: 0.832031] [G loss: 0.369905] [G acc: 0.828125]\n",
      "510 [D loss: 0.227827] [D acc: 0.941406] [G loss: 1.405803] [G acc: 0.187500]\n",
      "511 [D loss: 0.210574] [D acc: 0.917969] [G loss: 0.261346] [G acc: 0.914062]\n",
      "512 [D loss: 0.156228] [D acc: 0.960938] [G loss: 0.601769] [G acc: 0.664062]\n",
      "513 [D loss: 0.142329] [D acc: 0.949219] [G loss: 1.168497] [G acc: 0.257812]\n",
      "514 [D loss: 0.136521] [D acc: 0.988281] [G loss: 3.549095] [G acc: 0.101562]\n",
      "515 [D loss: 0.655729] [D acc: 0.769531] [G loss: 5.828383] [G acc: 0.000000]\n",
      "516 [D loss: 0.322922] [D acc: 0.882812] [G loss: 3.234109] [G acc: 0.007812]\n",
      "517 [D loss: 0.185143] [D acc: 0.945312] [G loss: 4.806341] [G acc: 0.007812]\n",
      "518 [D loss: 0.122161] [D acc: 0.992188] [G loss: 4.102333] [G acc: 0.000000]\n",
      "519 [D loss: 0.198138] [D acc: 0.929688] [G loss: 0.287774] [G acc: 0.914062]\n",
      "520 [D loss: 0.539976] [D acc: 0.648438] [G loss: 3.836421] [G acc: 0.000000]\n",
      "521 [D loss: 0.574751] [D acc: 0.761719] [G loss: 0.459951] [G acc: 0.820312]\n",
      "522 [D loss: 0.376935] [D acc: 0.804688] [G loss: 1.726356] [G acc: 0.226562]\n",
      "523 [D loss: 0.188124] [D acc: 0.937500] [G loss: 0.232662] [G acc: 0.937500]\n",
      "524 [D loss: 0.159083] [D acc: 0.957031] [G loss: 0.099585] [G acc: 0.984375]\n",
      "525 [D loss: 0.186020] [D acc: 0.949219] [G loss: 0.145106] [G acc: 0.984375]\n",
      "526 [D loss: 0.170941] [D acc: 0.953125] [G loss: 0.203242] [G acc: 0.945312]\n",
      "527 [D loss: 0.120735] [D acc: 0.980469] [G loss: 0.480447] [G acc: 0.796875]\n",
      "528 [D loss: 0.130186] [D acc: 0.972656] [G loss: 0.269880] [G acc: 0.890625]\n",
      "529 [D loss: 0.104864] [D acc: 0.980469] [G loss: 2.089440] [G acc: 0.203125]\n",
      "530 [D loss: 0.175337] [D acc: 0.933594] [G loss: 0.406851] [G acc: 0.789062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531 [D loss: 0.114035] [D acc: 0.972656] [G loss: 0.305736] [G acc: 0.875000]\n",
      "532 [D loss: 0.114174] [D acc: 0.957031] [G loss: 1.024525] [G acc: 0.523438]\n",
      "533 [D loss: 0.061553] [D acc: 0.992188] [G loss: 0.752596] [G acc: 0.609375]\n",
      "534 [D loss: 0.117497] [D acc: 0.964844] [G loss: 0.283955] [G acc: 0.882812]\n",
      "535 [D loss: 0.141381] [D acc: 0.941406] [G loss: 12.344214] [G acc: 0.000000]\n",
      "536 [D loss: 0.913648] [D acc: 0.726562] [G loss: 0.128462] [G acc: 0.976562]\n",
      "537 [D loss: 0.899084] [D acc: 0.589844] [G loss: 4.163315] [G acc: 0.000000]\n",
      "538 [D loss: 0.465835] [D acc: 0.796875] [G loss: 1.031731] [G acc: 0.390625]\n",
      "539 [D loss: 0.536014] [D acc: 0.718750] [G loss: 3.699102] [G acc: 0.007812]\n",
      "540 [D loss: 0.311904] [D acc: 0.878906] [G loss: 1.737850] [G acc: 0.140625]\n",
      "541 [D loss: 0.226580] [D acc: 0.933594] [G loss: 1.430021] [G acc: 0.132812]\n",
      "542 [D loss: 0.182167] [D acc: 0.941406] [G loss: 0.388126] [G acc: 0.812500]\n",
      "543 [D loss: 0.118016] [D acc: 0.960938] [G loss: 0.116740] [G acc: 0.992188]\n",
      "544 [D loss: 0.095125] [D acc: 0.968750] [G loss: 0.038367] [G acc: 1.000000]\n",
      "545 [D loss: 0.160030] [D acc: 0.929688] [G loss: 0.631055] [G acc: 0.640625]\n",
      "546 [D loss: 0.279414] [D acc: 0.882812] [G loss: 0.013303] [G acc: 1.000000]\n",
      "547 [D loss: 0.217450] [D acc: 0.906250] [G loss: 0.393151] [G acc: 0.843750]\n",
      "548 [D loss: 0.215384] [D acc: 0.910156] [G loss: 0.081827] [G acc: 1.000000]\n",
      "549 [D loss: 0.102864] [D acc: 0.960938] [G loss: 0.111518] [G acc: 0.992188]\n",
      "550 [D loss: 0.098558] [D acc: 0.957031] [G loss: 0.062702] [G acc: 0.992188]\n",
      "551 [D loss: 0.112822] [D acc: 0.968750] [G loss: 0.598820] [G acc: 0.656250]\n",
      "552 [D loss: 0.120027] [D acc: 0.960938] [G loss: 0.120152] [G acc: 0.992188]\n",
      "553 [D loss: 0.120724] [D acc: 0.953125] [G loss: 0.178363] [G acc: 0.929688]\n",
      "554 [D loss: 0.079965] [D acc: 0.980469] [G loss: 0.538718] [G acc: 0.710938]\n",
      "555 [D loss: 0.144874] [D acc: 0.957031] [G loss: 0.495200] [G acc: 0.734375]\n",
      "556 [D loss: 0.140810] [D acc: 0.945312] [G loss: 1.120133] [G acc: 0.601562]\n",
      "557 [D loss: 0.494780] [D acc: 0.789062] [G loss: 5.985345] [G acc: 0.000000]\n",
      "558 [D loss: 1.588109] [D acc: 0.585938] [G loss: 0.397995] [G acc: 0.812500]\n",
      "559 [D loss: 0.500406] [D acc: 0.695312] [G loss: 0.773772] [G acc: 0.468750]\n",
      "560 [D loss: 0.364461] [D acc: 0.816406] [G loss: 1.051979] [G acc: 0.351562]\n",
      "561 [D loss: 0.580471] [D acc: 0.730469] [G loss: 2.141036] [G acc: 0.000000]\n",
      "562 [D loss: 0.352831] [D acc: 0.855469] [G loss: 1.590652] [G acc: 0.015625]\n",
      "563 [D loss: 0.386822] [D acc: 0.808594] [G loss: 1.652594] [G acc: 0.070312]\n",
      "564 [D loss: 0.320107] [D acc: 0.851562] [G loss: 0.801832] [G acc: 0.500000]\n",
      "565 [D loss: 0.298777] [D acc: 0.867188] [G loss: 0.736803] [G acc: 0.492188]\n",
      "566 [D loss: 0.231426] [D acc: 0.941406] [G loss: 0.401706] [G acc: 0.843750]\n",
      "567 [D loss: 0.359297] [D acc: 0.863281] [G loss: 1.345906] [G acc: 0.148438]\n",
      "568 [D loss: 0.440057] [D acc: 0.781250] [G loss: 0.209884] [G acc: 0.929688]\n",
      "569 [D loss: 0.651617] [D acc: 0.742188] [G loss: 1.654356] [G acc: 0.156250]\n",
      "570 [D loss: 0.638093] [D acc: 0.695312] [G loss: 0.974053] [G acc: 0.429688]\n",
      "571 [D loss: 0.454679] [D acc: 0.800781] [G loss: 0.806660] [G acc: 0.531250]\n",
      "572 [D loss: 0.380185] [D acc: 0.832031] [G loss: 1.428287] [G acc: 0.359375]\n",
      "573 [D loss: 0.215411] [D acc: 0.929688] [G loss: 0.708520] [G acc: 0.640625]\n",
      "574 [D loss: 0.139777] [D acc: 0.953125] [G loss: 0.531304] [G acc: 0.742188]\n",
      "575 [D loss: 0.067057] [D acc: 0.988281] [G loss: 1.111429] [G acc: 0.492188]\n",
      "576 [D loss: 0.034480] [D acc: 0.996094] [G loss: 0.125037] [G acc: 0.953125]\n",
      "577 [D loss: 0.073113] [D acc: 0.964844] [G loss: 5.743368] [G acc: 0.000000]\n",
      "578 [D loss: 0.088732] [D acc: 0.960938] [G loss: 2.482157] [G acc: 0.234375]\n",
      "579 [D loss: 0.692667] [D acc: 0.832031] [G loss: 0.000088] [G acc: 1.000000]\n",
      "580 [D loss: 1.457033] [D acc: 0.574219] [G loss: 2.006200] [G acc: 0.039062]\n",
      "581 [D loss: 0.426317] [D acc: 0.792969] [G loss: 0.977143] [G acc: 0.257812]\n",
      "582 [D loss: 0.263847] [D acc: 0.921875] [G loss: 1.783978] [G acc: 0.085938]\n",
      "583 [D loss: 0.194976] [D acc: 0.921875] [G loss: 1.605577] [G acc: 0.039062]\n",
      "584 [D loss: 0.161415] [D acc: 0.941406] [G loss: 1.853297] [G acc: 0.093750]\n",
      "585 [D loss: 0.118994] [D acc: 0.972656] [G loss: 1.306250] [G acc: 0.367188]\n",
      "586 [D loss: 0.065737] [D acc: 0.976562] [G loss: 0.425406] [G acc: 0.773438]\n",
      "587 [D loss: 0.106992] [D acc: 0.976562] [G loss: 0.156835] [G acc: 0.914062]\n",
      "588 [D loss: 0.145191] [D acc: 0.949219] [G loss: 1.194400] [G acc: 0.468750]\n",
      "589 [D loss: 0.512951] [D acc: 0.816406] [G loss: 0.808032] [G acc: 0.554688]\n",
      "590 [D loss: 0.094244] [D acc: 0.960938] [G loss: 0.089499] [G acc: 1.000000]\n",
      "591 [D loss: 0.087491] [D acc: 0.984375] [G loss: 0.342385] [G acc: 0.875000]\n",
      "592 [D loss: 0.087125] [D acc: 0.980469] [G loss: 0.461557] [G acc: 0.796875]\n",
      "593 [D loss: 0.066770] [D acc: 0.980469] [G loss: 0.330835] [G acc: 0.898438]\n",
      "594 [D loss: 0.062907] [D acc: 0.980469] [G loss: 0.074834] [G acc: 1.000000]\n",
      "595 [D loss: 0.117863] [D acc: 0.949219] [G loss: 2.685291] [G acc: 0.000000]\n",
      "596 [D loss: 0.394875] [D acc: 0.859375] [G loss: 5.169435] [G acc: 0.000000]\n",
      "597 [D loss: 0.132767] [D acc: 0.945312] [G loss: 2.371492] [G acc: 0.109375]\n",
      "598 [D loss: 0.237722] [D acc: 0.910156] [G loss: 2.411434] [G acc: 0.148438]\n",
      "599 [D loss: 0.559032] [D acc: 0.777344] [G loss: 0.832878] [G acc: 0.445312]\n",
      "600 [D loss: 0.256535] [D acc: 0.929688] [G loss: 0.782425] [G acc: 0.546875]\n",
      "601 [D loss: 0.217107] [D acc: 0.925781] [G loss: 0.117954] [G acc: 0.976562]\n",
      "602 [D loss: 0.205546] [D acc: 0.906250] [G loss: 2.797899] [G acc: 0.007812]\n",
      "603 [D loss: 0.360113] [D acc: 0.847656] [G loss: 3.838341] [G acc: 0.000000]\n",
      "604 [D loss: 0.395840] [D acc: 0.828125] [G loss: 0.879665] [G acc: 0.492188]\n",
      "605 [D loss: 0.357128] [D acc: 0.796875] [G loss: 4.235999] [G acc: 0.007812]\n",
      "606 [D loss: 0.351249] [D acc: 0.832031] [G loss: 0.440536] [G acc: 0.742188]\n",
      "607 [D loss: 0.193137] [D acc: 0.945312] [G loss: 0.340900] [G acc: 0.843750]\n",
      "608 [D loss: 0.181342] [D acc: 0.945312] [G loss: 0.229112] [G acc: 0.890625]\n",
      "609 [D loss: 0.118912] [D acc: 0.957031] [G loss: 0.263646] [G acc: 0.882812]\n",
      "610 [D loss: 0.305568] [D acc: 0.867188] [G loss: 4.446329] [G acc: 0.000000]\n",
      "611 [D loss: 0.650087] [D acc: 0.722656] [G loss: 0.741810] [G acc: 0.585938]\n",
      "612 [D loss: 0.355812] [D acc: 0.855469] [G loss: 1.036003] [G acc: 0.414062]\n",
      "613 [D loss: 0.227812] [D acc: 0.921875] [G loss: 0.450530] [G acc: 0.765625]\n",
      "614 [D loss: 0.175045] [D acc: 0.953125] [G loss: 0.192403] [G acc: 0.914062]\n",
      "615 [D loss: 0.224430] [D acc: 0.902344] [G loss: 1.463850] [G acc: 0.218750]\n",
      "616 [D loss: 0.408326] [D acc: 0.781250] [G loss: 1.098727] [G acc: 0.484375]\n",
      "617 [D loss: 0.482163] [D acc: 0.804688] [G loss: 1.002090] [G acc: 0.398438]\n",
      "618 [D loss: 0.170089] [D acc: 0.949219] [G loss: 1.908365] [G acc: 0.109375]\n",
      "619 [D loss: 0.189733] [D acc: 0.925781] [G loss: 0.637788] [G acc: 0.671875]\n",
      "620 [D loss: 0.284490] [D acc: 0.875000] [G loss: 3.866346] [G acc: 0.007812]\n",
      "621 [D loss: 0.658325] [D acc: 0.652344] [G loss: 2.551529] [G acc: 0.015625]\n",
      "622 [D loss: 0.311888] [D acc: 0.878906] [G loss: 1.476399] [G acc: 0.187500]\n",
      "623 [D loss: 0.345227] [D acc: 0.839844] [G loss: 5.464768] [G acc: 0.000000]\n",
      "624 [D loss: 0.483054] [D acc: 0.847656] [G loss: 0.860466] [G acc: 0.421875]\n",
      "625 [D loss: 0.473891] [D acc: 0.738281] [G loss: 2.279191] [G acc: 0.000000]\n",
      "626 [D loss: 0.352945] [D acc: 0.832031] [G loss: 1.522522] [G acc: 0.007812]\n",
      "627 [D loss: 0.337894] [D acc: 0.851562] [G loss: 2.463502] [G acc: 0.000000]\n",
      "628 [D loss: 0.347923] [D acc: 0.835938] [G loss: 2.042090] [G acc: 0.000000]\n",
      "629 [D loss: 0.222753] [D acc: 0.921875] [G loss: 2.169651] [G acc: 0.007812]\n",
      "630 [D loss: 0.190222] [D acc: 0.937500] [G loss: 2.663221] [G acc: 0.007812]\n",
      "631 [D loss: 0.102288] [D acc: 0.968750] [G loss: 0.923444] [G acc: 0.421875]\n",
      "632 [D loss: 0.515588] [D acc: 0.734375] [G loss: 3.902258] [G acc: 0.000000]\n",
      "633 [D loss: 0.572273] [D acc: 0.800781] [G loss: 0.276090] [G acc: 0.914062]\n",
      "634 [D loss: 0.886800] [D acc: 0.578125] [G loss: 2.202426] [G acc: 0.015625]\n",
      "635 [D loss: 0.702775] [D acc: 0.699219] [G loss: 1.542567] [G acc: 0.000000]\n",
      "636 [D loss: 0.388621] [D acc: 0.812500] [G loss: 1.264289] [G acc: 0.031250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637 [D loss: 0.338601] [D acc: 0.886719] [G loss: 1.759636] [G acc: 0.015625]\n",
      "638 [D loss: 0.348454] [D acc: 0.847656] [G loss: 0.988143] [G acc: 0.257812]\n",
      "639 [D loss: 0.288804] [D acc: 0.871094] [G loss: 1.882991] [G acc: 0.007812]\n",
      "640 [D loss: 0.305405] [D acc: 0.890625] [G loss: 3.087876] [G acc: 0.000000]\n",
      "641 [D loss: 0.241055] [D acc: 0.921875] [G loss: 2.232852] [G acc: 0.007812]\n",
      "642 [D loss: 0.178007] [D acc: 0.941406] [G loss: 2.705921] [G acc: 0.093750]\n",
      "643 [D loss: 0.107706] [D acc: 0.984375] [G loss: 3.441033] [G acc: 0.007812]\n",
      "644 [D loss: 0.154404] [D acc: 0.957031] [G loss: 5.060428] [G acc: 0.000000]\n",
      "645 [D loss: 0.291740] [D acc: 0.890625] [G loss: 0.730620] [G acc: 0.593750]\n",
      "646 [D loss: 0.893003] [D acc: 0.601562] [G loss: 3.624443] [G acc: 0.000000]\n",
      "647 [D loss: 0.666798] [D acc: 0.687500] [G loss: 0.712263] [G acc: 0.500000]\n",
      "648 [D loss: 0.683252] [D acc: 0.710938] [G loss: 1.852794] [G acc: 0.023438]\n",
      "649 [D loss: 0.492122] [D acc: 0.761719] [G loss: 0.993705] [G acc: 0.226562]\n",
      "650 [D loss: 0.365459] [D acc: 0.867188] [G loss: 0.932962] [G acc: 0.304688]\n",
      "651 [D loss: 0.430106] [D acc: 0.832031] [G loss: 1.057578] [G acc: 0.242188]\n",
      "652 [D loss: 0.257270] [D acc: 0.906250] [G loss: 0.205996] [G acc: 0.945312]\n",
      "653 [D loss: 0.561337] [D acc: 0.714844] [G loss: 1.490350] [G acc: 0.226562]\n",
      "654 [D loss: 0.554214] [D acc: 0.730469] [G loss: 0.486997] [G acc: 0.789062]\n",
      "655 [D loss: 0.529773] [D acc: 0.679688] [G loss: 1.735976] [G acc: 0.078125]\n",
      "656 [D loss: 0.471743] [D acc: 0.785156] [G loss: 0.745625] [G acc: 0.531250]\n",
      "657 [D loss: 0.426710] [D acc: 0.789062] [G loss: 0.956246] [G acc: 0.406250]\n",
      "658 [D loss: 0.381742] [D acc: 0.835938] [G loss: 0.421592] [G acc: 0.789062]\n",
      "659 [D loss: 0.334768] [D acc: 0.863281] [G loss: 1.771205] [G acc: 0.203125]\n",
      "660 [D loss: 0.432415] [D acc: 0.785156] [G loss: 0.078377] [G acc: 0.992188]\n",
      "661 [D loss: 0.371129] [D acc: 0.816406] [G loss: 1.038586] [G acc: 0.414062]\n",
      "662 [D loss: 0.304361] [D acc: 0.839844] [G loss: 0.079763] [G acc: 0.984375]\n",
      "663 [D loss: 0.215858] [D acc: 0.929688] [G loss: 0.200705] [G acc: 0.914062]\n",
      "664 [D loss: 0.248593] [D acc: 0.914062] [G loss: 1.121585] [G acc: 0.437500]\n",
      "665 [D loss: 0.205258] [D acc: 0.941406] [G loss: 0.424571] [G acc: 0.820312]\n",
      "666 [D loss: 0.169797] [D acc: 0.957031] [G loss: 2.740963] [G acc: 0.132812]\n",
      "667 [D loss: 0.271471] [D acc: 0.886719] [G loss: 1.172789] [G acc: 0.546875]\n",
      "668 [D loss: 0.235642] [D acc: 0.910156] [G loss: 4.019054] [G acc: 0.101562]\n",
      "669 [D loss: 0.492905] [D acc: 0.789062] [G loss: 0.694564] [G acc: 0.671875]\n",
      "670 [D loss: 0.889134] [D acc: 0.601562] [G loss: 5.792221] [G acc: 0.007812]\n",
      "671 [D loss: 1.087748] [D acc: 0.621094] [G loss: 1.130359] [G acc: 0.367188]\n",
      "672 [D loss: 0.607834] [D acc: 0.699219] [G loss: 2.954593] [G acc: 0.000000]\n",
      "673 [D loss: 0.566156] [D acc: 0.816406] [G loss: 1.376729] [G acc: 0.007812]\n",
      "674 [D loss: 0.332348] [D acc: 0.882812] [G loss: 1.782109] [G acc: 0.000000]\n",
      "675 [D loss: 0.289274] [D acc: 0.886719] [G loss: 1.803715] [G acc: 0.023438]\n",
      "676 [D loss: 0.282209] [D acc: 0.898438] [G loss: 1.851696] [G acc: 0.031250]\n",
      "677 [D loss: 0.231887] [D acc: 0.902344] [G loss: 1.883271] [G acc: 0.078125]\n",
      "678 [D loss: 0.315220] [D acc: 0.867188] [G loss: 1.302292] [G acc: 0.265625]\n",
      "679 [D loss: 0.241396] [D acc: 0.917969] [G loss: 1.061521] [G acc: 0.468750]\n",
      "680 [D loss: 0.280059] [D acc: 0.882812] [G loss: 0.663757] [G acc: 0.648438]\n",
      "681 [D loss: 0.277704] [D acc: 0.898438] [G loss: 0.590934] [G acc: 0.726562]\n",
      "682 [D loss: 1.216224] [D acc: 0.675781] [G loss: 1.187746] [G acc: 0.382812]\n",
      "683 [D loss: 0.505477] [D acc: 0.765625] [G loss: 0.124027] [G acc: 0.968750]\n",
      "684 [D loss: 0.576735] [D acc: 0.636719] [G loss: 3.398406] [G acc: 0.000000]\n",
      "685 [D loss: 0.497564] [D acc: 0.773438] [G loss: 0.503307] [G acc: 0.726562]\n",
      "686 [D loss: 0.287247] [D acc: 0.910156] [G loss: 0.588084] [G acc: 0.671875]\n",
      "687 [D loss: 0.339318] [D acc: 0.871094] [G loss: 1.830149] [G acc: 0.109375]\n",
      "688 [D loss: 0.326727] [D acc: 0.878906] [G loss: 0.319324] [G acc: 0.851562]\n",
      "689 [D loss: 0.430297] [D acc: 0.816406] [G loss: 1.988851] [G acc: 0.078125]\n",
      "690 [D loss: 0.476654] [D acc: 0.789062] [G loss: 0.443285] [G acc: 0.796875]\n",
      "691 [D loss: 0.288200] [D acc: 0.871094] [G loss: 1.733387] [G acc: 0.093750]\n",
      "692 [D loss: 0.199084] [D acc: 0.925781] [G loss: 0.832187] [G acc: 0.523438]\n",
      "693 [D loss: 0.197315] [D acc: 0.941406] [G loss: 1.514039] [G acc: 0.203125]\n",
      "694 [D loss: 0.250672] [D acc: 0.906250] [G loss: 0.223730] [G acc: 0.882812]\n",
      "695 [D loss: 0.394078] [D acc: 0.804688] [G loss: 3.281445] [G acc: 0.007812]\n",
      "696 [D loss: 0.570345] [D acc: 0.765625] [G loss: 1.391299] [G acc: 0.054688]\n",
      "697 [D loss: 0.198885] [D acc: 0.941406] [G loss: 0.878004] [G acc: 0.429688]\n",
      "698 [D loss: 0.235208] [D acc: 0.914062] [G loss: 1.659787] [G acc: 0.046875]\n",
      "699 [D loss: 0.155036] [D acc: 0.941406] [G loss: 0.443047] [G acc: 0.796875]\n",
      "700 [D loss: 0.420449] [D acc: 0.785156] [G loss: 4.830532] [G acc: 0.000000]\n",
      "701 [D loss: 0.465427] [D acc: 0.800781] [G loss: 1.868943] [G acc: 0.023438]\n",
      "702 [D loss: 0.344410] [D acc: 0.851562] [G loss: 2.577909] [G acc: 0.000000]\n",
      "703 [D loss: 0.247699] [D acc: 0.910156] [G loss: 2.373610] [G acc: 0.000000]\n",
      "704 [D loss: 0.195627] [D acc: 0.953125] [G loss: 2.856718] [G acc: 0.000000]\n",
      "705 [D loss: 0.094078] [D acc: 0.980469] [G loss: 3.538500] [G acc: 0.015625]\n",
      "706 [D loss: 0.177332] [D acc: 0.945312] [G loss: 3.340183] [G acc: 0.015625]\n",
      "707 [D loss: 0.152496] [D acc: 0.945312] [G loss: 2.322811] [G acc: 0.140625]\n",
      "708 [D loss: 0.658355] [D acc: 0.691406] [G loss: 5.254101] [G acc: 0.000000]\n",
      "709 [D loss: 0.998775] [D acc: 0.636719] [G loss: 1.269334] [G acc: 0.109375]\n",
      "710 [D loss: 0.444651] [D acc: 0.808594] [G loss: 1.661449] [G acc: 0.015625]\n",
      "711 [D loss: 0.269580] [D acc: 0.933594] [G loss: 1.719384] [G acc: 0.062500]\n",
      "712 [D loss: 0.297482] [D acc: 0.871094] [G loss: 1.949280] [G acc: 0.039062]\n",
      "713 [D loss: 0.249747] [D acc: 0.921875] [G loss: 1.625986] [G acc: 0.125000]\n",
      "714 [D loss: 0.278796] [D acc: 0.906250] [G loss: 1.634544] [G acc: 0.140625]\n",
      "715 [D loss: 0.249230] [D acc: 0.894531] [G loss: 0.930048] [G acc: 0.445312]\n",
      "716 [D loss: 0.588192] [D acc: 0.691406] [G loss: 3.146870] [G acc: 0.000000]\n",
      "717 [D loss: 0.444283] [D acc: 0.777344] [G loss: 0.567399] [G acc: 0.671875]\n",
      "718 [D loss: 0.483414] [D acc: 0.757812] [G loss: 1.833822] [G acc: 0.109375]\n",
      "719 [D loss: 0.447284] [D acc: 0.773438] [G loss: 1.071541] [G acc: 0.312500]\n",
      "720 [D loss: 0.451483] [D acc: 0.769531] [G loss: 2.306877] [G acc: 0.015625]\n",
      "721 [D loss: 0.256072] [D acc: 0.875000] [G loss: 0.887047] [G acc: 0.414062]\n",
      "722 [D loss: 0.260851] [D acc: 0.898438] [G loss: 2.357533] [G acc: 0.085938]\n",
      "723 [D loss: 0.414049] [D acc: 0.839844] [G loss: 0.540194] [G acc: 0.703125]\n",
      "724 [D loss: 0.341537] [D acc: 0.859375] [G loss: 2.147985] [G acc: 0.015625]\n",
      "725 [D loss: 0.443313] [D acc: 0.800781] [G loss: 1.075680] [G acc: 0.304688]\n",
      "726 [D loss: 0.213215] [D acc: 0.949219] [G loss: 1.720686] [G acc: 0.070312]\n",
      "727 [D loss: 0.245689] [D acc: 0.906250] [G loss: 1.453580] [G acc: 0.242188]\n",
      "728 [D loss: 0.159276] [D acc: 0.941406] [G loss: 1.497866] [G acc: 0.203125]\n",
      "729 [D loss: 0.325225] [D acc: 0.851562] [G loss: 3.840865] [G acc: 0.007812]\n",
      "730 [D loss: 0.209556] [D acc: 0.937500] [G loss: 3.459845] [G acc: 0.000000]\n",
      "731 [D loss: 0.187080] [D acc: 0.937500] [G loss: 2.737184] [G acc: 0.007812]\n",
      "732 [D loss: 0.316099] [D acc: 0.871094] [G loss: 3.633380] [G acc: 0.054688]\n",
      "733 [D loss: 0.935480] [D acc: 0.707031] [G loss: 1.812790] [G acc: 0.015625]\n",
      "734 [D loss: 0.458304] [D acc: 0.792969] [G loss: 0.962872] [G acc: 0.187500]\n",
      "735 [D loss: 0.375798] [D acc: 0.789062] [G loss: 1.973697] [G acc: 0.007812]\n",
      "736 [D loss: 0.320663] [D acc: 0.894531] [G loss: 1.222792] [G acc: 0.203125]\n",
      "737 [D loss: 0.246431] [D acc: 0.933594] [G loss: 2.620634] [G acc: 0.000000]\n",
      "738 [D loss: 0.165683] [D acc: 0.953125] [G loss: 2.451926] [G acc: 0.015625]\n",
      "739 [D loss: 0.130542] [D acc: 0.957031] [G loss: 2.671170] [G acc: 0.023438]\n",
      "740 [D loss: 0.098713] [D acc: 0.980469] [G loss: 2.346111] [G acc: 0.101562]\n",
      "741 [D loss: 0.142008] [D acc: 0.953125] [G loss: 2.900168] [G acc: 0.046875]\n",
      "742 [D loss: 0.184301] [D acc: 0.941406] [G loss: 3.312596] [G acc: 0.023438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743 [D loss: 0.099689] [D acc: 0.968750] [G loss: 3.066801] [G acc: 0.015625]\n",
      "744 [D loss: 0.386504] [D acc: 0.828125] [G loss: 4.129186] [G acc: 0.000000]\n",
      "745 [D loss: 0.553456] [D acc: 0.777344] [G loss: 0.611378] [G acc: 0.648438]\n",
      "746 [D loss: 0.665542] [D acc: 0.617188] [G loss: 2.769614] [G acc: 0.015625]\n",
      "747 [D loss: 0.365612] [D acc: 0.851562] [G loss: 0.827171] [G acc: 0.507812]\n",
      "748 [D loss: 0.463849] [D acc: 0.738281] [G loss: 3.336588] [G acc: 0.000000]\n",
      "749 [D loss: 0.586000] [D acc: 0.738281] [G loss: 0.762496] [G acc: 0.484375]\n",
      "750 [D loss: 0.604793] [D acc: 0.675781] [G loss: 2.056925] [G acc: 0.015625]\n",
      "751 [D loss: 0.412789] [D acc: 0.789062] [G loss: 1.176024] [G acc: 0.250000]\n",
      "752 [D loss: 0.333051] [D acc: 0.890625] [G loss: 0.884342] [G acc: 0.515625]\n",
      "753 [D loss: 0.290290] [D acc: 0.894531] [G loss: 1.579753] [G acc: 0.148438]\n",
      "754 [D loss: 0.292813] [D acc: 0.886719] [G loss: 0.905299] [G acc: 0.500000]\n",
      "755 [D loss: 0.307154] [D acc: 0.871094] [G loss: 2.095316] [G acc: 0.125000]\n",
      "756 [D loss: 0.398444] [D acc: 0.808594] [G loss: 0.185560] [G acc: 0.921875]\n",
      "757 [D loss: 0.423690] [D acc: 0.777344] [G loss: 2.598867] [G acc: 0.015625]\n",
      "758 [D loss: 0.293228] [D acc: 0.886719] [G loss: 0.334208] [G acc: 0.875000]\n",
      "759 [D loss: 0.318091] [D acc: 0.851562] [G loss: 0.500722] [G acc: 0.703125]\n",
      "760 [D loss: 0.380978] [D acc: 0.843750] [G loss: 1.432710] [G acc: 0.304688]\n",
      "761 [D loss: 0.476420] [D acc: 0.781250] [G loss: 0.891968] [G acc: 0.484375]\n",
      "762 [D loss: 0.268692] [D acc: 0.902344] [G loss: 2.067778] [G acc: 0.070312]\n",
      "763 [D loss: 0.217439] [D acc: 0.921875] [G loss: 0.136503] [G acc: 0.960938]\n",
      "764 [D loss: 0.327968] [D acc: 0.839844] [G loss: 4.008286] [G acc: 0.007812]\n",
      "765 [D loss: 0.669143] [D acc: 0.718750] [G loss: 0.469524] [G acc: 0.789062]\n",
      "766 [D loss: 0.315876] [D acc: 0.890625] [G loss: 1.143805] [G acc: 0.203125]\n",
      "767 [D loss: 0.252838] [D acc: 0.902344] [G loss: 1.611840] [G acc: 0.101562]\n",
      "768 [D loss: 0.160943] [D acc: 0.957031] [G loss: 3.041797] [G acc: 0.000000]\n",
      "769 [D loss: 0.204702] [D acc: 0.945312] [G loss: 0.753518] [G acc: 0.546875]\n",
      "770 [D loss: 0.222091] [D acc: 0.910156] [G loss: 4.658453] [G acc: 0.000000]\n",
      "771 [D loss: 0.345130] [D acc: 0.875000] [G loss: 1.576137] [G acc: 0.031250]\n",
      "772 [D loss: 0.174756] [D acc: 0.945312] [G loss: 2.521633] [G acc: 0.000000]\n",
      "773 [D loss: 0.128399] [D acc: 0.968750] [G loss: 3.591498] [G acc: 0.000000]\n",
      "774 [D loss: 0.145914] [D acc: 0.945312] [G loss: 2.042357] [G acc: 0.039062]\n",
      "775 [D loss: 0.282281] [D acc: 0.890625] [G loss: 6.511312] [G acc: 0.000000]\n",
      "776 [D loss: 0.288810] [D acc: 0.906250] [G loss: 3.188568] [G acc: 0.015625]\n",
      "777 [D loss: 0.240428] [D acc: 0.898438] [G loss: 3.572086] [G acc: 0.007812]\n",
      "778 [D loss: 0.310317] [D acc: 0.863281] [G loss: 3.873147] [G acc: 0.015625]\n",
      "779 [D loss: 0.306844] [D acc: 0.894531] [G loss: 2.973217] [G acc: 0.000000]\n",
      "780 [D loss: 0.185769] [D acc: 0.933594] [G loss: 3.626924] [G acc: 0.000000]\n",
      "781 [D loss: 0.179905] [D acc: 0.937500] [G loss: 1.029285] [G acc: 0.375000]\n",
      "782 [D loss: 0.639355] [D acc: 0.714844] [G loss: 4.047027] [G acc: 0.007812]\n",
      "783 [D loss: 0.456650] [D acc: 0.789062] [G loss: 0.983926] [G acc: 0.320312]\n",
      "784 [D loss: 0.261839] [D acc: 0.933594] [G loss: 1.104043] [G acc: 0.289062]\n",
      "785 [D loss: 0.231378] [D acc: 0.917969] [G loss: 0.408688] [G acc: 0.804688]\n",
      "786 [D loss: 0.163124] [D acc: 0.957031] [G loss: 0.199715] [G acc: 0.953125]\n",
      "787 [D loss: 0.163807] [D acc: 0.929688] [G loss: 0.491764] [G acc: 0.726562]\n",
      "788 [D loss: 0.197854] [D acc: 0.917969] [G loss: 0.058490] [G acc: 0.992188]\n",
      "789 [D loss: 0.132520] [D acc: 0.960938] [G loss: 0.583893] [G acc: 0.679688]\n",
      "790 [D loss: 0.150103] [D acc: 0.941406] [G loss: 0.055208] [G acc: 0.984375]\n",
      "791 [D loss: 0.176682] [D acc: 0.925781] [G loss: 1.455310] [G acc: 0.351562]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-fa14df52bc6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mrun_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRUN_FOLDER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mprint_every_n_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/Personal/GDL/generative_deep_learning_code/models/GAN.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, batch_size, epochs, run_folder, print_every_n_batches, initial_epoch)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;31m# g_loss = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;31m# g_acc = 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;31m# Plot the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d_losses, g_losses, d_accs, g_accs = gan.train(     \n",
    "    x_train\n",
    "    , batch_size = 128\n",
    "    , epochs = 2000\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = 10\n",
    "    , initial_epoch = 0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, x_train.shape[0], 128)\n",
    "true_imgs = x_train[idx]\n",
    "\n",
    "noise = np.random.normal(0, 1, (128, gan.z_dim))\n",
    "gen_imgs = gan.generator.predict(noise)\n",
    "\n",
    "x = np.concatenate([true_imgs, gen_imgs])\n",
    "y = np.concatenate([np.ones((128,1)), np.zeros((128,1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss, d_acc = gan.discriminator.train_on_batch(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5897777"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12e7f13c8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((x[200]+1)*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7864815, 0.55078125]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.discriminator.train_on_batch(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g_accs, color='orange', linewidth=1)\n",
    "plt.plot(d_accs, color='green', linewidth=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99999976]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(gan.discriminator.predict(np.array([x_train[i]]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.5723013e-06], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = np.random.normal(0, 1, 100)\n",
    "img = gan.generator.predict(np.array([noise]))[0]\n",
    "\n",
    "print(img.shape)\n",
    "plt.imshow(img[:,:,0])\n",
    "\n",
    "gan.discriminator.predict(np.array([img]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gan.discriminator.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "for x, y in enumerate(gan.discriminator.layers):\n",
    "    \n",
    "    print(y)\n",
    "    print(y.trainable)\n",
    "    for i in gan.discriminator.layers[x].get_weights():\n",
    "        \n",
    "        print(pointer)\n",
    "        print(i.shape)\n",
    "        pointer+=1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gan.discriminator.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.get_weights()[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.model.save_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
