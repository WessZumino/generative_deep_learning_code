{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.GAN import GAN\n",
    "from utils.loaders import load_safari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = '0017'\n",
    "RUN_FOLDER = os.path.join(\"./run\", RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = load_safari('elephant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13ae01860>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0,:,:,0], cmap = 'gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(input_dim = (28,28,1)\n",
    "        , discriminator_conv_filters = [64,64,128,128]\n",
    "        , discriminator_conv_kernel_size = [5,5,5,5]\n",
    "        , discriminator_conv_strides = [2,2,2,1]\n",
    "        , discriminator_conv_padding = 'same'\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'relu'\n",
    "        , discriminator_dropout_rate = 0.4\n",
    "        , discriminator_learning_rate = 0.0008 \n",
    "        , generator_initial_dense_layer_size = (7, 7, 64)\n",
    "        , generator_use_upsampling = [True,True, False,False]\n",
    "        , generator_conv_t_filters = [128,64, 64,1]\n",
    "        , generator_conv_t_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_t_strides = [1,1,1,1]\n",
    "        , generator_conv_t_padding = 'same'\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0004 \n",
    "        , optimiser = 'rmsprop'\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "gan.save(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 1,441,666\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 720,833\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfoster/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_0 (Conv2DTr (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_1 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_2 (Conv2DTr (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_3 (Conv2DTr (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 844,161\n",
      "Trainable params: 837,377\n",
      "Non-trainable params: 6,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.701745] [D acc: 0.507812] [G loss: 0.138098] [G acc: 1.000000]\n",
      "1 [D loss: 1.749384] [D acc: 0.500000] [G loss: 0.675620] [G acc: 0.992188]\n",
      "2 [D loss: 0.686302] [D acc: 0.500000] [G loss: 0.659654] [G acc: 1.000000]\n",
      "3 [D loss: 0.677786] [D acc: 0.503906] [G loss: 0.568032] [G acc: 1.000000]\n",
      "4 [D loss: 0.652101] [D acc: 0.500000] [G loss: 0.270514] [G acc: 1.000000]\n",
      "5 [D loss: 1.527467] [D acc: 0.500000] [G loss: 0.712574] [G acc: 0.171875]\n",
      "6 [D loss: 0.679208] [D acc: 0.679688] [G loss: 0.705938] [G acc: 0.359375]\n",
      "7 [D loss: 0.643386] [D acc: 0.929688] [G loss: 0.779651] [G acc: 0.062500]\n",
      "8 [D loss: 0.570673] [D acc: 0.882812] [G loss: 0.605798] [G acc: 0.734375]\n",
      "9 [D loss: 0.327164] [D acc: 1.000000] [G loss: 0.703953] [G acc: 0.609375]\n",
      "10 [D loss: 1.474987] [D acc: 0.496094] [G loss: 1.091889] [G acc: 0.000000]\n",
      "11 [D loss: 0.987049] [D acc: 0.500000] [G loss: 0.697437] [G acc: 0.453125]\n",
      "12 [D loss: 0.633898] [D acc: 0.800781] [G loss: 0.697235] [G acc: 0.390625]\n",
      "13 [D loss: 0.592902] [D acc: 0.914062] [G loss: 0.695055] [G acc: 0.437500]\n",
      "14 [D loss: 0.531007] [D acc: 0.886719] [G loss: 0.630885] [G acc: 0.632812]\n",
      "15 [D loss: 0.619310] [D acc: 0.632812] [G loss: 0.858356] [G acc: 0.218750]\n",
      "16 [D loss: 0.584705] [D acc: 0.785156] [G loss: 0.554203] [G acc: 0.773438]\n",
      "17 [D loss: 0.393111] [D acc: 0.968750] [G loss: 0.230655] [G acc: 0.968750]\n",
      "18 [D loss: 0.192188] [D acc: 0.996094] [G loss: 0.044934] [G acc: 1.000000]\n",
      "19 [D loss: 0.081834] [D acc: 0.988281] [G loss: 0.018666] [G acc: 1.000000]\n",
      "20 [D loss: 0.038224] [D acc: 1.000000] [G loss: 0.006962] [G acc: 1.000000]\n",
      "21 [D loss: 0.021811] [D acc: 1.000000] [G loss: 0.005235] [G acc: 1.000000]\n",
      "22 [D loss: 0.027520] [D acc: 0.992188] [G loss: 0.011655] [G acc: 1.000000]\n",
      "23 [D loss: 0.045863] [D acc: 0.988281] [G loss: 0.000277] [G acc: 1.000000]\n",
      "24 [D loss: 0.218661] [D acc: 0.906250] [G loss: 0.021767] [G acc: 1.000000]\n",
      "25 [D loss: 0.531792] [D acc: 0.792969] [G loss: 0.000075] [G acc: 1.000000]\n",
      "26 [D loss: 1.001218] [D acc: 0.699219] [G loss: 0.027919] [G acc: 1.000000]\n",
      "27 [D loss: 0.371095] [D acc: 0.839844] [G loss: 0.035202] [G acc: 1.000000]\n",
      "28 [D loss: 0.232758] [D acc: 0.921875] [G loss: 0.020984] [G acc: 1.000000]\n",
      "29 [D loss: 0.276971] [D acc: 0.902344] [G loss: 0.024387] [G acc: 1.000000]\n",
      "30 [D loss: 0.280209] [D acc: 0.894531] [G loss: 0.019029] [G acc: 1.000000]\n",
      "31 [D loss: 0.323815] [D acc: 0.871094] [G loss: 0.036000] [G acc: 1.000000]\n",
      "32 [D loss: 0.490885] [D acc: 0.738281] [G loss: 0.039175] [G acc: 1.000000]\n",
      "33 [D loss: 0.389689] [D acc: 0.792969] [G loss: 0.311028] [G acc: 0.976562]\n",
      "34 [D loss: 0.387188] [D acc: 0.820312] [G loss: 0.010189] [G acc: 1.000000]\n",
      "35 [D loss: 0.107530] [D acc: 0.996094] [G loss: 0.006383] [G acc: 1.000000]\n",
      "36 [D loss: 0.252620] [D acc: 0.878906] [G loss: 0.018753] [G acc: 1.000000]\n",
      "37 [D loss: 0.878435] [D acc: 0.531250] [G loss: 0.510155] [G acc: 0.968750]\n",
      "38 [D loss: 0.639241] [D acc: 0.675781] [G loss: 0.194894] [G acc: 1.000000]\n",
      "39 [D loss: 0.809344] [D acc: 0.492188] [G loss: 0.634871] [G acc: 0.898438]\n",
      "40 [D loss: 0.579534] [D acc: 0.796875] [G loss: 0.509933] [G acc: 0.992188]\n",
      "41 [D loss: 0.732417] [D acc: 0.441406] [G loss: 0.633595] [G acc: 0.914062]\n",
      "42 [D loss: 0.666812] [D acc: 0.515625] [G loss: 0.630202] [G acc: 0.898438]\n",
      "43 [D loss: 0.664701] [D acc: 0.542969] [G loss: 0.606285] [G acc: 0.906250]\n",
      "44 [D loss: 0.695658] [D acc: 0.484375] [G loss: 0.638141] [G acc: 0.851562]\n",
      "45 [D loss: 0.686318] [D acc: 0.511719] [G loss: 0.601960] [G acc: 0.960938]\n",
      "46 [D loss: 0.627663] [D acc: 0.851562] [G loss: 0.565458] [G acc: 0.867188]\n",
      "47 [D loss: 0.646541] [D acc: 0.593750] [G loss: 1.323108] [G acc: 0.078125]\n",
      "48 [D loss: 0.579821] [D acc: 0.710938] [G loss: 1.169092] [G acc: 0.015625]\n",
      "49 [D loss: 0.489466] [D acc: 0.742188] [G loss: 0.247932] [G acc: 1.000000]\n",
      "50 [D loss: 0.660839] [D acc: 0.535156] [G loss: 2.711014] [G acc: 0.000000]\n",
      "51 [D loss: 1.003884] [D acc: 0.500000] [G loss: 0.535803] [G acc: 0.960938]\n",
      "52 [D loss: 0.705989] [D acc: 0.464844] [G loss: 0.549008] [G acc: 0.968750]\n",
      "53 [D loss: 0.707562] [D acc: 0.429688] [G loss: 0.629983] [G acc: 0.828125]\n",
      "54 [D loss: 0.691227] [D acc: 0.492188] [G loss: 0.699003] [G acc: 0.531250]\n",
      "55 [D loss: 0.656970] [D acc: 0.617188] [G loss: 0.735351] [G acc: 0.476562]\n",
      "56 [D loss: 0.550743] [D acc: 0.792969] [G loss: 0.608308] [G acc: 0.750000]\n",
      "57 [D loss: 0.832568] [D acc: 0.378906] [G loss: 0.739162] [G acc: 0.335938]\n",
      "58 [D loss: 0.739524] [D acc: 0.519531] [G loss: 0.609637] [G acc: 0.914062]\n",
      "59 [D loss: 0.690719] [D acc: 0.468750] [G loss: 0.674801] [G acc: 0.656250]\n",
      "60 [D loss: 0.685155] [D acc: 0.542969] [G loss: 0.715691] [G acc: 0.351562]\n",
      "61 [D loss: 0.658773] [D acc: 0.656250] [G loss: 0.729109] [G acc: 0.296875]\n",
      "62 [D loss: 0.667969] [D acc: 0.628906] [G loss: 0.754319] [G acc: 0.195312]\n",
      "63 [D loss: 0.661643] [D acc: 0.699219] [G loss: 0.800464] [G acc: 0.078125]\n",
      "64 [D loss: 0.635190] [D acc: 0.726562] [G loss: 0.709150] [G acc: 0.437500]\n",
      "65 [D loss: 0.548783] [D acc: 0.812500] [G loss: 0.456155] [G acc: 0.937500]\n",
      "66 [D loss: 0.849640] [D acc: 0.417969] [G loss: 1.206081] [G acc: 0.000000]\n",
      "67 [D loss: 0.730381] [D acc: 0.507812] [G loss: 0.611858] [G acc: 0.890625]\n",
      "68 [D loss: 0.690774] [D acc: 0.472656] [G loss: 0.692901] [G acc: 0.507812]\n",
      "69 [D loss: 0.662148] [D acc: 0.679688] [G loss: 0.667989] [G acc: 0.664062]\n",
      "70 [D loss: 0.649525] [D acc: 0.617188] [G loss: 0.652104] [G acc: 0.742188]\n",
      "71 [D loss: 0.639148] [D acc: 0.625000] [G loss: 0.702811] [G acc: 0.453125]\n",
      "72 [D loss: 0.620580] [D acc: 0.742188] [G loss: 0.589618] [G acc: 0.765625]\n",
      "73 [D loss: 0.673987] [D acc: 0.507812] [G loss: 0.805857] [G acc: 0.078125]\n",
      "74 [D loss: 0.650893] [D acc: 0.660156] [G loss: 0.528934] [G acc: 0.960938]\n",
      "75 [D loss: 0.785171] [D acc: 0.496094] [G loss: 1.049396] [G acc: 0.000000]\n",
      "76 [D loss: 0.706779] [D acc: 0.507812] [G loss: 0.651703] [G acc: 0.695312]\n",
      "77 [D loss: 0.676289] [D acc: 0.558594] [G loss: 0.838947] [G acc: 0.023438]\n",
      "78 [D loss: 0.607169] [D acc: 0.808594] [G loss: 1.031416] [G acc: 0.046875]\n",
      "79 [D loss: 0.454543] [D acc: 0.847656] [G loss: 0.163127] [G acc: 1.000000]\n",
      "80 [D loss: 1.150736] [D acc: 0.488281] [G loss: 1.001397] [G acc: 0.000000]\n",
      "81 [D loss: 0.670245] [D acc: 0.531250] [G loss: 0.855567] [G acc: 0.000000]\n",
      "82 [D loss: 0.659022] [D acc: 0.617188] [G loss: 0.975957] [G acc: 0.007812]\n",
      "83 [D loss: 0.621709] [D acc: 0.636719] [G loss: 1.175019] [G acc: 0.000000]\n",
      "84 [D loss: 0.630649] [D acc: 0.621094] [G loss: 0.965941] [G acc: 0.039062]\n",
      "85 [D loss: 0.626977] [D acc: 0.675781] [G loss: 1.022530] [G acc: 0.046875]\n",
      "86 [D loss: 0.514426] [D acc: 0.710938] [G loss: 0.430388] [G acc: 1.000000]\n",
      "87 [D loss: 0.772039] [D acc: 0.480469] [G loss: 0.639480] [G acc: 0.867188]\n",
      "88 [D loss: 0.658636] [D acc: 0.531250] [G loss: 0.696258] [G acc: 0.437500]\n",
      "89 [D loss: 0.634658] [D acc: 0.660156] [G loss: 0.779437] [G acc: 0.156250]\n",
      "90 [D loss: 0.622230] [D acc: 0.660156] [G loss: 0.794820] [G acc: 0.179688]\n",
      "91 [D loss: 0.502127] [D acc: 0.886719] [G loss: 1.782147] [G acc: 0.015625]\n",
      "92 [D loss: 1.333060] [D acc: 0.226562] [G loss: 0.991175] [G acc: 0.078125]\n",
      "93 [D loss: 0.355906] [D acc: 0.980469] [G loss: 1.584905] [G acc: 0.109375]\n",
      "94 [D loss: 0.621058] [D acc: 0.582031] [G loss: 3.191991] [G acc: 0.000000]\n",
      "95 [D loss: 1.018668] [D acc: 0.500000] [G loss: 0.690671] [G acc: 0.476562]\n",
      "96 [D loss: 0.637880] [D acc: 0.714844] [G loss: 0.571259] [G acc: 0.812500]\n",
      "97 [D loss: 0.676855] [D acc: 0.492188] [G loss: 0.567178] [G acc: 0.875000]\n",
      "98 [D loss: 0.636177] [D acc: 0.597656] [G loss: 0.744960] [G acc: 0.406250]\n",
      "99 [D loss: 0.560683] [D acc: 0.789062] [G loss: 0.881320] [G acc: 0.234375]\n",
      "100 [D loss: 0.532124] [D acc: 0.730469] [G loss: 0.301882] [G acc: 1.000000]\n",
      "101 [D loss: 0.908076] [D acc: 0.417969] [G loss: 1.305825] [G acc: 0.000000]\n",
      "102 [D loss: 0.644638] [D acc: 0.554688] [G loss: 0.981347] [G acc: 0.015625]\n",
      "103 [D loss: 0.573642] [D acc: 0.804688] [G loss: 1.602320] [G acc: 0.000000]\n",
      "104 [D loss: 0.540721] [D acc: 0.730469] [G loss: 0.911188] [G acc: 0.210938]\n",
      "105 [D loss: 0.609061] [D acc: 0.625000] [G loss: 1.850850] [G acc: 0.000000]\n",
      "106 [D loss: 0.594653] [D acc: 0.589844] [G loss: 0.351787] [G acc: 0.992188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 [D loss: 0.814210] [D acc: 0.496094] [G loss: 0.755342] [G acc: 0.343750]\n",
      "108 [D loss: 0.526486] [D acc: 0.914062] [G loss: 0.791126] [G acc: 0.351562]\n",
      "109 [D loss: 0.674646] [D acc: 0.621094] [G loss: 0.835819] [G acc: 0.164062]\n",
      "110 [D loss: 0.622558] [D acc: 0.757812] [G loss: 0.966733] [G acc: 0.109375]\n",
      "111 [D loss: 0.616877] [D acc: 0.683594] [G loss: 0.773359] [G acc: 0.398438]\n",
      "112 [D loss: 0.667709] [D acc: 0.578125] [G loss: 0.897861] [G acc: 0.171875]\n",
      "113 [D loss: 0.612586] [D acc: 0.671875] [G loss: 0.579831] [G acc: 0.773438]\n",
      "114 [D loss: 0.679178] [D acc: 0.578125] [G loss: 0.501181] [G acc: 0.960938]\n",
      "115 [D loss: 0.650603] [D acc: 0.632812] [G loss: 0.599595] [G acc: 0.804688]\n",
      "116 [D loss: 0.692583] [D acc: 0.523438] [G loss: 0.569345] [G acc: 0.851562]\n",
      "117 [D loss: 0.786243] [D acc: 0.496094] [G loss: 1.367882] [G acc: 0.000000]\n",
      "118 [D loss: 0.779485] [D acc: 0.476562] [G loss: 0.968374] [G acc: 0.148438]\n",
      "119 [D loss: 0.658665] [D acc: 0.601562] [G loss: 0.867058] [G acc: 0.140625]\n",
      "120 [D loss: 0.545554] [D acc: 0.820312] [G loss: 0.838670] [G acc: 0.265625]\n",
      "121 [D loss: 0.369479] [D acc: 0.800781] [G loss: 0.410613] [G acc: 0.953125]\n",
      "122 [D loss: 0.335892] [D acc: 0.906250] [G loss: 0.288770] [G acc: 0.976562]\n",
      "123 [D loss: 0.303099] [D acc: 0.886719] [G loss: 0.073864] [G acc: 1.000000]\n",
      "124 [D loss: 0.661677] [D acc: 0.644531] [G loss: 0.335577] [G acc: 0.984375]\n",
      "125 [D loss: 0.401671] [D acc: 0.796875] [G loss: 0.163197] [G acc: 1.000000]\n",
      "126 [D loss: 0.490843] [D acc: 0.769531] [G loss: 0.235511] [G acc: 0.968750]\n",
      "127 [D loss: 0.951303] [D acc: 0.445312] [G loss: 0.568367] [G acc: 0.703125]\n",
      "128 [D loss: 0.409108] [D acc: 0.800781] [G loss: 0.261688] [G acc: 1.000000]\n",
      "129 [D loss: 0.482989] [D acc: 0.792969] [G loss: 0.276254] [G acc: 0.968750]\n",
      "130 [D loss: 1.236977] [D acc: 0.257812] [G loss: 0.863897] [G acc: 0.101562]\n",
      "131 [D loss: 0.686987] [D acc: 0.554688] [G loss: 0.790567] [G acc: 0.257812]\n",
      "132 [D loss: 0.626005] [D acc: 0.683594] [G loss: 0.735139] [G acc: 0.390625]\n",
      "133 [D loss: 0.620449] [D acc: 0.781250] [G loss: 0.718119] [G acc: 0.468750]\n",
      "134 [D loss: 0.678581] [D acc: 0.527344] [G loss: 0.627524] [G acc: 0.789062]\n",
      "135 [D loss: 0.466568] [D acc: 0.847656] [G loss: 0.563199] [G acc: 0.867188]\n",
      "136 [D loss: 0.698305] [D acc: 0.519531] [G loss: 2.046521] [G acc: 0.015625]\n",
      "137 [D loss: 0.722951] [D acc: 0.488281] [G loss: 0.829894] [G acc: 0.171875]\n",
      "138 [D loss: 0.662356] [D acc: 0.542969] [G loss: 0.845050] [G acc: 0.203125]\n",
      "139 [D loss: 0.642728] [D acc: 0.667969] [G loss: 0.923722] [G acc: 0.085938]\n",
      "140 [D loss: 0.667262] [D acc: 0.628906] [G loss: 0.656018] [G acc: 0.601562]\n",
      "141 [D loss: 0.616492] [D acc: 0.687500] [G loss: 0.718035] [G acc: 0.429688]\n",
      "142 [D loss: 0.639014] [D acc: 0.683594] [G loss: 0.739103] [G acc: 0.414062]\n",
      "143 [D loss: 0.610449] [D acc: 0.679688] [G loss: 0.944218] [G acc: 0.132812]\n",
      "144 [D loss: 0.584271] [D acc: 0.730469] [G loss: 0.723246] [G acc: 0.476562]\n",
      "145 [D loss: 0.570541] [D acc: 0.726562] [G loss: 1.922434] [G acc: 0.000000]\n",
      "146 [D loss: 0.821847] [D acc: 0.519531] [G loss: 0.137624] [G acc: 1.000000]\n",
      "147 [D loss: 0.764447] [D acc: 0.535156] [G loss: 0.464029] [G acc: 0.984375]\n",
      "148 [D loss: 0.646611] [D acc: 0.632812] [G loss: 0.433200] [G acc: 1.000000]\n",
      "149 [D loss: 0.660795] [D acc: 0.566406] [G loss: 0.454526] [G acc: 0.984375]\n",
      "150 [D loss: 0.645791] [D acc: 0.582031] [G loss: 0.459199] [G acc: 0.992188]\n",
      "151 [D loss: 0.659293] [D acc: 0.550781] [G loss: 0.487430] [G acc: 0.984375]\n",
      "152 [D loss: 0.616838] [D acc: 0.691406] [G loss: 0.491462] [G acc: 0.929688]\n",
      "153 [D loss: 0.586660] [D acc: 0.757812] [G loss: 0.520201] [G acc: 0.859375]\n",
      "154 [D loss: 0.518156] [D acc: 0.820312] [G loss: 0.213472] [G acc: 1.000000]\n",
      "155 [D loss: 0.788024] [D acc: 0.480469] [G loss: 2.243479] [G acc: 0.000000]\n",
      "156 [D loss: 0.885832] [D acc: 0.500000] [G loss: 0.691887] [G acc: 0.476562]\n",
      "157 [D loss: 0.512653] [D acc: 0.917969] [G loss: 0.556634] [G acc: 0.921875]\n",
      "158 [D loss: 0.539336] [D acc: 0.832031] [G loss: 0.484534] [G acc: 0.929688]\n",
      "159 [D loss: 0.741733] [D acc: 0.488281] [G loss: 0.856468] [G acc: 0.070312]\n",
      "160 [D loss: 0.608407] [D acc: 0.789062] [G loss: 0.741345] [G acc: 0.390625]\n",
      "161 [D loss: 0.614548] [D acc: 0.640625] [G loss: 1.010090] [G acc: 0.023438]\n",
      "162 [D loss: 0.546672] [D acc: 0.851562] [G loss: 0.901910] [G acc: 0.218750]\n",
      "163 [D loss: 0.401651] [D acc: 0.902344] [G loss: 0.691953] [G acc: 0.593750]\n",
      "164 [D loss: 0.727184] [D acc: 0.527344] [G loss: 2.416718] [G acc: 0.000000]\n",
      "165 [D loss: 0.842376] [D acc: 0.535156] [G loss: 1.507956] [G acc: 0.000000]\n",
      "166 [D loss: 0.602175] [D acc: 0.578125] [G loss: 0.576873] [G acc: 0.812500]\n",
      "167 [D loss: 0.664221] [D acc: 0.562500] [G loss: 0.977657] [G acc: 0.046875]\n",
      "168 [D loss: 0.689888] [D acc: 0.671875] [G loss: 0.636306] [G acc: 0.679688]\n",
      "169 [D loss: 0.668745] [D acc: 0.554688] [G loss: 0.755077] [G acc: 0.289062]\n",
      "170 [D loss: 0.625346] [D acc: 0.742188] [G loss: 0.717517] [G acc: 0.429688]\n",
      "171 [D loss: 0.613674] [D acc: 0.667969] [G loss: 0.857751] [G acc: 0.148438]\n",
      "172 [D loss: 0.604176] [D acc: 0.730469] [G loss: 0.856460] [G acc: 0.179688]\n",
      "173 [D loss: 0.617195] [D acc: 0.687500] [G loss: 1.260499] [G acc: 0.000000]\n",
      "174 [D loss: 0.605122] [D acc: 0.644531] [G loss: 0.306716] [G acc: 1.000000]\n",
      "175 [D loss: 0.969969] [D acc: 0.500000] [G loss: 1.359495] [G acc: 0.000000]\n",
      "176 [D loss: 0.736056] [D acc: 0.500000] [G loss: 0.873143] [G acc: 0.023438]\n",
      "177 [D loss: 0.656467] [D acc: 0.609375] [G loss: 0.838603] [G acc: 0.039062]\n",
      "178 [D loss: 0.634073] [D acc: 0.652344] [G loss: 0.769480] [G acc: 0.296875]\n",
      "179 [D loss: 0.590548] [D acc: 0.726562] [G loss: 0.721664] [G acc: 0.421875]\n",
      "180 [D loss: 0.669213] [D acc: 0.609375] [G loss: 0.667841] [G acc: 0.640625]\n",
      "181 [D loss: 0.621188] [D acc: 0.683594] [G loss: 0.653713] [G acc: 0.593750]\n",
      "182 [D loss: 0.671156] [D acc: 0.625000] [G loss: 0.701490] [G acc: 0.500000]\n",
      "183 [D loss: 0.647843] [D acc: 0.640625] [G loss: 0.827103] [G acc: 0.289062]\n",
      "184 [D loss: 0.548041] [D acc: 0.765625] [G loss: 0.425392] [G acc: 0.992188]\n",
      "185 [D loss: 0.727574] [D acc: 0.500000] [G loss: 1.022336] [G acc: 0.015625]\n",
      "186 [D loss: 0.681770] [D acc: 0.628906] [G loss: 0.666328] [G acc: 0.593750]\n",
      "187 [D loss: 0.620225] [D acc: 0.613281] [G loss: 0.810685] [G acc: 0.210938]\n",
      "188 [D loss: 0.617184] [D acc: 0.718750] [G loss: 0.916836] [G acc: 0.093750]\n",
      "189 [D loss: 0.635908] [D acc: 0.640625] [G loss: 0.852181] [G acc: 0.226562]\n",
      "190 [D loss: 0.620260] [D acc: 0.671875] [G loss: 0.767572] [G acc: 0.367188]\n",
      "191 [D loss: 0.642170] [D acc: 0.625000] [G loss: 0.965014] [G acc: 0.101562]\n",
      "192 [D loss: 0.673847] [D acc: 0.578125] [G loss: 0.856021] [G acc: 0.164062]\n",
      "193 [D loss: 0.640954] [D acc: 0.664062] [G loss: 0.591206] [G acc: 0.765625]\n",
      "194 [D loss: 0.603092] [D acc: 0.671875] [G loss: 0.988936] [G acc: 0.085938]\n",
      "195 [D loss: 0.700521] [D acc: 0.578125] [G loss: 1.222961] [G acc: 0.093750]\n",
      "196 [D loss: 0.663730] [D acc: 0.558594] [G loss: 0.966612] [G acc: 0.156250]\n",
      "197 [D loss: 0.587312] [D acc: 0.703125] [G loss: 0.959052] [G acc: 0.171875]\n",
      "198 [D loss: 0.660224] [D acc: 0.609375] [G loss: 0.929436] [G acc: 0.101562]\n",
      "199 [D loss: 0.611884] [D acc: 0.722656] [G loss: 0.553567] [G acc: 0.796875]\n",
      "200 [D loss: 0.655547] [D acc: 0.570312] [G loss: 1.330266] [G acc: 0.000000]\n",
      "201 [D loss: 0.762419] [D acc: 0.503906] [G loss: 0.632676] [G acc: 0.664062]\n",
      "202 [D loss: 0.621317] [D acc: 0.644531] [G loss: 0.538658] [G acc: 0.843750]\n",
      "203 [D loss: 0.611895] [D acc: 0.707031] [G loss: 0.769214] [G acc: 0.351562]\n",
      "204 [D loss: 0.668967] [D acc: 0.609375] [G loss: 0.742945] [G acc: 0.375000]\n",
      "205 [D loss: 0.640806] [D acc: 0.664062] [G loss: 0.862441] [G acc: 0.171875]\n",
      "206 [D loss: 0.644714] [D acc: 0.636719] [G loss: 0.546941] [G acc: 0.859375]\n",
      "207 [D loss: 0.675523] [D acc: 0.539062] [G loss: 1.077125] [G acc: 0.007812]\n",
      "208 [D loss: 0.703078] [D acc: 0.535156] [G loss: 0.770571] [G acc: 0.226562]\n",
      "209 [D loss: 0.635300] [D acc: 0.703125] [G loss: 0.668399] [G acc: 0.625000]\n",
      "210 [D loss: 0.637493] [D acc: 0.656250] [G loss: 0.790838] [G acc: 0.289062]\n",
      "211 [D loss: 0.650620] [D acc: 0.582031] [G loss: 0.787335] [G acc: 0.265625]\n",
      "212 [D loss: 0.612213] [D acc: 0.734375] [G loss: 0.689150] [G acc: 0.476562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213 [D loss: 0.604948] [D acc: 0.636719] [G loss: 1.001253] [G acc: 0.070312]\n",
      "214 [D loss: 0.666367] [D acc: 0.609375] [G loss: 0.401838] [G acc: 0.945312]\n",
      "215 [D loss: 0.752862] [D acc: 0.515625] [G loss: 1.063354] [G acc: 0.000000]\n",
      "216 [D loss: 0.720719] [D acc: 0.503906] [G loss: 0.830712] [G acc: 0.046875]\n",
      "217 [D loss: 0.666113] [D acc: 0.617188] [G loss: 0.842946] [G acc: 0.070312]\n",
      "218 [D loss: 0.632850] [D acc: 0.699219] [G loss: 0.756870] [G acc: 0.320312]\n",
      "219 [D loss: 0.628608] [D acc: 0.660156] [G loss: 0.722548] [G acc: 0.406250]\n",
      "220 [D loss: 0.614854] [D acc: 0.687500] [G loss: 0.802988] [G acc: 0.234375]\n",
      "221 [D loss: 0.670583] [D acc: 0.585938] [G loss: 0.755694] [G acc: 0.359375]\n",
      "222 [D loss: 0.598014] [D acc: 0.742188] [G loss: 0.592561] [G acc: 0.664062]\n",
      "223 [D loss: 0.680302] [D acc: 0.570312] [G loss: 0.991162] [G acc: 0.070312]\n",
      "224 [D loss: 0.665713] [D acc: 0.558594] [G loss: 0.329189] [G acc: 0.992188]\n",
      "225 [D loss: 0.707871] [D acc: 0.523438] [G loss: 0.666198] [G acc: 0.546875]\n",
      "226 [D loss: 0.639828] [D acc: 0.671875] [G loss: 0.598386] [G acc: 0.742188]\n",
      "227 [D loss: 0.648536] [D acc: 0.628906] [G loss: 0.673188] [G acc: 0.500000]\n",
      "228 [D loss: 0.648951] [D acc: 0.578125] [G loss: 0.780207] [G acc: 0.320312]\n",
      "229 [D loss: 0.621458] [D acc: 0.671875] [G loss: 0.721868] [G acc: 0.414062]\n",
      "230 [D loss: 0.586439] [D acc: 0.742188] [G loss: 0.628108] [G acc: 0.632812]\n",
      "231 [D loss: 0.693576] [D acc: 0.511719] [G loss: 0.901697] [G acc: 0.070312]\n",
      "232 [D loss: 0.606323] [D acc: 0.742188] [G loss: 0.832559] [G acc: 0.226562]\n",
      "233 [D loss: 0.610420] [D acc: 0.667969] [G loss: 0.846513] [G acc: 0.257812]\n",
      "234 [D loss: 0.637344] [D acc: 0.632812] [G loss: 0.898305] [G acc: 0.203125]\n",
      "235 [D loss: 0.587253] [D acc: 0.707031] [G loss: 0.674168] [G acc: 0.531250]\n",
      "236 [D loss: 0.800195] [D acc: 0.523438] [G loss: 0.983205] [G acc: 0.070312]\n",
      "237 [D loss: 0.710441] [D acc: 0.492188] [G loss: 0.753036] [G acc: 0.343750]\n",
      "238 [D loss: 0.644359] [D acc: 0.613281] [G loss: 0.872848] [G acc: 0.148438]\n",
      "239 [D loss: 0.659371] [D acc: 0.609375] [G loss: 0.721369] [G acc: 0.468750]\n",
      "240 [D loss: 0.635140] [D acc: 0.675781] [G loss: 0.933209] [G acc: 0.125000]\n",
      "241 [D loss: 0.668621] [D acc: 0.628906] [G loss: 0.707850] [G acc: 0.476562]\n",
      "242 [D loss: 0.636587] [D acc: 0.636719] [G loss: 0.864305] [G acc: 0.203125]\n",
      "243 [D loss: 0.637342] [D acc: 0.683594] [G loss: 0.663729] [G acc: 0.570312]\n",
      "244 [D loss: 0.660058] [D acc: 0.613281] [G loss: 1.148905] [G acc: 0.007812]\n",
      "245 [D loss: 0.706238] [D acc: 0.496094] [G loss: 0.597736] [G acc: 0.750000]\n",
      "246 [D loss: 0.690361] [D acc: 0.531250] [G loss: 0.873892] [G acc: 0.093750]\n",
      "247 [D loss: 0.654842] [D acc: 0.632812] [G loss: 0.656000] [G acc: 0.562500]\n",
      "248 [D loss: 0.645943] [D acc: 0.621094] [G loss: 0.710684] [G acc: 0.437500]\n",
      "249 [D loss: 0.641345] [D acc: 0.648438] [G loss: 0.726865] [G acc: 0.406250]\n",
      "250 [D loss: 0.655915] [D acc: 0.644531] [G loss: 0.824751] [G acc: 0.210938]\n",
      "251 [D loss: 0.656845] [D acc: 0.632812] [G loss: 0.747334] [G acc: 0.382812]\n",
      "252 [D loss: 0.631183] [D acc: 0.691406] [G loss: 0.847576] [G acc: 0.218750]\n",
      "253 [D loss: 0.611883] [D acc: 0.679688] [G loss: 0.616469] [G acc: 0.617188]\n",
      "254 [D loss: 0.645845] [D acc: 0.585938] [G loss: 1.215019] [G acc: 0.054688]\n",
      "255 [D loss: 0.750286] [D acc: 0.527344] [G loss: 0.697010] [G acc: 0.492188]\n",
      "256 [D loss: 0.568865] [D acc: 0.773438] [G loss: 0.603328] [G acc: 0.617188]\n",
      "257 [D loss: 0.764055] [D acc: 0.519531] [G loss: 1.136676] [G acc: 0.000000]\n",
      "258 [D loss: 0.671439] [D acc: 0.589844] [G loss: 0.766873] [G acc: 0.328125]\n",
      "259 [D loss: 0.659328] [D acc: 0.558594] [G loss: 0.896864] [G acc: 0.070312]\n",
      "260 [D loss: 0.623259] [D acc: 0.710938] [G loss: 0.729089] [G acc: 0.367188]\n",
      "261 [D loss: 0.606824] [D acc: 0.687500] [G loss: 0.857131] [G acc: 0.234375]\n",
      "262 [D loss: 0.606310] [D acc: 0.695312] [G loss: 0.595593] [G acc: 0.726562]\n",
      "263 [D loss: 0.702037] [D acc: 0.535156] [G loss: 0.990192] [G acc: 0.132812]\n",
      "264 [D loss: 0.679278] [D acc: 0.605469] [G loss: 0.660438] [G acc: 0.570312]\n",
      "265 [D loss: 0.697626] [D acc: 0.570312] [G loss: 0.906544] [G acc: 0.109375]\n",
      "266 [D loss: 0.600627] [D acc: 0.671875] [G loss: 0.638927] [G acc: 0.632812]\n",
      "267 [D loss: 0.534999] [D acc: 0.785156] [G loss: 0.898333] [G acc: 0.343750]\n",
      "268 [D loss: 0.873373] [D acc: 0.414062] [G loss: 0.868994] [G acc: 0.195312]\n",
      "269 [D loss: 0.589707] [D acc: 0.687500] [G loss: 1.067750] [G acc: 0.015625]\n",
      "270 [D loss: 0.572004] [D acc: 0.714844] [G loss: 0.821986] [G acc: 0.273438]\n",
      "271 [D loss: 0.507373] [D acc: 0.789062] [G loss: 0.494770] [G acc: 0.750000]\n",
      "272 [D loss: 0.947942] [D acc: 0.472656] [G loss: 1.357861] [G acc: 0.000000]\n",
      "273 [D loss: 0.866537] [D acc: 0.500000] [G loss: 0.820465] [G acc: 0.203125]\n",
      "274 [D loss: 0.648014] [D acc: 0.667969] [G loss: 0.821924] [G acc: 0.234375]\n",
      "275 [D loss: 0.629613] [D acc: 0.664062] [G loss: 0.818226] [G acc: 0.234375]\n",
      "276 [D loss: 0.634444] [D acc: 0.660156] [G loss: 0.675317] [G acc: 0.578125]\n",
      "277 [D loss: 0.627571] [D acc: 0.703125] [G loss: 0.758109] [G acc: 0.421875]\n",
      "278 [D loss: 0.650969] [D acc: 0.617188] [G loss: 0.712112] [G acc: 0.468750]\n",
      "279 [D loss: 0.636589] [D acc: 0.664062] [G loss: 0.853798] [G acc: 0.242188]\n",
      "280 [D loss: 0.601536] [D acc: 0.695312] [G loss: 0.757900] [G acc: 0.398438]\n",
      "281 [D loss: 0.563794] [D acc: 0.738281] [G loss: 0.718630] [G acc: 0.437500]\n",
      "282 [D loss: 0.577298] [D acc: 0.742188] [G loss: 0.926791] [G acc: 0.210938]\n",
      "283 [D loss: 0.691569] [D acc: 0.554688] [G loss: 0.967504] [G acc: 0.117188]\n",
      "284 [D loss: 0.639480] [D acc: 0.648438] [G loss: 0.978112] [G acc: 0.132812]\n",
      "285 [D loss: 0.606864] [D acc: 0.683594] [G loss: 0.776663] [G acc: 0.445312]\n",
      "286 [D loss: 0.594270] [D acc: 0.742188] [G loss: 0.890250] [G acc: 0.281250]\n",
      "287 [D loss: 0.587859] [D acc: 0.691406] [G loss: 0.577613] [G acc: 0.726562]\n",
      "288 [D loss: 0.588333] [D acc: 0.671875] [G loss: 1.239885] [G acc: 0.054688]\n",
      "289 [D loss: 0.665296] [D acc: 0.589844] [G loss: 0.272020] [G acc: 0.984375]\n",
      "290 [D loss: 0.860946] [D acc: 0.511719] [G loss: 1.015821] [G acc: 0.031250]\n",
      "291 [D loss: 0.690339] [D acc: 0.550781] [G loss: 0.818670] [G acc: 0.273438]\n",
      "292 [D loss: 0.684598] [D acc: 0.539062] [G loss: 0.816041] [G acc: 0.226562]\n",
      "293 [D loss: 0.639438] [D acc: 0.609375] [G loss: 0.855362] [G acc: 0.171875]\n",
      "294 [D loss: 0.587476] [D acc: 0.746094] [G loss: 0.806533] [G acc: 0.289062]\n",
      "295 [D loss: 0.556550] [D acc: 0.757812] [G loss: 0.812559] [G acc: 0.367188]\n",
      "296 [D loss: 0.629491] [D acc: 0.621094] [G loss: 0.879981] [G acc: 0.281250]\n",
      "297 [D loss: 0.612403] [D acc: 0.695312] [G loss: 0.793598] [G acc: 0.367188]\n",
      "298 [D loss: 0.616198] [D acc: 0.632812] [G loss: 1.279457] [G acc: 0.039062]\n",
      "299 [D loss: 0.706162] [D acc: 0.570312] [G loss: 0.647649] [G acc: 0.578125]\n",
      "300 [D loss: 0.690539] [D acc: 0.546875] [G loss: 0.926960] [G acc: 0.125000]\n",
      "301 [D loss: 0.618934] [D acc: 0.683594] [G loss: 0.706841] [G acc: 0.445312]\n",
      "302 [D loss: 0.598948] [D acc: 0.695312] [G loss: 0.739270] [G acc: 0.406250]\n",
      "303 [D loss: 0.597319] [D acc: 0.667969] [G loss: 0.887701] [G acc: 0.218750]\n",
      "304 [D loss: 0.624326] [D acc: 0.652344] [G loss: 0.722394] [G acc: 0.429688]\n",
      "305 [D loss: 0.550139] [D acc: 0.726562] [G loss: 0.728066] [G acc: 0.460938]\n",
      "306 [D loss: 0.587217] [D acc: 0.707031] [G loss: 0.629528] [G acc: 0.609375]\n",
      "307 [D loss: 0.656129] [D acc: 0.597656] [G loss: 1.242387] [G acc: 0.039062]\n",
      "308 [D loss: 0.723988] [D acc: 0.535156] [G loss: 0.588192] [G acc: 0.656250]\n",
      "309 [D loss: 0.733306] [D acc: 0.539062] [G loss: 1.130465] [G acc: 0.007812]\n",
      "310 [D loss: 0.619607] [D acc: 0.636719] [G loss: 0.802914] [G acc: 0.328125]\n",
      "311 [D loss: 0.567925] [D acc: 0.753906] [G loss: 0.843593] [G acc: 0.289062]\n",
      "312 [D loss: 0.545206] [D acc: 0.785156] [G loss: 0.836081] [G acc: 0.351562]\n",
      "313 [D loss: 0.549463] [D acc: 0.761719] [G loss: 0.942879] [G acc: 0.273438]\n",
      "314 [D loss: 0.566091] [D acc: 0.703125] [G loss: 0.583862] [G acc: 0.695312]\n",
      "315 [D loss: 0.561098] [D acc: 0.746094] [G loss: 1.117443] [G acc: 0.046875]\n",
      "316 [D loss: 0.610527] [D acc: 0.632812] [G loss: 0.249704] [G acc: 0.953125]\n",
      "317 [D loss: 0.800733] [D acc: 0.519531] [G loss: 1.340551] [G acc: 0.007812]\n",
      "318 [D loss: 0.664161] [D acc: 0.566406] [G loss: 0.728932] [G acc: 0.429688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319 [D loss: 0.539344] [D acc: 0.750000] [G loss: 0.941706] [G acc: 0.203125]\n",
      "320 [D loss: 0.545337] [D acc: 0.746094] [G loss: 1.256455] [G acc: 0.023438]\n",
      "321 [D loss: 0.557192] [D acc: 0.734375] [G loss: 0.955338] [G acc: 0.218750]\n",
      "322 [D loss: 0.528476] [D acc: 0.703125] [G loss: 1.014298] [G acc: 0.234375]\n",
      "323 [D loss: 0.573730] [D acc: 0.714844] [G loss: 0.486206] [G acc: 0.820312]\n",
      "324 [D loss: 0.734978] [D acc: 0.523438] [G loss: 1.860682] [G acc: 0.000000]\n",
      "325 [D loss: 0.935513] [D acc: 0.503906] [G loss: 0.773304] [G acc: 0.343750]\n",
      "326 [D loss: 0.593491] [D acc: 0.679688] [G loss: 0.936812] [G acc: 0.085938]\n",
      "327 [D loss: 0.650502] [D acc: 0.636719] [G loss: 0.869316] [G acc: 0.171875]\n",
      "328 [D loss: 0.610237] [D acc: 0.707031] [G loss: 0.799092] [G acc: 0.328125]\n",
      "329 [D loss: 0.599386] [D acc: 0.718750] [G loss: 0.974408] [G acc: 0.148438]\n",
      "330 [D loss: 0.578288] [D acc: 0.722656] [G loss: 0.897382] [G acc: 0.218750]\n",
      "331 [D loss: 0.557050] [D acc: 0.746094] [G loss: 0.880766] [G acc: 0.296875]\n",
      "332 [D loss: 0.598424] [D acc: 0.667969] [G loss: 1.001779] [G acc: 0.171875]\n",
      "333 [D loss: 0.599563] [D acc: 0.671875] [G loss: 0.611324] [G acc: 0.648438]\n",
      "334 [D loss: 0.637397] [D acc: 0.617188] [G loss: 1.382524] [G acc: 0.015625]\n",
      "335 [D loss: 0.667910] [D acc: 0.593750] [G loss: 0.469940] [G acc: 0.835938]\n",
      "336 [D loss: 0.654158] [D acc: 0.621094] [G loss: 0.922005] [G acc: 0.179688]\n",
      "337 [D loss: 0.638848] [D acc: 0.617188] [G loss: 0.642671] [G acc: 0.671875]\n",
      "338 [D loss: 0.638618] [D acc: 0.609375] [G loss: 0.929469] [G acc: 0.187500]\n",
      "339 [D loss: 0.608953] [D acc: 0.679688] [G loss: 0.797515] [G acc: 0.343750]\n",
      "340 [D loss: 0.622252] [D acc: 0.675781] [G loss: 1.009814] [G acc: 0.140625]\n",
      "341 [D loss: 0.574668] [D acc: 0.746094] [G loss: 0.848336] [G acc: 0.382812]\n",
      "342 [D loss: 0.633946] [D acc: 0.652344] [G loss: 1.133658] [G acc: 0.101562]\n",
      "343 [D loss: 0.610893] [D acc: 0.671875] [G loss: 0.604226] [G acc: 0.656250]\n",
      "344 [D loss: 0.642806] [D acc: 0.640625] [G loss: 1.235853] [G acc: 0.015625]\n",
      "345 [D loss: 0.632283] [D acc: 0.621094] [G loss: 0.654175] [G acc: 0.679688]\n",
      "346 [D loss: 0.626800] [D acc: 0.636719] [G loss: 1.162027] [G acc: 0.031250]\n",
      "347 [D loss: 0.643480] [D acc: 0.601562] [G loss: 0.578176] [G acc: 0.687500]\n",
      "348 [D loss: 0.624657] [D acc: 0.640625] [G loss: 1.188416] [G acc: 0.031250]\n",
      "349 [D loss: 0.614767] [D acc: 0.652344] [G loss: 0.768949] [G acc: 0.421875]\n",
      "350 [D loss: 0.644892] [D acc: 0.617188] [G loss: 1.143351] [G acc: 0.070312]\n",
      "351 [D loss: 0.614092] [D acc: 0.671875] [G loss: 0.624725] [G acc: 0.656250]\n",
      "352 [D loss: 0.595894] [D acc: 0.691406] [G loss: 0.969157] [G acc: 0.203125]\n",
      "353 [D loss: 0.606144] [D acc: 0.695312] [G loss: 0.711732] [G acc: 0.453125]\n",
      "354 [D loss: 0.594001] [D acc: 0.656250] [G loss: 1.085081] [G acc: 0.093750]\n",
      "355 [D loss: 0.631007] [D acc: 0.652344] [G loss: 0.594254] [G acc: 0.664062]\n",
      "356 [D loss: 0.650559] [D acc: 0.628906] [G loss: 1.279176] [G acc: 0.007812]\n",
      "357 [D loss: 0.670737] [D acc: 0.574219] [G loss: 0.628955] [G acc: 0.695312]\n",
      "358 [D loss: 0.616394] [D acc: 0.625000] [G loss: 0.956199] [G acc: 0.156250]\n",
      "359 [D loss: 0.549180] [D acc: 0.773438] [G loss: 0.720505] [G acc: 0.492188]\n",
      "360 [D loss: 0.541017] [D acc: 0.781250] [G loss: 1.139506] [G acc: 0.109375]\n",
      "361 [D loss: 0.627462] [D acc: 0.660156] [G loss: 0.646891] [G acc: 0.593750]\n",
      "362 [D loss: 0.579813] [D acc: 0.703125] [G loss: 1.203783] [G acc: 0.070312]\n",
      "363 [D loss: 0.627655] [D acc: 0.660156] [G loss: 0.602941] [G acc: 0.656250]\n",
      "364 [D loss: 0.631636] [D acc: 0.644531] [G loss: 1.325480] [G acc: 0.015625]\n",
      "365 [D loss: 0.633761] [D acc: 0.632812] [G loss: 0.623480] [G acc: 0.648438]\n",
      "366 [D loss: 0.587942] [D acc: 0.722656] [G loss: 0.955707] [G acc: 0.171875]\n",
      "367 [D loss: 0.581446] [D acc: 0.679688] [G loss: 0.641208] [G acc: 0.648438]\n",
      "368 [D loss: 0.576331] [D acc: 0.707031] [G loss: 0.935783] [G acc: 0.257812]\n",
      "369 [D loss: 0.573542] [D acc: 0.746094] [G loss: 0.955672] [G acc: 0.218750]\n",
      "370 [D loss: 0.565945] [D acc: 0.695312] [G loss: 1.344185] [G acc: 0.031250]\n",
      "371 [D loss: 0.653793] [D acc: 0.605469] [G loss: 0.697451] [G acc: 0.515625]\n",
      "372 [D loss: 0.653524] [D acc: 0.578125] [G loss: 1.599221] [G acc: 0.000000]\n",
      "373 [D loss: 0.699446] [D acc: 0.558594] [G loss: 0.709098] [G acc: 0.539062]\n",
      "374 [D loss: 0.617675] [D acc: 0.656250] [G loss: 1.007162] [G acc: 0.132812]\n",
      "375 [D loss: 0.544263] [D acc: 0.773438] [G loss: 0.835072] [G acc: 0.312500]\n",
      "376 [D loss: 0.577533] [D acc: 0.718750] [G loss: 1.000887] [G acc: 0.218750]\n",
      "377 [D loss: 0.582968] [D acc: 0.683594] [G loss: 0.886183] [G acc: 0.296875]\n"
     ]
    }
   ],
   "source": [
    "d_losses, g_losses, d_accs, g_accs = gan.train(     \n",
    "    x_train\n",
    "    , batch_size = 128\n",
    "    , epochs = 2000\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = 10\n",
    "    , initial_epoch = 0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g_accs, color='orange', linewidth=1)\n",
    "plt.plot(d_accs, color='green', linewidth=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(gan.discriminator.predict(np.array([x_train[i]]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.uniform(-1, 1, 100)\n",
    "img = gan.generator.predict(np.array([noise]))[0]\n",
    "\n",
    "print(img.shape)\n",
    "plt.imshow(img[:,:,0])\n",
    "\n",
    "gan.discriminator.predict(np.array([img]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gan.discriminator.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "for x, y in enumerate(gan.discriminator.layers):\n",
    "    \n",
    "    print(y)\n",
    "    print(y.trainable)\n",
    "    for i in gan.discriminator.layers[x].get_weights():\n",
    "        \n",
    "        print(pointer)\n",
    "        print(i.shape)\n",
    "        pointer+=1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gan.discriminator.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.get_weights()[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.model.save_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
