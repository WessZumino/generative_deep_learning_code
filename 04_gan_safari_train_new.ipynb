{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.GAN import GAN\n",
    "from utils.loaders import load_safari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = '0015'\n",
    "RUN_FOLDER = os.path.join(\"./run\", RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = load_safari('elephant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x140a586a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0,:,:,0], cmap = 'gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(input_dim = (28,28,1)\n",
    "        , discriminator_conv_filters = [64,64,128,128]\n",
    "        , discriminator_conv_kernel_size = [5,5,5,5]\n",
    "        , discriminator_conv_strides = [2,2,2,1]\n",
    "        , discriminator_conv_padding = 'same'\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'relu'\n",
    "        , discriminator_dropout_rate = 0.4\n",
    "        , discriminator_learning_rate = 0.0008 #0.0002 #\n",
    "        , generator_initial_dense_layer_size = (7, 7, 64)\n",
    "        , generator_use_upsampling = [True,True, False,False]\n",
    "        , generator_conv_t_filters = [128,64, 64,1]\n",
    "        , generator_conv_t_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_t_strides = [1,1,1,1]\n",
    "        , generator_conv_t_padding = 'same'\n",
    "        , generator_batch_norm_momentum = 0.9\n",
    "        , generator_activation = 'relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.0004 #0.0002 #\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "gan.save(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 1,441,666\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 720,833\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfoster/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_0 (Conv2DTr (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_1 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_2 (Conv2DTr (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_3 (Conv2DTr (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 844,161\n",
      "Trainable params: 837,377\n",
      "Non-trainable params: 6,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.701534] [D acc: 0.500000] [G loss: 0.134240] [G acc: 1.000000]\n",
      "1 [D loss: 1.598336] [D acc: 0.500000] [G loss: 0.677296] [G acc: 1.000000]\n",
      "2 [D loss: 0.692099] [D acc: 0.500000] [G loss: 0.676456] [G acc: 1.000000]\n",
      "3 [D loss: 0.692772] [D acc: 0.500000] [G loss: 0.675559] [G acc: 1.000000]\n",
      "4 [D loss: 0.691367] [D acc: 0.500000] [G loss: 0.678061] [G acc: 1.000000]\n",
      "5 [D loss: 0.687769] [D acc: 0.500000] [G loss: 0.681422] [G acc: 0.968750]\n",
      "6 [D loss: 0.685176] [D acc: 0.503906] [G loss: 0.688807] [G acc: 0.718750]\n",
      "7 [D loss: 0.681770] [D acc: 0.535156] [G loss: 0.662356] [G acc: 1.000000]\n",
      "8 [D loss: 0.687236] [D acc: 0.500000] [G loss: 0.646932] [G acc: 1.000000]\n",
      "9 [D loss: 0.700758] [D acc: 0.500000] [G loss: 0.659628] [G acc: 1.000000]\n",
      "10 [D loss: 0.689615] [D acc: 0.500000] [G loss: 0.650920] [G acc: 1.000000]\n",
      "11 [D loss: 0.687853] [D acc: 0.500000] [G loss: 0.638037] [G acc: 1.000000]\n",
      "12 [D loss: 0.692332] [D acc: 0.500000] [G loss: 0.646178] [G acc: 1.000000]\n",
      "13 [D loss: 0.692293] [D acc: 0.500000] [G loss: 0.651044] [G acc: 1.000000]\n",
      "14 [D loss: 0.692692] [D acc: 0.503906] [G loss: 0.665810] [G acc: 0.976562]\n",
      "15 [D loss: 0.692547] [D acc: 0.496094] [G loss: 0.652155] [G acc: 1.000000]\n",
      "16 [D loss: 0.690930] [D acc: 0.500000] [G loss: 0.674335] [G acc: 0.890625]\n",
      "17 [D loss: 0.691660] [D acc: 0.515625] [G loss: 0.676028] [G acc: 0.859375]\n",
      "18 [D loss: 0.692721] [D acc: 0.476562] [G loss: 0.681955] [G acc: 0.750000]\n",
      "19 [D loss: 0.690242] [D acc: 0.574219] [G loss: 0.688112] [G acc: 0.578125]\n",
      "20 [D loss: 0.692847] [D acc: 0.523438] [G loss: 0.699947] [G acc: 0.359375]\n",
      "21 [D loss: 0.688333] [D acc: 0.597656] [G loss: 0.665254] [G acc: 0.835938]\n",
      "22 [D loss: 0.697613] [D acc: 0.457031] [G loss: 0.722877] [G acc: 0.039062]\n",
      "23 [D loss: 0.689626] [D acc: 0.539062] [G loss: 0.720697] [G acc: 0.109375]\n",
      "24 [D loss: 0.699648] [D acc: 0.347656] [G loss: 0.713463] [G acc: 0.148438]\n",
      "25 [D loss: 0.688190] [D acc: 0.542969] [G loss: 0.703864] [G acc: 0.312500]\n",
      "26 [D loss: 0.695516] [D acc: 0.449219] [G loss: 0.689934] [G acc: 0.554688]\n",
      "27 [D loss: 0.691267] [D acc: 0.535156] [G loss: 0.695972] [G acc: 0.437500]\n",
      "28 [D loss: 0.690361] [D acc: 0.546875] [G loss: 0.692364] [G acc: 0.492188]\n",
      "29 [D loss: 0.690772] [D acc: 0.578125] [G loss: 0.696494] [G acc: 0.421875]\n",
      "30 [D loss: 0.688808] [D acc: 0.570312] [G loss: 0.704605] [G acc: 0.359375]\n",
      "31 [D loss: 0.688428] [D acc: 0.578125] [G loss: 0.697222] [G acc: 0.507812]\n",
      "32 [D loss: 0.691248] [D acc: 0.492188] [G loss: 0.676406] [G acc: 0.648438]\n",
      "33 [D loss: 0.709364] [D acc: 0.496094] [G loss: 0.712053] [G acc: 0.265625]\n",
      "34 [D loss: 0.689057] [D acc: 0.554688] [G loss: 0.689819] [G acc: 0.546875]\n",
      "35 [D loss: 0.697940] [D acc: 0.429688] [G loss: 0.710971] [G acc: 0.203125]\n",
      "36 [D loss: 0.689626] [D acc: 0.531250] [G loss: 0.704086] [G acc: 0.312500]\n",
      "37 [D loss: 0.690611] [D acc: 0.542969] [G loss: 0.712790] [G acc: 0.203125]\n",
      "38 [D loss: 0.690879] [D acc: 0.546875] [G loss: 0.703221] [G acc: 0.359375]\n",
      "39 [D loss: 0.688561] [D acc: 0.574219] [G loss: 0.713978] [G acc: 0.210938]\n",
      "40 [D loss: 0.690331] [D acc: 0.507812] [G loss: 0.707398] [G acc: 0.328125]\n",
      "41 [D loss: 0.687738] [D acc: 0.593750] [G loss: 0.714829] [G acc: 0.281250]\n",
      "42 [D loss: 0.683548] [D acc: 0.648438] [G loss: 0.728488] [G acc: 0.179688]\n",
      "43 [D loss: 0.677348] [D acc: 0.683594] [G loss: 0.743433] [G acc: 0.195312]\n",
      "44 [D loss: 0.721764] [D acc: 0.406250] [G loss: 0.718154] [G acc: 0.281250]\n",
      "45 [D loss: 0.718668] [D acc: 0.472656] [G loss: 0.711707] [G acc: 0.164062]\n",
      "46 [D loss: 0.689771] [D acc: 0.582031] [G loss: 0.710309] [G acc: 0.179688]\n",
      "47 [D loss: 0.683623] [D acc: 0.679688] [G loss: 0.701408] [G acc: 0.367188]\n",
      "48 [D loss: 0.687630] [D acc: 0.578125] [G loss: 0.718519] [G acc: 0.210938]\n",
      "49 [D loss: 0.693545] [D acc: 0.500000] [G loss: 0.720628] [G acc: 0.148438]\n",
      "50 [D loss: 0.684889] [D acc: 0.601562] [G loss: 0.700108] [G acc: 0.406250]\n",
      "51 [D loss: 0.681889] [D acc: 0.609375] [G loss: 0.765478] [G acc: 0.015625]\n",
      "52 [D loss: 0.685391] [D acc: 0.554688] [G loss: 0.697450] [G acc: 0.492188]\n",
      "53 [D loss: 0.688219] [D acc: 0.542969] [G loss: 0.828865] [G acc: 0.000000]\n",
      "54 [D loss: 0.709439] [D acc: 0.453125] [G loss: 0.735397] [G acc: 0.109375]\n",
      "55 [D loss: 0.682410] [D acc: 0.632812] [G loss: 0.726041] [G acc: 0.273438]\n",
      "56 [D loss: 0.686253] [D acc: 0.574219] [G loss: 0.654248] [G acc: 0.804688]\n",
      "57 [D loss: 0.696965] [D acc: 0.496094] [G loss: 0.785572] [G acc: 0.000000]\n",
      "58 [D loss: 0.687465] [D acc: 0.523438] [G loss: 0.682086] [G acc: 0.632812]\n",
      "59 [D loss: 0.679002] [D acc: 0.597656] [G loss: 0.788987] [G acc: 0.039062]\n",
      "60 [D loss: 0.686108] [D acc: 0.531250] [G loss: 0.649882] [G acc: 0.812500]\n",
      "61 [D loss: 0.680841] [D acc: 0.531250] [G loss: 0.902753] [G acc: 0.000000]\n",
      "62 [D loss: 0.684320] [D acc: 0.519531] [G loss: 0.587454] [G acc: 0.992188]\n",
      "63 [D loss: 0.703132] [D acc: 0.500000] [G loss: 0.757157] [G acc: 0.101562]\n",
      "64 [D loss: 0.680975] [D acc: 0.625000] [G loss: 0.698921] [G acc: 0.484375]\n",
      "65 [D loss: 0.680345] [D acc: 0.574219] [G loss: 0.745220] [G acc: 0.179688]\n",
      "66 [D loss: 0.685279] [D acc: 0.546875] [G loss: 0.729076] [G acc: 0.265625]\n",
      "67 [D loss: 0.674777] [D acc: 0.601562] [G loss: 0.730973] [G acc: 0.289062]\n",
      "68 [D loss: 0.739550] [D acc: 0.492188] [G loss: 0.786148] [G acc: 0.031250]\n",
      "69 [D loss: 0.680233] [D acc: 0.582031] [G loss: 0.751376] [G acc: 0.171875]\n",
      "70 [D loss: 0.684681] [D acc: 0.593750] [G loss: 0.694019] [G acc: 0.500000]\n",
      "71 [D loss: 0.669561] [D acc: 0.621094] [G loss: 0.903821] [G acc: 0.015625]\n",
      "72 [D loss: 0.696600] [D acc: 0.503906] [G loss: 0.637388] [G acc: 0.843750]\n",
      "73 [D loss: 0.690271] [D acc: 0.519531] [G loss: 0.962064] [G acc: 0.000000]\n",
      "74 [D loss: 0.698805] [D acc: 0.503906] [G loss: 0.663203] [G acc: 0.773438]\n",
      "75 [D loss: 0.671304] [D acc: 0.593750] [G loss: 0.784907] [G acc: 0.054688]\n",
      "76 [D loss: 0.668712] [D acc: 0.667969] [G loss: 0.719849] [G acc: 0.335938]\n",
      "77 [D loss: 0.665926] [D acc: 0.628906] [G loss: 0.805931] [G acc: 0.148438]\n",
      "78 [D loss: 0.671643] [D acc: 0.613281] [G loss: 0.748825] [G acc: 0.296875]\n",
      "79 [D loss: 0.665857] [D acc: 0.601562] [G loss: 0.685023] [G acc: 0.492188]\n",
      "80 [D loss: 0.733568] [D acc: 0.460938] [G loss: 0.934454] [G acc: 0.000000]\n",
      "81 [D loss: 0.670837] [D acc: 0.574219] [G loss: 0.620666] [G acc: 0.843750]\n",
      "82 [D loss: 0.687545] [D acc: 0.476562] [G loss: 0.709894] [G acc: 0.398438]\n",
      "83 [D loss: 0.692306] [D acc: 0.503906] [G loss: 0.687217] [G acc: 0.578125]\n",
      "84 [D loss: 0.715931] [D acc: 0.406250] [G loss: 0.712452] [G acc: 0.359375]\n",
      "85 [D loss: 0.664937] [D acc: 0.687500] [G loss: 0.692836] [G acc: 0.500000]\n",
      "86 [D loss: 0.658645] [D acc: 0.664062] [G loss: 0.733359] [G acc: 0.320312]\n",
      "87 [D loss: 0.736321] [D acc: 0.332031] [G loss: 0.769024] [G acc: 0.046875]\n",
      "88 [D loss: 0.673889] [D acc: 0.660156] [G loss: 0.726202] [G acc: 0.312500]\n",
      "89 [D loss: 0.684931] [D acc: 0.531250] [G loss: 0.769089] [G acc: 0.125000]\n",
      "90 [D loss: 0.679407] [D acc: 0.632812] [G loss: 0.738644] [G acc: 0.257812]\n",
      "91 [D loss: 0.665808] [D acc: 0.617188] [G loss: 0.780458] [G acc: 0.171875]\n",
      "92 [D loss: 0.668230] [D acc: 0.640625] [G loss: 0.583183] [G acc: 0.851562]\n",
      "93 [D loss: 0.716721] [D acc: 0.464844] [G loss: 0.941906] [G acc: 0.007812]\n",
      "94 [D loss: 0.696674] [D acc: 0.503906] [G loss: 0.683853] [G acc: 0.562500]\n",
      "95 [D loss: 0.667695] [D acc: 0.636719] [G loss: 0.663885] [G acc: 0.640625]\n",
      "96 [D loss: 0.658254] [D acc: 0.675781] [G loss: 0.699472] [G acc: 0.507812]\n",
      "97 [D loss: 0.670429] [D acc: 0.621094] [G loss: 0.730097] [G acc: 0.351562]\n",
      "98 [D loss: 0.664585] [D acc: 0.625000] [G loss: 0.689477] [G acc: 0.468750]\n",
      "99 [D loss: 0.695000] [D acc: 0.531250] [G loss: 0.982520] [G acc: 0.000000]\n",
      "100 [D loss: 0.717460] [D acc: 0.523438] [G loss: 0.565388] [G acc: 0.968750]\n",
      "101 [D loss: 0.685837] [D acc: 0.500000] [G loss: 0.715608] [G acc: 0.304688]\n",
      "102 [D loss: 0.660880] [D acc: 0.656250] [G loss: 0.668056] [G acc: 0.664062]\n",
      "103 [D loss: 0.673255] [D acc: 0.585938] [G loss: 0.782659] [G acc: 0.203125]\n",
      "104 [D loss: 0.662684] [D acc: 0.644531] [G loss: 0.671303] [G acc: 0.625000]\n",
      "105 [D loss: 0.674620] [D acc: 0.542969] [G loss: 0.895145] [G acc: 0.046875]\n",
      "106 [D loss: 0.651580] [D acc: 0.683594] [G loss: 0.349933] [G acc: 1.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 [D loss: 0.800053] [D acc: 0.500000] [G loss: 0.883842] [G acc: 0.000000]\n",
      "108 [D loss: 0.691589] [D acc: 0.500000] [G loss: 0.771385] [G acc: 0.093750]\n",
      "109 [D loss: 0.681668] [D acc: 0.566406] [G loss: 0.739733] [G acc: 0.203125]\n",
      "110 [D loss: 0.667526] [D acc: 0.656250] [G loss: 0.723016] [G acc: 0.335938]\n",
      "111 [D loss: 0.662891] [D acc: 0.679688] [G loss: 0.723820] [G acc: 0.359375]\n",
      "112 [D loss: 0.672216] [D acc: 0.570312] [G loss: 0.806318] [G acc: 0.156250]\n",
      "113 [D loss: 0.662447] [D acc: 0.640625] [G loss: 0.733506] [G acc: 0.351562]\n",
      "114 [D loss: 0.678794] [D acc: 0.562500] [G loss: 0.968443] [G acc: 0.000000]\n",
      "115 [D loss: 0.684516] [D acc: 0.542969] [G loss: 0.618449] [G acc: 0.828125]\n",
      "116 [D loss: 0.676266] [D acc: 0.511719] [G loss: 0.842856] [G acc: 0.054688]\n",
      "117 [D loss: 0.672420] [D acc: 0.585938] [G loss: 0.660501] [G acc: 0.625000]\n",
      "118 [D loss: 0.669463] [D acc: 0.613281] [G loss: 0.917926] [G acc: 0.007812]\n",
      "119 [D loss: 0.678715] [D acc: 0.582031] [G loss: 0.604774] [G acc: 0.851562]\n",
      "120 [D loss: 0.685749] [D acc: 0.550781] [G loss: 0.776869] [G acc: 0.218750]\n",
      "121 [D loss: 0.665111] [D acc: 0.648438] [G loss: 0.694227] [G acc: 0.468750]\n",
      "122 [D loss: 0.676739] [D acc: 0.574219] [G loss: 0.788618] [G acc: 0.179688]\n",
      "123 [D loss: 0.666299] [D acc: 0.605469] [G loss: 0.658492] [G acc: 0.648438]\n",
      "124 [D loss: 0.659353] [D acc: 0.632812] [G loss: 0.919246] [G acc: 0.023438]\n",
      "125 [D loss: 0.665130] [D acc: 0.574219] [G loss: 0.530925] [G acc: 0.929688]\n",
      "126 [D loss: 0.703441] [D acc: 0.500000] [G loss: 1.062538] [G acc: 0.007812]\n",
      "127 [D loss: 0.703389] [D acc: 0.503906] [G loss: 0.643166] [G acc: 0.726562]\n",
      "128 [D loss: 0.665562] [D acc: 0.589844] [G loss: 0.722692] [G acc: 0.296875]\n",
      "129 [D loss: 0.679679] [D acc: 0.613281] [G loss: 0.750821] [G acc: 0.265625]\n",
      "130 [D loss: 0.668032] [D acc: 0.632812] [G loss: 0.740727] [G acc: 0.304688]\n",
      "131 [D loss: 0.649410] [D acc: 0.707031] [G loss: 0.724629] [G acc: 0.453125]\n",
      "132 [D loss: 0.656438] [D acc: 0.597656] [G loss: 0.775336] [G acc: 0.281250]\n",
      "133 [D loss: 0.665678] [D acc: 0.617188] [G loss: 0.723677] [G acc: 0.398438]\n",
      "134 [D loss: 0.641725] [D acc: 0.644531] [G loss: 0.862836] [G acc: 0.125000]\n",
      "135 [D loss: 0.683004] [D acc: 0.546875] [G loss: 1.052455] [G acc: 0.031250]\n",
      "136 [D loss: 0.656424] [D acc: 0.601562] [G loss: 0.373776] [G acc: 1.000000]\n",
      "137 [D loss: 0.762529] [D acc: 0.500000] [G loss: 1.130237] [G acc: 0.000000]\n",
      "138 [D loss: 0.727213] [D acc: 0.500000] [G loss: 0.702380] [G acc: 0.453125]\n",
      "139 [D loss: 0.661473] [D acc: 0.617188] [G loss: 0.714286] [G acc: 0.367188]\n",
      "140 [D loss: 0.676127] [D acc: 0.570312] [G loss: 0.745764] [G acc: 0.257812]\n",
      "141 [D loss: 0.663329] [D acc: 0.644531] [G loss: 0.719096] [G acc: 0.406250]\n",
      "142 [D loss: 0.662889] [D acc: 0.632812] [G loss: 0.794656] [G acc: 0.148438]\n",
      "143 [D loss: 0.648079] [D acc: 0.660156] [G loss: 0.715531] [G acc: 0.445312]\n",
      "144 [D loss: 0.657807] [D acc: 0.621094] [G loss: 0.870047] [G acc: 0.085938]\n",
      "145 [D loss: 0.676867] [D acc: 0.562500] [G loss: 0.683745] [G acc: 0.515625]\n",
      "146 [D loss: 0.646829] [D acc: 0.656250] [G loss: 0.840904] [G acc: 0.109375]\n",
      "147 [D loss: 0.650685] [D acc: 0.628906] [G loss: 0.697718] [G acc: 0.500000]\n",
      "148 [D loss: 0.662535] [D acc: 0.597656] [G loss: 1.025194] [G acc: 0.015625]\n",
      "149 [D loss: 0.643155] [D acc: 0.652344] [G loss: 0.461587] [G acc: 0.960938]\n",
      "150 [D loss: 0.711635] [D acc: 0.507812] [G loss: 1.150850] [G acc: 0.000000]\n",
      "151 [D loss: 0.727458] [D acc: 0.500000] [G loss: 0.708033] [G acc: 0.437500]\n",
      "152 [D loss: 0.637473] [D acc: 0.718750] [G loss: 0.717148] [G acc: 0.398438]\n",
      "153 [D loss: 0.698813] [D acc: 0.562500] [G loss: 0.780196] [G acc: 0.234375]\n",
      "154 [D loss: 0.661280] [D acc: 0.617188] [G loss: 0.693240] [G acc: 0.484375]\n",
      "155 [D loss: 0.666243] [D acc: 0.628906] [G loss: 0.784407] [G acc: 0.242188]\n",
      "156 [D loss: 0.675483] [D acc: 0.562500] [G loss: 0.805688] [G acc: 0.210938]\n",
      "157 [D loss: 0.643491] [D acc: 0.687500] [G loss: 0.641535] [G acc: 0.640625]\n",
      "158 [D loss: 0.647411] [D acc: 0.636719] [G loss: 0.926979] [G acc: 0.039062]\n",
      "159 [D loss: 0.662708] [D acc: 0.605469] [G loss: 0.550496] [G acc: 0.859375]\n",
      "160 [D loss: 0.685164] [D acc: 0.550781] [G loss: 1.063033] [G acc: 0.007812]\n",
      "161 [D loss: 0.700527] [D acc: 0.519531] [G loss: 0.674972] [G acc: 0.515625]\n",
      "162 [D loss: 0.653622] [D acc: 0.605469] [G loss: 0.778765] [G acc: 0.250000]\n",
      "163 [D loss: 0.637899] [D acc: 0.671875] [G loss: 0.762474] [G acc: 0.351562]\n",
      "164 [D loss: 0.641245] [D acc: 0.679688] [G loss: 0.832807] [G acc: 0.187500]\n",
      "165 [D loss: 0.637607] [D acc: 0.683594] [G loss: 0.752871] [G acc: 0.367188]\n",
      "166 [D loss: 0.624773] [D acc: 0.691406] [G loss: 0.910547] [G acc: 0.125000]\n",
      "167 [D loss: 0.660515] [D acc: 0.621094] [G loss: 0.512207] [G acc: 0.882812]\n",
      "168 [D loss: 0.602054] [D acc: 0.558594] [G loss: 1.231605] [G acc: 0.000000]\n",
      "169 [D loss: 0.673968] [D acc: 0.570312] [G loss: 0.381591] [G acc: 0.984375]\n",
      "170 [D loss: 0.904663] [D acc: 0.500000] [G loss: 0.976427] [G acc: 0.007812]\n",
      "171 [D loss: 0.670092] [D acc: 0.570312] [G loss: 0.792159] [G acc: 0.250000]\n",
      "172 [D loss: 0.668021] [D acc: 0.574219] [G loss: 0.824907] [G acc: 0.203125]\n",
      "173 [D loss: 0.664624] [D acc: 0.628906] [G loss: 0.769212] [G acc: 0.296875]\n",
      "174 [D loss: 0.642275] [D acc: 0.671875] [G loss: 0.735575] [G acc: 0.335938]\n",
      "175 [D loss: 0.664221] [D acc: 0.585938] [G loss: 0.743326] [G acc: 0.375000]\n",
      "176 [D loss: 0.627183] [D acc: 0.699219] [G loss: 0.703681] [G acc: 0.515625]\n",
      "177 [D loss: 0.640068] [D acc: 0.640625] [G loss: 0.867707] [G acc: 0.101562]\n",
      "178 [D loss: 0.628498] [D acc: 0.691406] [G loss: 0.709998] [G acc: 0.507812]\n",
      "179 [D loss: 0.629386] [D acc: 0.617188] [G loss: 1.155324] [G acc: 0.007812]\n",
      "180 [D loss: 0.690070] [D acc: 0.558594] [G loss: 0.507854] [G acc: 0.882812]\n",
      "181 [D loss: 0.713056] [D acc: 0.511719] [G loss: 0.965693] [G acc: 0.046875]\n",
      "182 [D loss: 0.663117] [D acc: 0.625000] [G loss: 0.728016] [G acc: 0.421875]\n",
      "183 [D loss: 0.629682] [D acc: 0.707031] [G loss: 0.744424] [G acc: 0.375000]\n",
      "184 [D loss: 0.668922] [D acc: 0.617188] [G loss: 0.919670] [G acc: 0.148438]\n",
      "185 [D loss: 0.652551] [D acc: 0.640625] [G loss: 0.625098] [G acc: 0.703125]\n",
      "186 [D loss: 0.635144] [D acc: 0.640625] [G loss: 0.918451] [G acc: 0.117188]\n",
      "187 [D loss: 0.669542] [D acc: 0.578125] [G loss: 0.553406] [G acc: 0.789062]\n",
      "188 [D loss: 0.720436] [D acc: 0.531250] [G loss: 1.054930] [G acc: 0.015625]\n",
      "189 [D loss: 0.665114] [D acc: 0.558594] [G loss: 0.722433] [G acc: 0.406250]\n",
      "190 [D loss: 0.651198] [D acc: 0.621094] [G loss: 0.812640] [G acc: 0.218750]\n",
      "191 [D loss: 0.644140] [D acc: 0.664062] [G loss: 0.761659] [G acc: 0.296875]\n",
      "192 [D loss: 0.621877] [D acc: 0.699219] [G loss: 0.884973] [G acc: 0.171875]\n",
      "193 [D loss: 0.634266] [D acc: 0.648438] [G loss: 0.682676] [G acc: 0.531250]\n",
      "194 [D loss: 0.612105] [D acc: 0.675781] [G loss: 1.219648] [G acc: 0.023438]\n",
      "195 [D loss: 0.693511] [D acc: 0.558594] [G loss: 0.463085] [G acc: 0.945312]\n",
      "196 [D loss: 0.704724] [D acc: 0.535156] [G loss: 1.127251] [G acc: 0.000000]\n",
      "197 [D loss: 0.690808] [D acc: 0.535156] [G loss: 0.773501] [G acc: 0.335938]\n",
      "198 [D loss: 0.647765] [D acc: 0.664062] [G loss: 0.787416] [G acc: 0.242188]\n",
      "199 [D loss: 0.650561] [D acc: 0.652344] [G loss: 0.810211] [G acc: 0.289062]\n",
      "200 [D loss: 0.640257] [D acc: 0.636719] [G loss: 0.759462] [G acc: 0.398438]\n",
      "201 [D loss: 0.626064] [D acc: 0.675781] [G loss: 0.880335] [G acc: 0.085938]\n",
      "202 [D loss: 0.619388] [D acc: 0.687500] [G loss: 0.672027] [G acc: 0.578125]\n",
      "203 [D loss: 0.637526] [D acc: 0.632812] [G loss: 1.209782] [G acc: 0.015625]\n",
      "204 [D loss: 0.687184] [D acc: 0.531250] [G loss: 0.419897] [G acc: 0.945312]\n",
      "205 [D loss: 0.744893] [D acc: 0.500000] [G loss: 1.067351] [G acc: 0.000000]\n",
      "206 [D loss: 0.673921] [D acc: 0.558594] [G loss: 0.757643] [G acc: 0.320312]\n",
      "207 [D loss: 0.630272] [D acc: 0.660156] [G loss: 0.848255] [G acc: 0.218750]\n",
      "208 [D loss: 0.636725] [D acc: 0.671875] [G loss: 0.748041] [G acc: 0.382812]\n",
      "209 [D loss: 0.644764] [D acc: 0.644531] [G loss: 0.833674] [G acc: 0.242188]\n",
      "210 [D loss: 0.638981] [D acc: 0.648438] [G loss: 0.799412] [G acc: 0.335938]\n",
      "211 [D loss: 0.642715] [D acc: 0.636719] [G loss: 0.953575] [G acc: 0.078125]\n",
      "212 [D loss: 0.637547] [D acc: 0.613281] [G loss: 0.625695] [G acc: 0.679688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213 [D loss: 0.618373] [D acc: 0.648438] [G loss: 1.215900] [G acc: 0.015625]\n",
      "214 [D loss: 0.647003] [D acc: 0.601562] [G loss: 0.451045] [G acc: 0.921875]\n",
      "215 [D loss: 0.681673] [D acc: 0.562500] [G loss: 1.059582] [G acc: 0.015625]\n",
      "216 [D loss: 0.687070] [D acc: 0.562500] [G loss: 0.686652] [G acc: 0.554688]\n",
      "217 [D loss: 0.656964] [D acc: 0.597656] [G loss: 0.828998] [G acc: 0.218750]\n",
      "218 [D loss: 0.619146] [D acc: 0.710938] [G loss: 0.750460] [G acc: 0.414062]\n",
      "219 [D loss: 0.655596] [D acc: 0.625000] [G loss: 0.959396] [G acc: 0.093750]\n",
      "220 [D loss: 0.634079] [D acc: 0.648438] [G loss: 0.652116] [G acc: 0.585938]\n",
      "221 [D loss: 0.651132] [D acc: 0.621094] [G loss: 1.039486] [G acc: 0.046875]\n",
      "222 [D loss: 0.650271] [D acc: 0.578125] [G loss: 0.527283] [G acc: 0.843750]\n",
      "223 [D loss: 0.711089] [D acc: 0.523438] [G loss: 1.135573] [G acc: 0.007812]\n",
      "224 [D loss: 0.686172] [D acc: 0.539062] [G loss: 0.707064] [G acc: 0.492188]\n",
      "225 [D loss: 0.640040] [D acc: 0.617188] [G loss: 0.887269] [G acc: 0.132812]\n",
      "226 [D loss: 0.630671] [D acc: 0.671875] [G loss: 0.822751] [G acc: 0.257812]\n",
      "227 [D loss: 0.631788] [D acc: 0.648438] [G loss: 0.815471] [G acc: 0.281250]\n",
      "228 [D loss: 0.616572] [D acc: 0.691406] [G loss: 0.905510] [G acc: 0.203125]\n",
      "229 [D loss: 0.599820] [D acc: 0.699219] [G loss: 0.655708] [G acc: 0.601562]\n",
      "230 [D loss: 0.575052] [D acc: 0.730469] [G loss: 1.064129] [G acc: 0.125000]\n",
      "231 [D loss: 0.621847] [D acc: 0.664062] [G loss: 0.792710] [G acc: 0.367188]\n",
      "232 [D loss: 0.640130] [D acc: 0.574219] [G loss: 1.696492] [G acc: 0.000000]\n",
      "233 [D loss: 0.855779] [D acc: 0.500000] [G loss: 0.638390] [G acc: 0.609375]\n",
      "234 [D loss: 0.682460] [D acc: 0.531250] [G loss: 1.046654] [G acc: 0.039062]\n",
      "235 [D loss: 0.670204] [D acc: 0.578125] [G loss: 0.715729] [G acc: 0.460938]\n",
      "236 [D loss: 0.637074] [D acc: 0.613281] [G loss: 0.926687] [G acc: 0.132812]\n",
      "237 [D loss: 0.646832] [D acc: 0.648438] [G loss: 0.709408] [G acc: 0.437500]\n",
      "238 [D loss: 0.645417] [D acc: 0.625000] [G loss: 0.809026] [G acc: 0.265625]\n",
      "239 [D loss: 0.618266] [D acc: 0.714844] [G loss: 0.864098] [G acc: 0.210938]\n",
      "240 [D loss: 0.618480] [D acc: 0.687500] [G loss: 0.722799] [G acc: 0.484375]\n",
      "241 [D loss: 0.623548] [D acc: 0.687500] [G loss: 1.059798] [G acc: 0.078125]\n",
      "242 [D loss: 0.629590] [D acc: 0.636719] [G loss: 0.465830] [G acc: 0.867188]\n",
      "243 [D loss: 0.713754] [D acc: 0.542969] [G loss: 1.215309] [G acc: 0.015625]\n",
      "244 [D loss: 0.725400] [D acc: 0.515625] [G loss: 0.762977] [G acc: 0.382812]\n",
      "245 [D loss: 0.629573] [D acc: 0.652344] [G loss: 0.813860] [G acc: 0.296875]\n",
      "246 [D loss: 0.654445] [D acc: 0.621094] [G loss: 0.872195] [G acc: 0.187500]\n",
      "247 [D loss: 0.634377] [D acc: 0.664062] [G loss: 0.808402] [G acc: 0.289062]\n",
      "248 [D loss: 0.618720] [D acc: 0.683594] [G loss: 0.827220] [G acc: 0.289062]\n",
      "249 [D loss: 0.611582] [D acc: 0.683594] [G loss: 0.812574] [G acc: 0.328125]\n",
      "250 [D loss: 0.622449] [D acc: 0.667969] [G loss: 0.867605] [G acc: 0.257812]\n",
      "251 [D loss: 0.636552] [D acc: 0.660156] [G loss: 0.797067] [G acc: 0.335938]\n",
      "252 [D loss: 0.637263] [D acc: 0.625000] [G loss: 1.209064] [G acc: 0.015625]\n",
      "253 [D loss: 0.646010] [D acc: 0.597656] [G loss: 0.399270] [G acc: 0.968750]\n",
      "254 [D loss: 0.751093] [D acc: 0.531250] [G loss: 1.245133] [G acc: 0.000000]\n",
      "255 [D loss: 0.696614] [D acc: 0.527344] [G loss: 0.766505] [G acc: 0.351562]\n",
      "256 [D loss: 0.596569] [D acc: 0.734375] [G loss: 0.798983] [G acc: 0.296875]\n",
      "257 [D loss: 0.627208] [D acc: 0.636719] [G loss: 0.944409] [G acc: 0.164062]\n",
      "258 [D loss: 0.594384] [D acc: 0.718750] [G loss: 0.830762] [G acc: 0.328125]\n",
      "259 [D loss: 0.615126] [D acc: 0.648438] [G loss: 1.023876] [G acc: 0.070312]\n",
      "260 [D loss: 0.585530] [D acc: 0.714844] [G loss: 0.466111] [G acc: 0.867188]\n",
      "261 [D loss: 0.726189] [D acc: 0.515625] [G loss: 1.224062] [G acc: 0.000000]\n",
      "262 [D loss: 0.686612] [D acc: 0.546875] [G loss: 0.733414] [G acc: 0.437500]\n",
      "263 [D loss: 0.637850] [D acc: 0.652344] [G loss: 0.921755] [G acc: 0.132812]\n",
      "264 [D loss: 0.634571] [D acc: 0.652344] [G loss: 0.788992] [G acc: 0.335938]\n",
      "265 [D loss: 0.609449] [D acc: 0.703125] [G loss: 0.754489] [G acc: 0.375000]\n",
      "266 [D loss: 0.606668] [D acc: 0.726562] [G loss: 0.953862] [G acc: 0.101562]\n",
      "267 [D loss: 0.594265] [D acc: 0.687500] [G loss: 0.599336] [G acc: 0.687500]\n",
      "268 [D loss: 0.638686] [D acc: 0.585938] [G loss: 1.235317] [G acc: 0.015625]\n",
      "269 [D loss: 0.682226] [D acc: 0.570312] [G loss: 0.570580] [G acc: 0.710938]\n",
      "270 [D loss: 0.700626] [D acc: 0.531250] [G loss: 1.105989] [G acc: 0.015625]\n",
      "271 [D loss: 0.625653] [D acc: 0.625000] [G loss: 0.702554] [G acc: 0.523438]\n",
      "272 [D loss: 0.635128] [D acc: 0.613281] [G loss: 0.937611] [G acc: 0.117188]\n",
      "273 [D loss: 0.610419] [D acc: 0.632812] [G loss: 0.696045] [G acc: 0.539062]\n",
      "274 [D loss: 0.590449] [D acc: 0.726562] [G loss: 0.989731] [G acc: 0.093750]\n",
      "275 [D loss: 0.642791] [D acc: 0.621094] [G loss: 0.732482] [G acc: 0.515625]\n",
      "276 [D loss: 0.606313] [D acc: 0.640625] [G loss: 1.172432] [G acc: 0.046875]\n",
      "277 [D loss: 0.633804] [D acc: 0.625000] [G loss: 0.537203] [G acc: 0.773438]\n",
      "278 [D loss: 0.691369] [D acc: 0.578125] [G loss: 1.277568] [G acc: 0.000000]\n",
      "279 [D loss: 0.657196] [D acc: 0.578125] [G loss: 0.629292] [G acc: 0.640625]\n",
      "280 [D loss: 0.627596] [D acc: 0.648438] [G loss: 1.003594] [G acc: 0.085938]\n",
      "281 [D loss: 0.608124] [D acc: 0.683594] [G loss: 0.697868] [G acc: 0.523438]\n",
      "282 [D loss: 0.573594] [D acc: 0.707031] [G loss: 0.962015] [G acc: 0.179688]\n",
      "283 [D loss: 0.572076] [D acc: 0.757812] [G loss: 0.766331] [G acc: 0.359375]\n",
      "284 [D loss: 0.624128] [D acc: 0.675781] [G loss: 1.174654] [G acc: 0.031250]\n",
      "285 [D loss: 0.621418] [D acc: 0.675781] [G loss: 0.495528] [G acc: 0.851562]\n",
      "286 [D loss: 0.709178] [D acc: 0.554688] [G loss: 1.222571] [G acc: 0.015625]\n",
      "287 [D loss: 0.684607] [D acc: 0.597656] [G loss: 0.630276] [G acc: 0.617188]\n",
      "288 [D loss: 0.629770] [D acc: 0.613281] [G loss: 0.947713] [G acc: 0.132812]\n",
      "289 [D loss: 0.604919] [D acc: 0.710938] [G loss: 0.755281] [G acc: 0.398438]\n",
      "290 [D loss: 0.606005] [D acc: 0.703125] [G loss: 0.937290] [G acc: 0.179688]\n",
      "291 [D loss: 0.619269] [D acc: 0.628906] [G loss: 0.702692] [G acc: 0.484375]\n",
      "292 [D loss: 0.635215] [D acc: 0.605469] [G loss: 1.186531] [G acc: 0.054688]\n",
      "293 [D loss: 0.642985] [D acc: 0.605469] [G loss: 0.604051] [G acc: 0.648438]\n",
      "294 [D loss: 0.662125] [D acc: 0.597656] [G loss: 1.162793] [G acc: 0.031250]\n",
      "295 [D loss: 0.655557] [D acc: 0.585938] [G loss: 0.646987] [G acc: 0.625000]\n",
      "296 [D loss: 0.653213] [D acc: 0.601562] [G loss: 0.985600] [G acc: 0.140625]\n",
      "297 [D loss: 0.608337] [D acc: 0.683594] [G loss: 0.760955] [G acc: 0.421875]\n",
      "298 [D loss: 0.602446] [D acc: 0.675781] [G loss: 1.020967] [G acc: 0.140625]\n",
      "299 [D loss: 0.615839] [D acc: 0.675781] [G loss: 0.747903] [G acc: 0.453125]\n",
      "300 [D loss: 0.590724] [D acc: 0.679688] [G loss: 1.023251] [G acc: 0.148438]\n",
      "301 [D loss: 0.586593] [D acc: 0.710938] [G loss: 0.795215] [G acc: 0.390625]\n",
      "302 [D loss: 0.585138] [D acc: 0.695312] [G loss: 1.219678] [G acc: 0.093750]\n",
      "303 [D loss: 0.620528] [D acc: 0.664062] [G loss: 0.601179] [G acc: 0.632812]\n",
      "304 [D loss: 0.606409] [D acc: 0.621094] [G loss: 1.732621] [G acc: 0.000000]\n",
      "305 [D loss: 0.847682] [D acc: 0.507812] [G loss: 0.543317] [G acc: 0.773438]\n",
      "306 [D loss: 0.644254] [D acc: 0.652344] [G loss: 1.018049] [G acc: 0.101562]\n",
      "307 [D loss: 0.617236] [D acc: 0.695312] [G loss: 0.823717] [G acc: 0.320312]\n",
      "308 [D loss: 0.616322] [D acc: 0.664062] [G loss: 0.975646] [G acc: 0.140625]\n",
      "309 [D loss: 0.632908] [D acc: 0.636719] [G loss: 0.697003] [G acc: 0.507812]\n",
      "310 [D loss: 0.586551] [D acc: 0.699219] [G loss: 1.134097] [G acc: 0.046875]\n",
      "311 [D loss: 0.636034] [D acc: 0.613281] [G loss: 0.579661] [G acc: 0.710938]\n",
      "312 [D loss: 0.689717] [D acc: 0.585938] [G loss: 1.178182] [G acc: 0.054688]\n",
      "313 [D loss: 0.647601] [D acc: 0.574219] [G loss: 0.746311] [G acc: 0.437500]\n",
      "314 [D loss: 0.615062] [D acc: 0.683594] [G loss: 0.991638] [G acc: 0.148438]\n",
      "315 [D loss: 0.607521] [D acc: 0.679688] [G loss: 0.727484] [G acc: 0.523438]\n",
      "316 [D loss: 0.598870] [D acc: 0.703125] [G loss: 0.974330] [G acc: 0.132812]\n",
      "317 [D loss: 0.601643] [D acc: 0.703125] [G loss: 0.730265] [G acc: 0.492188]\n",
      "318 [D loss: 0.578490] [D acc: 0.707031] [G loss: 1.207772] [G acc: 0.039062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319 [D loss: 0.623788] [D acc: 0.628906] [G loss: 0.460183] [G acc: 0.859375]\n",
      "320 [D loss: 0.720128] [D acc: 0.531250] [G loss: 1.388968] [G acc: 0.007812]\n",
      "321 [D loss: 0.696439] [D acc: 0.542969] [G loss: 0.712122] [G acc: 0.515625]\n",
      "322 [D loss: 0.607433] [D acc: 0.656250] [G loss: 0.902680] [G acc: 0.250000]\n",
      "323 [D loss: 0.622641] [D acc: 0.632812] [G loss: 0.836454] [G acc: 0.289062]\n",
      "324 [D loss: 0.611799] [D acc: 0.656250] [G loss: 0.898619] [G acc: 0.257812]\n",
      "325 [D loss: 0.587243] [D acc: 0.687500] [G loss: 0.753140] [G acc: 0.445312]\n",
      "326 [D loss: 0.580315] [D acc: 0.703125] [G loss: 1.111020] [G acc: 0.085938]\n",
      "327 [D loss: 0.617725] [D acc: 0.660156] [G loss: 0.818911] [G acc: 0.359375]\n",
      "328 [D loss: 0.576668] [D acc: 0.671875] [G loss: 1.321727] [G acc: 0.023438]\n",
      "329 [D loss: 0.649420] [D acc: 0.593750] [G loss: 0.553408] [G acc: 0.742188]\n",
      "330 [D loss: 0.659523] [D acc: 0.593750] [G loss: 1.288012] [G acc: 0.031250]\n",
      "331 [D loss: 0.661829] [D acc: 0.589844] [G loss: 0.700911] [G acc: 0.515625]\n",
      "332 [D loss: 0.644147] [D acc: 0.636719] [G loss: 1.023679] [G acc: 0.125000]\n",
      "333 [D loss: 0.595906] [D acc: 0.691406] [G loss: 0.865878] [G acc: 0.335938]\n",
      "334 [D loss: 0.588461] [D acc: 0.718750] [G loss: 0.914940] [G acc: 0.218750]\n",
      "335 [D loss: 0.565332] [D acc: 0.734375] [G loss: 0.950380] [G acc: 0.242188]\n",
      "336 [D loss: 0.600555] [D acc: 0.683594] [G loss: 0.820004] [G acc: 0.375000]\n",
      "337 [D loss: 0.615182] [D acc: 0.667969] [G loss: 1.273745] [G acc: 0.039062]\n",
      "338 [D loss: 0.620715] [D acc: 0.625000] [G loss: 0.509319] [G acc: 0.773438]\n",
      "339 [D loss: 0.711962] [D acc: 0.554688] [G loss: 1.314812] [G acc: 0.000000]\n",
      "340 [D loss: 0.659061] [D acc: 0.574219] [G loss: 0.739384] [G acc: 0.437500]\n",
      "341 [D loss: 0.586106] [D acc: 0.714844] [G loss: 0.968539] [G acc: 0.187500]\n",
      "342 [D loss: 0.604977] [D acc: 0.683594] [G loss: 0.880983] [G acc: 0.257812]\n",
      "343 [D loss: 0.561144] [D acc: 0.691406] [G loss: 0.987752] [G acc: 0.156250]\n",
      "344 [D loss: 0.615510] [D acc: 0.652344] [G loss: 0.698641] [G acc: 0.578125]\n",
      "345 [D loss: 0.595446] [D acc: 0.671875] [G loss: 1.269022] [G acc: 0.070312]\n",
      "346 [D loss: 0.619853] [D acc: 0.652344] [G loss: 0.687451] [G acc: 0.531250]\n",
      "347 [D loss: 0.605512] [D acc: 0.644531] [G loss: 1.357078] [G acc: 0.007812]\n",
      "348 [D loss: 0.684361] [D acc: 0.574219] [G loss: 0.696578] [G acc: 0.507812]\n",
      "349 [D loss: 0.590894] [D acc: 0.687500] [G loss: 1.159423] [G acc: 0.132812]\n",
      "350 [D loss: 0.613514] [D acc: 0.683594] [G loss: 0.835617] [G acc: 0.359375]\n",
      "351 [D loss: 0.565840] [D acc: 0.730469] [G loss: 1.101725] [G acc: 0.140625]\n",
      "352 [D loss: 0.552705] [D acc: 0.726562] [G loss: 0.645147] [G acc: 0.609375]\n",
      "353 [D loss: 0.666024] [D acc: 0.613281] [G loss: 1.435715] [G acc: 0.015625]\n",
      "354 [D loss: 0.688583] [D acc: 0.562500] [G loss: 0.556925] [G acc: 0.695312]\n",
      "355 [D loss: 0.661211] [D acc: 0.566406] [G loss: 1.196697] [G acc: 0.015625]\n",
      "356 [D loss: 0.649096] [D acc: 0.613281] [G loss: 0.783641] [G acc: 0.429688]\n",
      "357 [D loss: 0.599513] [D acc: 0.644531] [G loss: 0.892347] [G acc: 0.250000]\n",
      "358 [D loss: 0.602839] [D acc: 0.656250] [G loss: 0.828425] [G acc: 0.320312]\n",
      "359 [D loss: 0.586811] [D acc: 0.699219] [G loss: 1.030055] [G acc: 0.156250]\n",
      "360 [D loss: 0.588170] [D acc: 0.710938] [G loss: 0.832235] [G acc: 0.351562]\n",
      "361 [D loss: 0.592723] [D acc: 0.703125] [G loss: 1.119390] [G acc: 0.078125]\n",
      "362 [D loss: 0.629358] [D acc: 0.636719] [G loss: 0.551732] [G acc: 0.710938]\n",
      "363 [D loss: 0.615713] [D acc: 0.621094] [G loss: 1.382760] [G acc: 0.000000]\n",
      "364 [D loss: 0.656019] [D acc: 0.585938] [G loss: 0.589614] [G acc: 0.664062]\n",
      "365 [D loss: 0.729485] [D acc: 0.535156] [G loss: 1.220470] [G acc: 0.023438]\n",
      "366 [D loss: 0.631646] [D acc: 0.613281] [G loss: 0.727887] [G acc: 0.476562]\n",
      "367 [D loss: 0.612846] [D acc: 0.644531] [G loss: 0.956795] [G acc: 0.179688]\n",
      "368 [D loss: 0.574208] [D acc: 0.722656] [G loss: 0.843334] [G acc: 0.335938]\n",
      "369 [D loss: 0.543792] [D acc: 0.773438] [G loss: 1.012603] [G acc: 0.187500]\n",
      "370 [D loss: 0.582293] [D acc: 0.722656] [G loss: 0.906456] [G acc: 0.273438]\n",
      "371 [D loss: 0.554917] [D acc: 0.750000] [G loss: 1.034633] [G acc: 0.179688]\n",
      "372 [D loss: 0.545406] [D acc: 0.738281] [G loss: 0.930515] [G acc: 0.273438]\n",
      "373 [D loss: 0.596101] [D acc: 0.664062] [G loss: 1.623996] [G acc: 0.015625]\n",
      "374 [D loss: 0.701120] [D acc: 0.589844] [G loss: 0.376009] [G acc: 0.882812]\n",
      "375 [D loss: 0.762128] [D acc: 0.558594] [G loss: 1.264325] [G acc: 0.046875]\n",
      "376 [D loss: 0.644265] [D acc: 0.605469] [G loss: 0.758965] [G acc: 0.468750]\n",
      "377 [D loss: 0.596211] [D acc: 0.687500] [G loss: 0.887860] [G acc: 0.265625]\n",
      "378 [D loss: 0.587108] [D acc: 0.691406] [G loss: 0.898008] [G acc: 0.257812]\n",
      "379 [D loss: 0.574488] [D acc: 0.726562] [G loss: 1.076621] [G acc: 0.109375]\n",
      "380 [D loss: 0.575000] [D acc: 0.695312] [G loss: 0.782442] [G acc: 0.421875]\n",
      "381 [D loss: 0.595661] [D acc: 0.679688] [G loss: 1.152525] [G acc: 0.101562]\n",
      "382 [D loss: 0.588740] [D acc: 0.683594] [G loss: 0.635035] [G acc: 0.640625]\n",
      "383 [D loss: 0.626659] [D acc: 0.656250] [G loss: 1.422519] [G acc: 0.007812]\n",
      "384 [D loss: 0.662253] [D acc: 0.601562] [G loss: 0.622577] [G acc: 0.609375]\n",
      "385 [D loss: 0.635728] [D acc: 0.605469] [G loss: 1.152202] [G acc: 0.062500]\n",
      "386 [D loss: 0.617716] [D acc: 0.679688] [G loss: 0.821132] [G acc: 0.328125]\n",
      "387 [D loss: 0.588465] [D acc: 0.707031] [G loss: 0.943541] [G acc: 0.226562]\n",
      "388 [D loss: 0.544921] [D acc: 0.742188] [G loss: 0.784766] [G acc: 0.406250]\n",
      "389 [D loss: 0.582445] [D acc: 0.683594] [G loss: 1.328805] [G acc: 0.023438]\n",
      "390 [D loss: 0.624812] [D acc: 0.671875] [G loss: 0.580064] [G acc: 0.679688]\n",
      "391 [D loss: 0.675930] [D acc: 0.593750] [G loss: 1.433669] [G acc: 0.007812]\n",
      "392 [D loss: 0.650009] [D acc: 0.609375] [G loss: 0.723839] [G acc: 0.507812]\n",
      "393 [D loss: 0.632978] [D acc: 0.648438] [G loss: 1.022094] [G acc: 0.132812]\n",
      "394 [D loss: 0.580294] [D acc: 0.742188] [G loss: 0.793504] [G acc: 0.406250]\n",
      "395 [D loss: 0.597512] [D acc: 0.703125] [G loss: 1.055944] [G acc: 0.125000]\n",
      "396 [D loss: 0.583122] [D acc: 0.707031] [G loss: 0.848217] [G acc: 0.406250]\n",
      "397 [D loss: 0.576560] [D acc: 0.695312] [G loss: 1.211525] [G acc: 0.085938]\n",
      "398 [D loss: 0.601633] [D acc: 0.679688] [G loss: 0.685020] [G acc: 0.554688]\n",
      "399 [D loss: 0.605734] [D acc: 0.664062] [G loss: 1.400674] [G acc: 0.007812]\n",
      "400 [D loss: 0.628694] [D acc: 0.613281] [G loss: 0.592791] [G acc: 0.664062]\n",
      "401 [D loss: 0.654089] [D acc: 0.601562] [G loss: 1.201155] [G acc: 0.054688]\n",
      "402 [D loss: 0.595107] [D acc: 0.691406] [G loss: 0.745319] [G acc: 0.414062]\n",
      "403 [D loss: 0.591128] [D acc: 0.679688] [G loss: 1.280781] [G acc: 0.039062]\n",
      "404 [D loss: 0.613716] [D acc: 0.664062] [G loss: 0.636549] [G acc: 0.625000]\n",
      "405 [D loss: 0.676245] [D acc: 0.574219] [G loss: 1.360978] [G acc: 0.023438]\n",
      "406 [D loss: 0.656372] [D acc: 0.621094] [G loss: 0.741378] [G acc: 0.492188]\n",
      "407 [D loss: 0.606389] [D acc: 0.664062] [G loss: 0.964859] [G acc: 0.203125]\n",
      "408 [D loss: 0.585906] [D acc: 0.707031] [G loss: 0.746264] [G acc: 0.468750]\n",
      "409 [D loss: 0.603402] [D acc: 0.683594] [G loss: 1.100199] [G acc: 0.117188]\n",
      "410 [D loss: 0.588009] [D acc: 0.707031] [G loss: 0.717584] [G acc: 0.492188]\n",
      "411 [D loss: 0.624714] [D acc: 0.656250] [G loss: 1.430019] [G acc: 0.023438]\n",
      "412 [D loss: 0.633879] [D acc: 0.621094] [G loss: 0.614795] [G acc: 0.609375]\n",
      "413 [D loss: 0.607829] [D acc: 0.644531] [G loss: 1.114644] [G acc: 0.085938]\n",
      "414 [D loss: 0.576454] [D acc: 0.703125] [G loss: 0.807686] [G acc: 0.398438]\n",
      "415 [D loss: 0.629054] [D acc: 0.648438] [G loss: 1.198189] [G acc: 0.054688]\n",
      "416 [D loss: 0.596936] [D acc: 0.644531] [G loss: 0.783260] [G acc: 0.390625]\n",
      "417 [D loss: 0.596619] [D acc: 0.707031] [G loss: 1.163674] [G acc: 0.085938]\n",
      "418 [D loss: 0.618171] [D acc: 0.667969] [G loss: 0.791178] [G acc: 0.398438]\n",
      "419 [D loss: 0.586254] [D acc: 0.687500] [G loss: 1.224260] [G acc: 0.023438]\n",
      "420 [D loss: 0.586185] [D acc: 0.679688] [G loss: 0.583064] [G acc: 0.695312]\n",
      "421 [D loss: 0.634556] [D acc: 0.667969] [G loss: 1.241045] [G acc: 0.062500]\n",
      "422 [D loss: 0.614775] [D acc: 0.664062] [G loss: 0.715202] [G acc: 0.515625]\n",
      "423 [D loss: 0.608816] [D acc: 0.644531] [G loss: 1.102861] [G acc: 0.156250]\n",
      "424 [D loss: 0.597111] [D acc: 0.683594] [G loss: 0.855508] [G acc: 0.351562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425 [D loss: 0.583156] [D acc: 0.664062] [G loss: 1.331340] [G acc: 0.062500]\n",
      "426 [D loss: 0.624245] [D acc: 0.628906] [G loss: 0.678246] [G acc: 0.546875]\n",
      "427 [D loss: 0.600580] [D acc: 0.632812] [G loss: 1.269971] [G acc: 0.015625]\n",
      "428 [D loss: 0.627934] [D acc: 0.644531] [G loss: 0.692935] [G acc: 0.539062]\n",
      "429 [D loss: 0.573440] [D acc: 0.683594] [G loss: 1.126293] [G acc: 0.085938]\n",
      "430 [D loss: 0.560749] [D acc: 0.707031] [G loss: 0.834039] [G acc: 0.359375]\n",
      "431 [D loss: 0.580812] [D acc: 0.667969] [G loss: 1.207880] [G acc: 0.101562]\n",
      "432 [D loss: 0.585182] [D acc: 0.691406] [G loss: 0.556581] [G acc: 0.671875]\n",
      "433 [D loss: 0.656829] [D acc: 0.605469] [G loss: 1.511319] [G acc: 0.000000]\n",
      "434 [D loss: 0.721705] [D acc: 0.554688] [G loss: 0.677817] [G acc: 0.546875]\n",
      "435 [D loss: 0.629277] [D acc: 0.617188] [G loss: 1.006653] [G acc: 0.179688]\n",
      "436 [D loss: 0.607998] [D acc: 0.679688] [G loss: 0.933210] [G acc: 0.281250]\n",
      "437 [D loss: 0.582271] [D acc: 0.699219] [G loss: 0.994112] [G acc: 0.203125]\n",
      "438 [D loss: 0.572236] [D acc: 0.718750] [G loss: 0.940760] [G acc: 0.281250]\n",
      "439 [D loss: 0.572717] [D acc: 0.726562] [G loss: 1.162545] [G acc: 0.109375]\n",
      "440 [D loss: 0.569170] [D acc: 0.683594] [G loss: 0.768704] [G acc: 0.468750]\n",
      "441 [D loss: 0.668097] [D acc: 0.585938] [G loss: 1.388288] [G acc: 0.054688]\n",
      "442 [D loss: 0.640818] [D acc: 0.617188] [G loss: 0.639328] [G acc: 0.601562]\n",
      "443 [D loss: 0.642468] [D acc: 0.625000] [G loss: 1.256706] [G acc: 0.054688]\n",
      "444 [D loss: 0.601034] [D acc: 0.671875] [G loss: 0.768303] [G acc: 0.414062]\n",
      "445 [D loss: 0.579999] [D acc: 0.703125] [G loss: 1.056848] [G acc: 0.140625]\n",
      "446 [D loss: 0.611069] [D acc: 0.660156] [G loss: 0.770782] [G acc: 0.421875]\n",
      "447 [D loss: 0.633939] [D acc: 0.671875] [G loss: 1.252013] [G acc: 0.031250]\n",
      "448 [D loss: 0.600214] [D acc: 0.652344] [G loss: 0.690676] [G acc: 0.500000]\n",
      "449 [D loss: 0.605764] [D acc: 0.632812] [G loss: 1.292189] [G acc: 0.054688]\n",
      "450 [D loss: 0.619304] [D acc: 0.648438] [G loss: 0.731527] [G acc: 0.523438]\n",
      "451 [D loss: 0.614246] [D acc: 0.656250] [G loss: 1.234834] [G acc: 0.054688]\n",
      "452 [D loss: 0.579099] [D acc: 0.679688] [G loss: 0.700353] [G acc: 0.507812]\n",
      "453 [D loss: 0.605041] [D acc: 0.667969] [G loss: 1.392501] [G acc: 0.015625]\n",
      "454 [D loss: 0.651918] [D acc: 0.628906] [G loss: 0.685019] [G acc: 0.562500]\n",
      "455 [D loss: 0.614041] [D acc: 0.664062] [G loss: 1.102461] [G acc: 0.132812]\n",
      "456 [D loss: 0.602968] [D acc: 0.652344] [G loss: 0.910758] [G acc: 0.273438]\n",
      "457 [D loss: 0.558353] [D acc: 0.742188] [G loss: 1.201156] [G acc: 0.109375]\n",
      "458 [D loss: 0.590178] [D acc: 0.699219] [G loss: 0.746335] [G acc: 0.500000]\n",
      "459 [D loss: 0.599657] [D acc: 0.687500] [G loss: 1.517089] [G acc: 0.015625]\n",
      "460 [D loss: 0.636289] [D acc: 0.601562] [G loss: 0.486179] [G acc: 0.851562]\n",
      "461 [D loss: 0.724895] [D acc: 0.546875] [G loss: 1.199410] [G acc: 0.039062]\n",
      "462 [D loss: 0.624963] [D acc: 0.632812] [G loss: 0.814616] [G acc: 0.335938]\n",
      "463 [D loss: 0.569990] [D acc: 0.714844] [G loss: 0.973637] [G acc: 0.195312]\n",
      "464 [D loss: 0.554150] [D acc: 0.726562] [G loss: 1.086103] [G acc: 0.101562]\n",
      "465 [D loss: 0.591235] [D acc: 0.699219] [G loss: 0.694923] [G acc: 0.546875]\n",
      "466 [D loss: 0.620976] [D acc: 0.609375] [G loss: 1.349340] [G acc: 0.039062]\n",
      "467 [D loss: 0.616720] [D acc: 0.656250] [G loss: 0.800832] [G acc: 0.421875]\n",
      "468 [D loss: 0.625634] [D acc: 0.613281] [G loss: 1.117294] [G acc: 0.117188]\n",
      "469 [D loss: 0.584630] [D acc: 0.726562] [G loss: 0.730865] [G acc: 0.500000]\n",
      "470 [D loss: 0.588082] [D acc: 0.707031] [G loss: 1.150734] [G acc: 0.085938]\n",
      "471 [D loss: 0.571317] [D acc: 0.710938] [G loss: 0.916967] [G acc: 0.257812]\n",
      "472 [D loss: 0.571674] [D acc: 0.722656] [G loss: 1.274085] [G acc: 0.070312]\n",
      "473 [D loss: 0.537174] [D acc: 0.753906] [G loss: 0.706309] [G acc: 0.554688]\n",
      "474 [D loss: 0.615060] [D acc: 0.648438] [G loss: 1.733116] [G acc: 0.023438]\n",
      "475 [D loss: 0.683734] [D acc: 0.605469] [G loss: 0.562812] [G acc: 0.734375]\n",
      "476 [D loss: 0.702259] [D acc: 0.574219] [G loss: 1.209383] [G acc: 0.046875]\n",
      "477 [D loss: 0.605643] [D acc: 0.648438] [G loss: 0.862697] [G acc: 0.312500]\n",
      "478 [D loss: 0.571737] [D acc: 0.695312] [G loss: 1.032072] [G acc: 0.148438]\n",
      "479 [D loss: 0.577703] [D acc: 0.695312] [G loss: 0.878158] [G acc: 0.281250]\n",
      "480 [D loss: 0.569119] [D acc: 0.691406] [G loss: 1.033228] [G acc: 0.226562]\n",
      "481 [D loss: 0.527517] [D acc: 0.757812] [G loss: 1.036123] [G acc: 0.195312]\n",
      "482 [D loss: 0.525808] [D acc: 0.734375] [G loss: 1.210140] [G acc: 0.117188]\n",
      "483 [D loss: 0.585508] [D acc: 0.707031] [G loss: 0.672418] [G acc: 0.609375]\n",
      "484 [D loss: 0.594700] [D acc: 0.644531] [G loss: 1.634560] [G acc: 0.023438]\n",
      "485 [D loss: 0.689374] [D acc: 0.585938] [G loss: 0.434999] [G acc: 0.859375]\n",
      "486 [D loss: 0.713364] [D acc: 0.523438] [G loss: 1.141603] [G acc: 0.078125]\n",
      "487 [D loss: 0.602805] [D acc: 0.695312] [G loss: 0.908702] [G acc: 0.304688]\n",
      "488 [D loss: 0.557271] [D acc: 0.714844] [G loss: 1.059587] [G acc: 0.156250]\n",
      "489 [D loss: 0.572718] [D acc: 0.679688] [G loss: 1.044680] [G acc: 0.140625]\n",
      "490 [D loss: 0.568708] [D acc: 0.699219] [G loss: 1.156339] [G acc: 0.117188]\n",
      "491 [D loss: 0.558462] [D acc: 0.722656] [G loss: 1.070418] [G acc: 0.187500]\n",
      "492 [D loss: 0.568398] [D acc: 0.722656] [G loss: 1.237435] [G acc: 0.101562]\n",
      "493 [D loss: 0.593092] [D acc: 0.679688] [G loss: 0.650692] [G acc: 0.593750]\n",
      "494 [D loss: 0.609096] [D acc: 0.628906] [G loss: 1.578940] [G acc: 0.007812]\n",
      "495 [D loss: 0.621304] [D acc: 0.613281] [G loss: 0.579611] [G acc: 0.750000]\n",
      "496 [D loss: 0.637007] [D acc: 0.636719] [G loss: 1.211702] [G acc: 0.085938]\n",
      "497 [D loss: 0.537485] [D acc: 0.730469] [G loss: 0.865234] [G acc: 0.343750]\n",
      "498 [D loss: 0.551484] [D acc: 0.714844] [G loss: 1.091969] [G acc: 0.156250]\n",
      "499 [D loss: 0.549826] [D acc: 0.722656] [G loss: 0.928015] [G acc: 0.312500]\n",
      "500 [D loss: 0.587101] [D acc: 0.660156] [G loss: 1.292854] [G acc: 0.117188]\n",
      "501 [D loss: 0.611416] [D acc: 0.679688] [G loss: 0.717474] [G acc: 0.539062]\n",
      "502 [D loss: 0.596497] [D acc: 0.617188] [G loss: 1.506029] [G acc: 0.039062]\n",
      "503 [D loss: 0.610985] [D acc: 0.652344] [G loss: 0.485425] [G acc: 0.757812]\n",
      "504 [D loss: 0.689047] [D acc: 0.578125] [G loss: 1.447514] [G acc: 0.031250]\n",
      "505 [D loss: 0.656453] [D acc: 0.660156] [G loss: 0.877889] [G acc: 0.328125]\n",
      "506 [D loss: 0.563087] [D acc: 0.691406] [G loss: 1.084621] [G acc: 0.148438]\n",
      "507 [D loss: 0.608722] [D acc: 0.675781] [G loss: 0.939945] [G acc: 0.289062]\n",
      "508 [D loss: 0.568276] [D acc: 0.714844] [G loss: 1.005623] [G acc: 0.203125]\n",
      "509 [D loss: 0.531931] [D acc: 0.753906] [G loss: 1.072483] [G acc: 0.203125]\n",
      "510 [D loss: 0.548842] [D acc: 0.730469] [G loss: 0.880925] [G acc: 0.320312]\n",
      "511 [D loss: 0.568597] [D acc: 0.695312] [G loss: 1.806322] [G acc: 0.007812]\n",
      "512 [D loss: 0.681599] [D acc: 0.601562] [G loss: 0.482337] [G acc: 0.789062]\n",
      "513 [D loss: 0.739518] [D acc: 0.539062] [G loss: 1.403500] [G acc: 0.046875]\n",
      "514 [D loss: 0.585989] [D acc: 0.687500] [G loss: 0.824781] [G acc: 0.359375]\n",
      "515 [D loss: 0.616454] [D acc: 0.628906] [G loss: 1.024005] [G acc: 0.187500]\n",
      "516 [D loss: 0.580594] [D acc: 0.722656] [G loss: 0.974754] [G acc: 0.203125]\n",
      "517 [D loss: 0.508670] [D acc: 0.808594] [G loss: 1.032852] [G acc: 0.164062]\n",
      "518 [D loss: 0.547919] [D acc: 0.738281] [G loss: 1.193351] [G acc: 0.140625]\n",
      "519 [D loss: 0.583304] [D acc: 0.695312] [G loss: 0.850262] [G acc: 0.367188]\n",
      "520 [D loss: 0.612706] [D acc: 0.617188] [G loss: 1.523233] [G acc: 0.023438]\n",
      "521 [D loss: 0.594271] [D acc: 0.660156] [G loss: 0.620152] [G acc: 0.656250]\n",
      "522 [D loss: 0.578195] [D acc: 0.683594] [G loss: 1.216115] [G acc: 0.085938]\n",
      "523 [D loss: 0.578333] [D acc: 0.687500] [G loss: 0.827892] [G acc: 0.398438]\n",
      "524 [D loss: 0.582924] [D acc: 0.683594] [G loss: 1.359815] [G acc: 0.070312]\n",
      "525 [D loss: 0.639353] [D acc: 0.656250] [G loss: 0.685793] [G acc: 0.562500]\n",
      "526 [D loss: 0.662972] [D acc: 0.644531] [G loss: 1.317038] [G acc: 0.062500]\n",
      "527 [D loss: 0.599112] [D acc: 0.671875] [G loss: 0.807459] [G acc: 0.367188]\n",
      "528 [D loss: 0.604768] [D acc: 0.652344] [G loss: 1.174690] [G acc: 0.093750]\n",
      "529 [D loss: 0.575317] [D acc: 0.707031] [G loss: 0.952154] [G acc: 0.289062]\n",
      "530 [D loss: 0.534068] [D acc: 0.773438] [G loss: 1.375280] [G acc: 0.078125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531 [D loss: 0.626185] [D acc: 0.652344] [G loss: 0.665570] [G acc: 0.593750]\n",
      "532 [D loss: 0.652562] [D acc: 0.609375] [G loss: 1.534308] [G acc: 0.023438]\n",
      "533 [D loss: 0.632993] [D acc: 0.625000] [G loss: 0.685710] [G acc: 0.546875]\n",
      "534 [D loss: 0.640217] [D acc: 0.640625] [G loss: 1.077641] [G acc: 0.164062]\n",
      "535 [D loss: 0.601811] [D acc: 0.671875] [G loss: 0.915242] [G acc: 0.281250]\n",
      "536 [D loss: 0.531404] [D acc: 0.769531] [G loss: 1.079855] [G acc: 0.195312]\n",
      "537 [D loss: 0.550749] [D acc: 0.753906] [G loss: 0.884818] [G acc: 0.351562]\n",
      "538 [D loss: 0.479409] [D acc: 0.789062] [G loss: 1.401227] [G acc: 0.046875]\n",
      "539 [D loss: 0.545215] [D acc: 0.726562] [G loss: 0.947258] [G acc: 0.289062]\n",
      "540 [D loss: 0.506197] [D acc: 0.761719] [G loss: 1.584939] [G acc: 0.062500]\n",
      "541 [D loss: 0.607481] [D acc: 0.683594] [G loss: 0.581804] [G acc: 0.671875]\n",
      "542 [D loss: 0.647872] [D acc: 0.648438] [G loss: 1.679256] [G acc: 0.015625]\n",
      "543 [D loss: 0.666947] [D acc: 0.609375] [G loss: 0.643028] [G acc: 0.570312]\n",
      "544 [D loss: 0.567408] [D acc: 0.675781] [G loss: 1.105260] [G acc: 0.140625]\n",
      "545 [D loss: 0.521080] [D acc: 0.765625] [G loss: 0.893779] [G acc: 0.382812]\n",
      "546 [D loss: 0.540786] [D acc: 0.738281] [G loss: 1.189834] [G acc: 0.132812]\n",
      "547 [D loss: 0.510275] [D acc: 0.753906] [G loss: 0.741501] [G acc: 0.531250]\n",
      "548 [D loss: 0.531325] [D acc: 0.710938] [G loss: 1.528909] [G acc: 0.039062]\n",
      "549 [D loss: 0.687476] [D acc: 0.589844] [G loss: 0.692289] [G acc: 0.515625]\n",
      "550 [D loss: 0.613059] [D acc: 0.632812] [G loss: 1.576113] [G acc: 0.015625]\n",
      "551 [D loss: 0.683931] [D acc: 0.613281] [G loss: 0.835937] [G acc: 0.351562]\n",
      "552 [D loss: 0.627271] [D acc: 0.636719] [G loss: 0.966172] [G acc: 0.250000]\n",
      "553 [D loss: 0.576544] [D acc: 0.699219] [G loss: 0.982765] [G acc: 0.226562]\n",
      "554 [D loss: 0.555143] [D acc: 0.718750] [G loss: 1.067982] [G acc: 0.187500]\n",
      "555 [D loss: 0.538592] [D acc: 0.714844] [G loss: 0.854370] [G acc: 0.390625]\n",
      "556 [D loss: 0.540909] [D acc: 0.726562] [G loss: 1.284455] [G acc: 0.093750]\n",
      "557 [D loss: 0.496210] [D acc: 0.765625] [G loss: 0.987386] [G acc: 0.304688]\n",
      "558 [D loss: 0.555734] [D acc: 0.742188] [G loss: 1.296665] [G acc: 0.101562]\n",
      "559 [D loss: 0.568229] [D acc: 0.714844] [G loss: 0.470627] [G acc: 0.804688]\n",
      "560 [D loss: 0.834210] [D acc: 0.562500] [G loss: 1.699356] [G acc: 0.015625]\n",
      "561 [D loss: 0.709807] [D acc: 0.582031] [G loss: 0.892707] [G acc: 0.343750]\n",
      "562 [D loss: 0.606142] [D acc: 0.660156] [G loss: 1.149117] [G acc: 0.125000]\n",
      "563 [D loss: 0.562047] [D acc: 0.722656] [G loss: 0.943483] [G acc: 0.257812]\n",
      "564 [D loss: 0.586044] [D acc: 0.691406] [G loss: 1.112040] [G acc: 0.156250]\n",
      "565 [D loss: 0.537372] [D acc: 0.765625] [G loss: 0.876400] [G acc: 0.398438]\n",
      "566 [D loss: 0.568836] [D acc: 0.710938] [G loss: 1.109141] [G acc: 0.179688]\n",
      "567 [D loss: 0.524705] [D acc: 0.730469] [G loss: 0.939521] [G acc: 0.304688]\n",
      "568 [D loss: 0.530437] [D acc: 0.734375] [G loss: 1.481774] [G acc: 0.039062]\n",
      "569 [D loss: 0.556113] [D acc: 0.675781] [G loss: 0.602600] [G acc: 0.656250]\n",
      "570 [D loss: 0.638994] [D acc: 0.632812] [G loss: 1.555639] [G acc: 0.023438]\n",
      "571 [D loss: 0.629694] [D acc: 0.609375] [G loss: 0.636818] [G acc: 0.632812]\n",
      "572 [D loss: 0.625129] [D acc: 0.601562] [G loss: 1.111332] [G acc: 0.132812]\n",
      "573 [D loss: 0.550540] [D acc: 0.734375] [G loss: 0.966474] [G acc: 0.257812]\n",
      "574 [D loss: 0.541645] [D acc: 0.738281] [G loss: 1.059212] [G acc: 0.156250]\n",
      "575 [D loss: 0.556528] [D acc: 0.726562] [G loss: 1.069455] [G acc: 0.179688]\n",
      "576 [D loss: 0.533752] [D acc: 0.738281] [G loss: 1.101147] [G acc: 0.242188]\n",
      "577 [D loss: 0.551414] [D acc: 0.738281] [G loss: 1.228364] [G acc: 0.125000]\n",
      "578 [D loss: 0.534572] [D acc: 0.742188] [G loss: 1.167564] [G acc: 0.187500]\n",
      "579 [D loss: 0.554305] [D acc: 0.699219] [G loss: 1.037148] [G acc: 0.265625]\n",
      "580 [D loss: 0.524229] [D acc: 0.718750] [G loss: 1.309601] [G acc: 0.164062]\n",
      "581 [D loss: 0.549406] [D acc: 0.718750] [G loss: 0.705305] [G acc: 0.515625]\n",
      "582 [D loss: 0.667751] [D acc: 0.605469] [G loss: 2.195260] [G acc: 0.000000]\n",
      "583 [D loss: 0.861429] [D acc: 0.535156] [G loss: 0.617554] [G acc: 0.609375]\n",
      "584 [D loss: 0.599494] [D acc: 0.683594] [G loss: 0.970840] [G acc: 0.296875]\n",
      "585 [D loss: 0.557244] [D acc: 0.691406] [G loss: 1.043393] [G acc: 0.195312]\n",
      "586 [D loss: 0.601232] [D acc: 0.652344] [G loss: 1.083183] [G acc: 0.125000]\n",
      "587 [D loss: 0.566657] [D acc: 0.710938] [G loss: 0.874719] [G acc: 0.398438]\n",
      "588 [D loss: 0.512457] [D acc: 0.765625] [G loss: 1.184016] [G acc: 0.125000]\n",
      "589 [D loss: 0.517245] [D acc: 0.738281] [G loss: 1.004296] [G acc: 0.296875]\n",
      "590 [D loss: 0.526467] [D acc: 0.738281] [G loss: 1.461564] [G acc: 0.070312]\n",
      "591 [D loss: 0.543469] [D acc: 0.710938] [G loss: 0.769916] [G acc: 0.492188]\n",
      "592 [D loss: 0.613723] [D acc: 0.679688] [G loss: 1.721099] [G acc: 0.000000]\n",
      "593 [D loss: 0.606447] [D acc: 0.632812] [G loss: 0.655801] [G acc: 0.601562]\n",
      "594 [D loss: 0.670416] [D acc: 0.593750] [G loss: 1.323239] [G acc: 0.070312]\n",
      "595 [D loss: 0.578930] [D acc: 0.671875] [G loss: 0.763821] [G acc: 0.500000]\n",
      "596 [D loss: 0.592797] [D acc: 0.675781] [G loss: 1.261212] [G acc: 0.101562]\n",
      "597 [D loss: 0.548871] [D acc: 0.738281] [G loss: 0.909087] [G acc: 0.312500]\n",
      "598 [D loss: 0.551301] [D acc: 0.722656] [G loss: 1.318996] [G acc: 0.078125]\n",
      "599 [D loss: 0.516776] [D acc: 0.746094] [G loss: 0.778367] [G acc: 0.476562]\n",
      "600 [D loss: 0.609371] [D acc: 0.671875] [G loss: 1.702170] [G acc: 0.015625]\n",
      "601 [D loss: 0.640535] [D acc: 0.609375] [G loss: 0.673298] [G acc: 0.578125]\n",
      "602 [D loss: 0.581130] [D acc: 0.667969] [G loss: 1.188666] [G acc: 0.132812]\n",
      "603 [D loss: 0.585662] [D acc: 0.691406] [G loss: 0.881755] [G acc: 0.359375]\n",
      "604 [D loss: 0.537509] [D acc: 0.726562] [G loss: 1.154109] [G acc: 0.164062]\n",
      "605 [D loss: 0.578842] [D acc: 0.683594] [G loss: 0.798772] [G acc: 0.460938]\n",
      "606 [D loss: 0.561373] [D acc: 0.703125] [G loss: 1.396790] [G acc: 0.062500]\n",
      "607 [D loss: 0.541132] [D acc: 0.703125] [G loss: 0.724574] [G acc: 0.500000]\n",
      "608 [D loss: 0.556568] [D acc: 0.703125] [G loss: 1.654660] [G acc: 0.031250]\n",
      "609 [D loss: 0.616698] [D acc: 0.632812] [G loss: 0.589055] [G acc: 0.679688]\n",
      "610 [D loss: 0.660053] [D acc: 0.636719] [G loss: 1.408148] [G acc: 0.078125]\n",
      "611 [D loss: 0.588867] [D acc: 0.679688] [G loss: 0.829267] [G acc: 0.375000]\n",
      "612 [D loss: 0.532174] [D acc: 0.753906] [G loss: 1.076945] [G acc: 0.218750]\n",
      "613 [D loss: 0.506638] [D acc: 0.785156] [G loss: 0.966210] [G acc: 0.312500]\n",
      "614 [D loss: 0.530048] [D acc: 0.734375] [G loss: 1.120172] [G acc: 0.226562]\n",
      "615 [D loss: 0.492511] [D acc: 0.757812] [G loss: 0.954853] [G acc: 0.312500]\n",
      "616 [D loss: 0.561438] [D acc: 0.695312] [G loss: 1.368689] [G acc: 0.140625]\n",
      "617 [D loss: 0.538026] [D acc: 0.757812] [G loss: 0.896769] [G acc: 0.398438]\n",
      "618 [D loss: 0.590551] [D acc: 0.667969] [G loss: 1.754641] [G acc: 0.078125]\n",
      "619 [D loss: 0.650826] [D acc: 0.667969] [G loss: 0.482375] [G acc: 0.781250]\n",
      "620 [D loss: 0.688570] [D acc: 0.601562] [G loss: 1.520845] [G acc: 0.031250]\n",
      "621 [D loss: 0.573182] [D acc: 0.710938] [G loss: 0.952381] [G acc: 0.265625]\n",
      "622 [D loss: 0.508910] [D acc: 0.765625] [G loss: 1.023254] [G acc: 0.242188]\n",
      "623 [D loss: 0.508016] [D acc: 0.773438] [G loss: 1.047597] [G acc: 0.242188]\n",
      "624 [D loss: 0.551276] [D acc: 0.726562] [G loss: 1.331054] [G acc: 0.093750]\n",
      "625 [D loss: 0.497543] [D acc: 0.750000] [G loss: 1.101124] [G acc: 0.195312]\n",
      "626 [D loss: 0.567274] [D acc: 0.703125] [G loss: 1.502882] [G acc: 0.093750]\n",
      "627 [D loss: 0.555503] [D acc: 0.714844] [G loss: 0.823546] [G acc: 0.414062]\n",
      "628 [D loss: 0.601359] [D acc: 0.656250] [G loss: 1.741510] [G acc: 0.015625]\n",
      "629 [D loss: 0.618523] [D acc: 0.628906] [G loss: 0.594772] [G acc: 0.640625]\n",
      "630 [D loss: 0.626572] [D acc: 0.656250] [G loss: 1.351297] [G acc: 0.078125]\n",
      "631 [D loss: 0.600267] [D acc: 0.687500] [G loss: 0.859519] [G acc: 0.367188]\n",
      "632 [D loss: 0.593346] [D acc: 0.687500] [G loss: 1.091024] [G acc: 0.164062]\n",
      "633 [D loss: 0.558433] [D acc: 0.718750] [G loss: 1.125118] [G acc: 0.140625]\n",
      "634 [D loss: 0.571940] [D acc: 0.722656] [G loss: 0.920895] [G acc: 0.351562]\n",
      "635 [D loss: 0.529321] [D acc: 0.750000] [G loss: 1.309994] [G acc: 0.132812]\n",
      "636 [D loss: 0.528871] [D acc: 0.726562] [G loss: 0.857758] [G acc: 0.351562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637 [D loss: 0.535440] [D acc: 0.742188] [G loss: 1.521984] [G acc: 0.015625]\n",
      "638 [D loss: 0.553223] [D acc: 0.703125] [G loss: 0.786996] [G acc: 0.445312]\n",
      "639 [D loss: 0.579592] [D acc: 0.683594] [G loss: 1.415549] [G acc: 0.085938]\n",
      "640 [D loss: 0.588039] [D acc: 0.683594] [G loss: 0.822831] [G acc: 0.375000]\n",
      "641 [D loss: 0.547997] [D acc: 0.714844] [G loss: 1.435794] [G acc: 0.085938]\n",
      "642 [D loss: 0.563215] [D acc: 0.714844] [G loss: 0.757896] [G acc: 0.453125]\n",
      "643 [D loss: 0.613321] [D acc: 0.660156] [G loss: 1.406456] [G acc: 0.054688]\n",
      "644 [D loss: 0.582744] [D acc: 0.667969] [G loss: 0.703163] [G acc: 0.492188]\n",
      "645 [D loss: 0.578304] [D acc: 0.675781] [G loss: 1.456939] [G acc: 0.046875]\n",
      "646 [D loss: 0.576491] [D acc: 0.667969] [G loss: 0.744026] [G acc: 0.500000]\n",
      "647 [D loss: 0.614047] [D acc: 0.632812] [G loss: 1.225527] [G acc: 0.101562]\n",
      "648 [D loss: 0.605592] [D acc: 0.687500] [G loss: 0.862189] [G acc: 0.335938]\n",
      "649 [D loss: 0.579566] [D acc: 0.710938] [G loss: 1.196631] [G acc: 0.140625]\n",
      "650 [D loss: 0.533977] [D acc: 0.726562] [G loss: 0.861299] [G acc: 0.359375]\n",
      "651 [D loss: 0.590821] [D acc: 0.687500] [G loss: 1.460884] [G acc: 0.046875]\n",
      "652 [D loss: 0.588519] [D acc: 0.675781] [G loss: 0.733175] [G acc: 0.523438]\n",
      "653 [D loss: 0.588503] [D acc: 0.671875] [G loss: 1.258618] [G acc: 0.125000]\n",
      "654 [D loss: 0.537143] [D acc: 0.738281] [G loss: 0.835691] [G acc: 0.390625]\n",
      "655 [D loss: 0.553248] [D acc: 0.710938] [G loss: 1.414447] [G acc: 0.070312]\n",
      "656 [D loss: 0.547719] [D acc: 0.699219] [G loss: 0.794230] [G acc: 0.476562]\n",
      "657 [D loss: 0.554636] [D acc: 0.699219] [G loss: 1.355319] [G acc: 0.070312]\n",
      "658 [D loss: 0.569165] [D acc: 0.691406] [G loss: 0.747198] [G acc: 0.484375]\n",
      "659 [D loss: 0.577902] [D acc: 0.695312] [G loss: 1.533762] [G acc: 0.039062]\n",
      "660 [D loss: 0.586348] [D acc: 0.675781] [G loss: 0.749860] [G acc: 0.507812]\n",
      "661 [D loss: 0.626799] [D acc: 0.605469] [G loss: 1.454033] [G acc: 0.062500]\n",
      "662 [D loss: 0.569758] [D acc: 0.699219] [G loss: 0.796076] [G acc: 0.421875]\n",
      "663 [D loss: 0.579389] [D acc: 0.699219] [G loss: 1.367201] [G acc: 0.101562]\n",
      "664 [D loss: 0.571040] [D acc: 0.707031] [G loss: 0.779673] [G acc: 0.421875]\n",
      "665 [D loss: 0.590967] [D acc: 0.660156] [G loss: 1.333445] [G acc: 0.101562]\n",
      "666 [D loss: 0.611748] [D acc: 0.656250] [G loss: 0.773674] [G acc: 0.445312]\n",
      "667 [D loss: 0.555468] [D acc: 0.714844] [G loss: 1.285366] [G acc: 0.101562]\n",
      "668 [D loss: 0.537113] [D acc: 0.742188] [G loss: 0.761555] [G acc: 0.492188]\n",
      "669 [D loss: 0.542909] [D acc: 0.726562] [G loss: 1.378690] [G acc: 0.093750]\n",
      "670 [D loss: 0.543848] [D acc: 0.734375] [G loss: 0.771407] [G acc: 0.476562]\n",
      "671 [D loss: 0.570545] [D acc: 0.687500] [G loss: 1.635445] [G acc: 0.031250]\n",
      "672 [D loss: 0.673810] [D acc: 0.613281] [G loss: 0.720915] [G acc: 0.515625]\n",
      "673 [D loss: 0.623391] [D acc: 0.628906] [G loss: 1.395159] [G acc: 0.093750]\n",
      "674 [D loss: 0.586994] [D acc: 0.667969] [G loss: 0.928139] [G acc: 0.320312]\n",
      "675 [D loss: 0.576740] [D acc: 0.687500] [G loss: 1.098715] [G acc: 0.140625]\n",
      "676 [D loss: 0.523519] [D acc: 0.753906] [G loss: 0.981015] [G acc: 0.234375]\n",
      "677 [D loss: 0.524252] [D acc: 0.742188] [G loss: 1.141458] [G acc: 0.234375]\n",
      "678 [D loss: 0.501314] [D acc: 0.746094] [G loss: 0.829122] [G acc: 0.390625]\n",
      "679 [D loss: 0.557949] [D acc: 0.714844] [G loss: 2.062236] [G acc: 0.007812]\n",
      "680 [D loss: 0.666500] [D acc: 0.613281] [G loss: 0.646124] [G acc: 0.625000]\n",
      "681 [D loss: 0.651055] [D acc: 0.648438] [G loss: 1.439116] [G acc: 0.039062]\n",
      "682 [D loss: 0.578973] [D acc: 0.671875] [G loss: 1.039275] [G acc: 0.226562]\n",
      "683 [D loss: 0.551731] [D acc: 0.738281] [G loss: 1.150476] [G acc: 0.195312]\n",
      "684 [D loss: 0.538620] [D acc: 0.746094] [G loss: 1.022978] [G acc: 0.328125]\n",
      "685 [D loss: 0.562396] [D acc: 0.734375] [G loss: 1.070616] [G acc: 0.218750]\n",
      "686 [D loss: 0.534118] [D acc: 0.742188] [G loss: 0.971104] [G acc: 0.250000]\n",
      "687 [D loss: 0.523551] [D acc: 0.738281] [G loss: 1.187838] [G acc: 0.179688]\n",
      "688 [D loss: 0.531684] [D acc: 0.746094] [G loss: 0.851692] [G acc: 0.421875]\n",
      "689 [D loss: 0.565285] [D acc: 0.703125] [G loss: 1.889264] [G acc: 0.015625]\n",
      "690 [D loss: 0.619719] [D acc: 0.660156] [G loss: 0.531909] [G acc: 0.703125]\n",
      "691 [D loss: 0.719887] [D acc: 0.585938] [G loss: 1.220044] [G acc: 0.070312]\n",
      "692 [D loss: 0.595601] [D acc: 0.687500] [G loss: 0.861210] [G acc: 0.390625]\n",
      "693 [D loss: 0.514464] [D acc: 0.750000] [G loss: 1.204404] [G acc: 0.156250]\n",
      "694 [D loss: 0.584348] [D acc: 0.703125] [G loss: 0.950626] [G acc: 0.296875]\n",
      "695 [D loss: 0.508431] [D acc: 0.757812] [G loss: 1.130151] [G acc: 0.171875]\n",
      "696 [D loss: 0.513236] [D acc: 0.777344] [G loss: 1.123009] [G acc: 0.273438]\n",
      "697 [D loss: 0.576979] [D acc: 0.710938] [G loss: 1.348938] [G acc: 0.140625]\n",
      "698 [D loss: 0.538237] [D acc: 0.714844] [G loss: 0.797060] [G acc: 0.445312]\n",
      "699 [D loss: 0.566358] [D acc: 0.683594] [G loss: 1.851445] [G acc: 0.000000]\n",
      "700 [D loss: 0.664625] [D acc: 0.613281] [G loss: 0.627512] [G acc: 0.648438]\n",
      "701 [D loss: 0.627260] [D acc: 0.625000] [G loss: 1.241257] [G acc: 0.054688]\n",
      "702 [D loss: 0.516236] [D acc: 0.785156] [G loss: 0.999547] [G acc: 0.257812]\n",
      "703 [D loss: 0.535250] [D acc: 0.707031] [G loss: 1.116322] [G acc: 0.164062]\n",
      "704 [D loss: 0.540609] [D acc: 0.738281] [G loss: 0.928342] [G acc: 0.304688]\n",
      "705 [D loss: 0.529034] [D acc: 0.722656] [G loss: 1.350467] [G acc: 0.109375]\n",
      "706 [D loss: 0.554998] [D acc: 0.726562] [G loss: 0.900156] [G acc: 0.406250]\n",
      "707 [D loss: 0.529927] [D acc: 0.738281] [G loss: 1.992715] [G acc: 0.007812]\n",
      "708 [D loss: 0.660213] [D acc: 0.644531] [G loss: 0.486575] [G acc: 0.757812]\n",
      "709 [D loss: 0.725075] [D acc: 0.578125] [G loss: 1.323508] [G acc: 0.046875]\n",
      "710 [D loss: 0.569080] [D acc: 0.718750] [G loss: 0.880586] [G acc: 0.320312]\n",
      "711 [D loss: 0.544756] [D acc: 0.710938] [G loss: 1.063754] [G acc: 0.218750]\n",
      "712 [D loss: 0.535810] [D acc: 0.726562] [G loss: 1.173012] [G acc: 0.125000]\n",
      "713 [D loss: 0.578187] [D acc: 0.679688] [G loss: 0.990486] [G acc: 0.304688]\n",
      "714 [D loss: 0.533872] [D acc: 0.746094] [G loss: 1.229880] [G acc: 0.125000]\n",
      "715 [D loss: 0.535878] [D acc: 0.710938] [G loss: 0.989606] [G acc: 0.242188]\n",
      "716 [D loss: 0.543281] [D acc: 0.714844] [G loss: 1.606388] [G acc: 0.039062]\n",
      "717 [D loss: 0.634504] [D acc: 0.656250] [G loss: 0.596211] [G acc: 0.687500]\n",
      "718 [D loss: 0.586920] [D acc: 0.679688] [G loss: 1.349666] [G acc: 0.109375]\n",
      "719 [D loss: 0.572841] [D acc: 0.707031] [G loss: 0.910727] [G acc: 0.343750]\n",
      "720 [D loss: 0.558419] [D acc: 0.687500] [G loss: 1.510328] [G acc: 0.078125]\n",
      "721 [D loss: 0.561657] [D acc: 0.675781] [G loss: 0.911851] [G acc: 0.304688]\n",
      "722 [D loss: 0.574804] [D acc: 0.703125] [G loss: 1.345361] [G acc: 0.062500]\n",
      "723 [D loss: 0.610372] [D acc: 0.640625] [G loss: 0.731584] [G acc: 0.468750]\n",
      "724 [D loss: 0.541847] [D acc: 0.699219] [G loss: 1.261937] [G acc: 0.109375]\n",
      "725 [D loss: 0.556060] [D acc: 0.703125] [G loss: 1.104324] [G acc: 0.281250]\n",
      "726 [D loss: 0.511208] [D acc: 0.789062] [G loss: 1.084836] [G acc: 0.203125]\n",
      "727 [D loss: 0.514783] [D acc: 0.742188] [G loss: 1.302951] [G acc: 0.078125]\n",
      "728 [D loss: 0.542653] [D acc: 0.710938] [G loss: 0.677858] [G acc: 0.578125]\n",
      "729 [D loss: 0.662488] [D acc: 0.632812] [G loss: 1.838094] [G acc: 0.023438]\n",
      "730 [D loss: 0.676385] [D acc: 0.597656] [G loss: 0.674259] [G acc: 0.546875]\n",
      "731 [D loss: 0.630914] [D acc: 0.636719] [G loss: 1.099566] [G acc: 0.195312]\n",
      "732 [D loss: 0.541060] [D acc: 0.734375] [G loss: 1.011770] [G acc: 0.242188]\n",
      "733 [D loss: 0.497761] [D acc: 0.816406] [G loss: 1.151046] [G acc: 0.242188]\n",
      "734 [D loss: 0.510306] [D acc: 0.765625] [G loss: 1.076039] [G acc: 0.226562]\n",
      "735 [D loss: 0.544621] [D acc: 0.718750] [G loss: 1.223853] [G acc: 0.171875]\n",
      "736 [D loss: 0.526322] [D acc: 0.742188] [G loss: 1.024435] [G acc: 0.289062]\n",
      "737 [D loss: 0.539709] [D acc: 0.718750] [G loss: 1.608420] [G acc: 0.062500]\n",
      "738 [D loss: 0.525892] [D acc: 0.726562] [G loss: 0.712555] [G acc: 0.539062]\n",
      "739 [D loss: 0.690016] [D acc: 0.613281] [G loss: 1.738431] [G acc: 0.015625]\n",
      "740 [D loss: 0.631659] [D acc: 0.621094] [G loss: 0.754828] [G acc: 0.453125]\n",
      "741 [D loss: 0.573915] [D acc: 0.707031] [G loss: 1.215292] [G acc: 0.109375]\n",
      "742 [D loss: 0.576227] [D acc: 0.707031] [G loss: 1.045276] [G acc: 0.234375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743 [D loss: 0.510860] [D acc: 0.757812] [G loss: 1.202475] [G acc: 0.171875]\n",
      "744 [D loss: 0.540702] [D acc: 0.699219] [G loss: 1.093896] [G acc: 0.218750]\n",
      "745 [D loss: 0.491507] [D acc: 0.765625] [G loss: 1.334984] [G acc: 0.179688]\n",
      "746 [D loss: 0.547353] [D acc: 0.742188] [G loss: 0.961179] [G acc: 0.320312]\n",
      "747 [D loss: 0.505491] [D acc: 0.769531] [G loss: 1.739915] [G acc: 0.039062]\n",
      "748 [D loss: 0.602474] [D acc: 0.675781] [G loss: 0.564268] [G acc: 0.687500]\n",
      "749 [D loss: 0.689134] [D acc: 0.574219] [G loss: 1.497529] [G acc: 0.039062]\n",
      "750 [D loss: 0.576133] [D acc: 0.679688] [G loss: 0.826141] [G acc: 0.382812]\n",
      "751 [D loss: 0.552346] [D acc: 0.722656] [G loss: 1.321478] [G acc: 0.093750]\n",
      "752 [D loss: 0.523364] [D acc: 0.726562] [G loss: 0.862814] [G acc: 0.390625]\n",
      "753 [D loss: 0.527650] [D acc: 0.761719] [G loss: 1.101521] [G acc: 0.203125]\n",
      "754 [D loss: 0.530204] [D acc: 0.730469] [G loss: 1.260773] [G acc: 0.164062]\n",
      "755 [D loss: 0.553742] [D acc: 0.691406] [G loss: 1.062476] [G acc: 0.210938]\n",
      "756 [D loss: 0.489317] [D acc: 0.796875] [G loss: 1.299908] [G acc: 0.164062]\n",
      "757 [D loss: 0.486796] [D acc: 0.781250] [G loss: 1.025271] [G acc: 0.328125]\n",
      "758 [D loss: 0.513448] [D acc: 0.777344] [G loss: 1.549382] [G acc: 0.062500]\n",
      "759 [D loss: 0.599691] [D acc: 0.687500] [G loss: 0.670282] [G acc: 0.593750]\n",
      "760 [D loss: 0.614605] [D acc: 0.656250] [G loss: 1.729225] [G acc: 0.054688]\n",
      "761 [D loss: 0.645822] [D acc: 0.632812] [G loss: 0.860790] [G acc: 0.414062]\n",
      "762 [D loss: 0.639151] [D acc: 0.664062] [G loss: 1.516533] [G acc: 0.039062]\n",
      "763 [D loss: 0.633358] [D acc: 0.648438] [G loss: 0.911376] [G acc: 0.382812]\n",
      "764 [D loss: 0.592108] [D acc: 0.683594] [G loss: 1.224954] [G acc: 0.125000]\n",
      "765 [D loss: 0.558857] [D acc: 0.746094] [G loss: 1.038377] [G acc: 0.242188]\n",
      "766 [D loss: 0.518928] [D acc: 0.718750] [G loss: 1.414760] [G acc: 0.078125]\n",
      "767 [D loss: 0.510653] [D acc: 0.761719] [G loss: 0.860570] [G acc: 0.406250]\n",
      "768 [D loss: 0.572368] [D acc: 0.687500] [G loss: 1.430896] [G acc: 0.093750]\n",
      "769 [D loss: 0.526821] [D acc: 0.753906] [G loss: 0.883892] [G acc: 0.406250]\n",
      "770 [D loss: 0.583887] [D acc: 0.703125] [G loss: 1.887364] [G acc: 0.031250]\n",
      "771 [D loss: 0.642081] [D acc: 0.625000] [G loss: 0.630037] [G acc: 0.546875]\n",
      "772 [D loss: 0.690410] [D acc: 0.671875] [G loss: 1.193837] [G acc: 0.085938]\n",
      "773 [D loss: 0.583465] [D acc: 0.683594] [G loss: 1.009151] [G acc: 0.242188]\n",
      "774 [D loss: 0.532496] [D acc: 0.750000] [G loss: 1.081657] [G acc: 0.250000]\n",
      "775 [D loss: 0.536547] [D acc: 0.730469] [G loss: 1.033792] [G acc: 0.234375]\n",
      "776 [D loss: 0.558737] [D acc: 0.675781] [G loss: 1.255035] [G acc: 0.164062]\n",
      "777 [D loss: 0.521215] [D acc: 0.734375] [G loss: 0.901164] [G acc: 0.390625]\n",
      "778 [D loss: 0.563827] [D acc: 0.718750] [G loss: 1.331936] [G acc: 0.085938]\n",
      "779 [D loss: 0.517259] [D acc: 0.753906] [G loss: 0.928624] [G acc: 0.335938]\n",
      "780 [D loss: 0.559416] [D acc: 0.671875] [G loss: 1.751880] [G acc: 0.015625]\n",
      "781 [D loss: 0.553101] [D acc: 0.699219] [G loss: 0.732072] [G acc: 0.539062]\n",
      "782 [D loss: 0.665572] [D acc: 0.632812] [G loss: 1.638736] [G acc: 0.007812]\n",
      "783 [D loss: 0.637077] [D acc: 0.671875] [G loss: 0.695963] [G acc: 0.562500]\n",
      "784 [D loss: 0.599173] [D acc: 0.648438] [G loss: 1.201706] [G acc: 0.132812]\n",
      "785 [D loss: 0.560708] [D acc: 0.699219] [G loss: 0.882212] [G acc: 0.390625]\n",
      "786 [D loss: 0.528054] [D acc: 0.753906] [G loss: 1.385517] [G acc: 0.085938]\n",
      "787 [D loss: 0.593645] [D acc: 0.691406] [G loss: 0.772666] [G acc: 0.476562]\n",
      "788 [D loss: 0.583777] [D acc: 0.691406] [G loss: 1.209107] [G acc: 0.179688]\n",
      "789 [D loss: 0.557242] [D acc: 0.726562] [G loss: 0.874102] [G acc: 0.382812]\n",
      "790 [D loss: 0.549267] [D acc: 0.703125] [G loss: 1.282234] [G acc: 0.140625]\n",
      "791 [D loss: 0.523233] [D acc: 0.718750] [G loss: 0.917626] [G acc: 0.328125]\n",
      "792 [D loss: 0.538228] [D acc: 0.734375] [G loss: 1.541386] [G acc: 0.046875]\n",
      "793 [D loss: 0.554038] [D acc: 0.714844] [G loss: 0.675418] [G acc: 0.593750]\n",
      "794 [D loss: 0.619434] [D acc: 0.679688] [G loss: 1.878877] [G acc: 0.007812]\n",
      "795 [D loss: 0.664099] [D acc: 0.613281] [G loss: 0.684769] [G acc: 0.585938]\n",
      "796 [D loss: 0.589947] [D acc: 0.644531] [G loss: 1.177477] [G acc: 0.148438]\n",
      "797 [D loss: 0.549784] [D acc: 0.714844] [G loss: 1.004188] [G acc: 0.234375]\n",
      "798 [D loss: 0.529305] [D acc: 0.765625] [G loss: 1.119663] [G acc: 0.179688]\n",
      "799 [D loss: 0.517149] [D acc: 0.769531] [G loss: 0.990659] [G acc: 0.265625]\n",
      "800 [D loss: 0.539053] [D acc: 0.742188] [G loss: 1.324528] [G acc: 0.125000]\n",
      "801 [D loss: 0.517485] [D acc: 0.765625] [G loss: 0.996268] [G acc: 0.281250]\n",
      "802 [D loss: 0.538241] [D acc: 0.742188] [G loss: 1.504766] [G acc: 0.054688]\n",
      "803 [D loss: 0.535653] [D acc: 0.738281] [G loss: 1.048764] [G acc: 0.250000]\n",
      "804 [D loss: 0.540661] [D acc: 0.714844] [G loss: 1.595762] [G acc: 0.062500]\n",
      "805 [D loss: 0.554037] [D acc: 0.722656] [G loss: 0.788777] [G acc: 0.492188]\n",
      "806 [D loss: 0.585065] [D acc: 0.675781] [G loss: 1.809754] [G acc: 0.007812]\n",
      "807 [D loss: 0.629111] [D acc: 0.660156] [G loss: 0.743948] [G acc: 0.507812]\n",
      "808 [D loss: 0.595956] [D acc: 0.652344] [G loss: 1.438597] [G acc: 0.101562]\n",
      "809 [D loss: 0.576547] [D acc: 0.691406] [G loss: 0.963952] [G acc: 0.304688]\n",
      "810 [D loss: 0.529302] [D acc: 0.718750] [G loss: 1.114095] [G acc: 0.195312]\n",
      "811 [D loss: 0.551497] [D acc: 0.722656] [G loss: 1.269388] [G acc: 0.125000]\n",
      "812 [D loss: 0.562114] [D acc: 0.714844] [G loss: 1.062659] [G acc: 0.273438]\n",
      "813 [D loss: 0.535412] [D acc: 0.726562] [G loss: 1.491425] [G acc: 0.101562]\n",
      "814 [D loss: 0.553928] [D acc: 0.699219] [G loss: 0.752631] [G acc: 0.492188]\n",
      "815 [D loss: 0.644928] [D acc: 0.621094] [G loss: 1.836797] [G acc: 0.054688]\n",
      "816 [D loss: 0.673983] [D acc: 0.585938] [G loss: 0.684508] [G acc: 0.546875]\n",
      "817 [D loss: 0.616513] [D acc: 0.656250] [G loss: 1.079370] [G acc: 0.179688]\n",
      "818 [D loss: 0.537040] [D acc: 0.714844] [G loss: 1.116560] [G acc: 0.179688]\n",
      "819 [D loss: 0.535896] [D acc: 0.730469] [G loss: 1.070800] [G acc: 0.203125]\n",
      "820 [D loss: 0.527689] [D acc: 0.722656] [G loss: 1.176773] [G acc: 0.187500]\n",
      "821 [D loss: 0.522015] [D acc: 0.757812] [G loss: 1.109235] [G acc: 0.187500]\n",
      "822 [D loss: 0.548170] [D acc: 0.722656] [G loss: 1.279569] [G acc: 0.148438]\n",
      "823 [D loss: 0.582990] [D acc: 0.710938] [G loss: 0.978891] [G acc: 0.289062]\n",
      "824 [D loss: 0.524598] [D acc: 0.734375] [G loss: 1.821258] [G acc: 0.015625]\n",
      "825 [D loss: 0.653179] [D acc: 0.636719] [G loss: 0.610158] [G acc: 0.617188]\n",
      "826 [D loss: 0.687190] [D acc: 0.605469] [G loss: 1.358935] [G acc: 0.062500]\n",
      "827 [D loss: 0.548931] [D acc: 0.699219] [G loss: 0.915852] [G acc: 0.296875]\n",
      "828 [D loss: 0.485360] [D acc: 0.789062] [G loss: 1.247616] [G acc: 0.140625]\n",
      "829 [D loss: 0.535669] [D acc: 0.722656] [G loss: 1.039175] [G acc: 0.195312]\n",
      "830 [D loss: 0.511676] [D acc: 0.746094] [G loss: 1.223649] [G acc: 0.179688]\n",
      "831 [D loss: 0.527828] [D acc: 0.710938] [G loss: 1.094026] [G acc: 0.242188]\n",
      "832 [D loss: 0.526884] [D acc: 0.730469] [G loss: 1.405010] [G acc: 0.101562]\n",
      "833 [D loss: 0.468550] [D acc: 0.792969] [G loss: 1.297193] [G acc: 0.156250]\n",
      "834 [D loss: 0.482876] [D acc: 0.750000] [G loss: 1.807323] [G acc: 0.046875]\n",
      "835 [D loss: 0.525000] [D acc: 0.722656] [G loss: 0.597663] [G acc: 0.695312]\n",
      "836 [D loss: 0.765130] [D acc: 0.601562] [G loss: 2.191347] [G acc: 0.007812]\n",
      "837 [D loss: 0.794560] [D acc: 0.566406] [G loss: 0.734570] [G acc: 0.523438]\n",
      "838 [D loss: 0.554762] [D acc: 0.714844] [G loss: 1.056890] [G acc: 0.242188]\n",
      "839 [D loss: 0.546572] [D acc: 0.757812] [G loss: 1.059060] [G acc: 0.226562]\n",
      "840 [D loss: 0.559606] [D acc: 0.710938] [G loss: 1.091632] [G acc: 0.140625]\n",
      "841 [D loss: 0.534281] [D acc: 0.726562] [G loss: 1.279459] [G acc: 0.140625]\n",
      "842 [D loss: 0.550280] [D acc: 0.707031] [G loss: 1.186359] [G acc: 0.179688]\n",
      "843 [D loss: 0.523243] [D acc: 0.738281] [G loss: 0.939924] [G acc: 0.367188]\n",
      "844 [D loss: 0.529036] [D acc: 0.750000] [G loss: 1.435691] [G acc: 0.046875]\n",
      "845 [D loss: 0.632795] [D acc: 0.660156] [G loss: 0.952926] [G acc: 0.328125]\n",
      "846 [D loss: 0.481470] [D acc: 0.746094] [G loss: 1.527714] [G acc: 0.085938]\n",
      "847 [D loss: 0.552723] [D acc: 0.703125] [G loss: 0.756993] [G acc: 0.554688]\n",
      "848 [D loss: 0.664372] [D acc: 0.640625] [G loss: 1.783615] [G acc: 0.023438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849 [D loss: 0.624916] [D acc: 0.667969] [G loss: 0.888749] [G acc: 0.343750]\n",
      "850 [D loss: 0.582049] [D acc: 0.710938] [G loss: 1.147088] [G acc: 0.203125]\n",
      "851 [D loss: 0.558836] [D acc: 0.707031] [G loss: 1.165813] [G acc: 0.156250]\n",
      "852 [D loss: 0.497906] [D acc: 0.796875] [G loss: 1.112293] [G acc: 0.187500]\n",
      "853 [D loss: 0.473528] [D acc: 0.808594] [G loss: 1.265977] [G acc: 0.179688]\n",
      "854 [D loss: 0.524149] [D acc: 0.765625] [G loss: 1.294122] [G acc: 0.117188]\n",
      "855 [D loss: 0.528470] [D acc: 0.683594] [G loss: 1.071848] [G acc: 0.289062]\n",
      "856 [D loss: 0.589208] [D acc: 0.691406] [G loss: 1.429785] [G acc: 0.117188]\n",
      "857 [D loss: 0.495423] [D acc: 0.746094] [G loss: 0.940057] [G acc: 0.359375]\n",
      "858 [D loss: 0.604921] [D acc: 0.660156] [G loss: 1.972707] [G acc: 0.015625]\n",
      "859 [D loss: 0.721129] [D acc: 0.597656] [G loss: 0.684480] [G acc: 0.562500]\n",
      "860 [D loss: 0.665388] [D acc: 0.597656] [G loss: 1.262942] [G acc: 0.085938]\n",
      "861 [D loss: 0.584479] [D acc: 0.679688] [G loss: 0.978773] [G acc: 0.289062]\n",
      "862 [D loss: 0.558130] [D acc: 0.679688] [G loss: 1.131192] [G acc: 0.156250]\n",
      "863 [D loss: 0.498948] [D acc: 0.785156] [G loss: 1.042464] [G acc: 0.257812]\n",
      "864 [D loss: 0.508751] [D acc: 0.753906] [G loss: 1.209259] [G acc: 0.148438]\n",
      "865 [D loss: 0.508236] [D acc: 0.753906] [G loss: 0.956158] [G acc: 0.296875]\n",
      "866 [D loss: 0.536716] [D acc: 0.730469] [G loss: 1.340021] [G acc: 0.093750]\n",
      "867 [D loss: 0.529073] [D acc: 0.734375] [G loss: 0.941139] [G acc: 0.273438]\n",
      "868 [D loss: 0.517735] [D acc: 0.742188] [G loss: 1.430249] [G acc: 0.085938]\n",
      "869 [D loss: 0.534141] [D acc: 0.742188] [G loss: 0.884957] [G acc: 0.406250]\n",
      "870 [D loss: 0.515366] [D acc: 0.730469] [G loss: 1.377858] [G acc: 0.125000]\n",
      "871 [D loss: 0.530997] [D acc: 0.695312] [G loss: 0.932517] [G acc: 0.375000]\n",
      "872 [D loss: 0.526767] [D acc: 0.710938] [G loss: 1.781441] [G acc: 0.015625]\n",
      "873 [D loss: 0.663422] [D acc: 0.605469] [G loss: 0.595763] [G acc: 0.656250]\n",
      "874 [D loss: 0.611813] [D acc: 0.656250] [G loss: 1.471447] [G acc: 0.031250]\n",
      "875 [D loss: 0.554007] [D acc: 0.710938] [G loss: 0.896960] [G acc: 0.296875]\n",
      "876 [D loss: 0.550091] [D acc: 0.769531] [G loss: 1.255872] [G acc: 0.109375]\n",
      "877 [D loss: 0.509771] [D acc: 0.750000] [G loss: 1.062577] [G acc: 0.234375]\n",
      "878 [D loss: 0.580011] [D acc: 0.714844] [G loss: 1.214400] [G acc: 0.109375]\n",
      "879 [D loss: 0.498287] [D acc: 0.769531] [G loss: 1.126184] [G acc: 0.257812]\n",
      "880 [D loss: 0.521186] [D acc: 0.703125] [G loss: 1.492888] [G acc: 0.117188]\n",
      "881 [D loss: 0.532338] [D acc: 0.722656] [G loss: 0.882273] [G acc: 0.390625]\n",
      "882 [D loss: 0.625566] [D acc: 0.667969] [G loss: 1.761043] [G acc: 0.015625]\n",
      "883 [D loss: 0.658964] [D acc: 0.617188] [G loss: 0.731039] [G acc: 0.507812]\n",
      "884 [D loss: 0.584004] [D acc: 0.707031] [G loss: 1.346953] [G acc: 0.062500]\n",
      "885 [D loss: 0.497759] [D acc: 0.769531] [G loss: 1.061211] [G acc: 0.164062]\n",
      "886 [D loss: 0.544834] [D acc: 0.738281] [G loss: 1.362116] [G acc: 0.101562]\n",
      "887 [D loss: 0.521471] [D acc: 0.734375] [G loss: 0.936124] [G acc: 0.312500]\n",
      "888 [D loss: 0.558248] [D acc: 0.703125] [G loss: 1.440761] [G acc: 0.117188]\n",
      "889 [D loss: 0.579654] [D acc: 0.687500] [G loss: 0.933482] [G acc: 0.351562]\n",
      "890 [D loss: 0.546698] [D acc: 0.742188] [G loss: 1.468623] [G acc: 0.093750]\n",
      "891 [D loss: 0.549274] [D acc: 0.707031] [G loss: 0.924728] [G acc: 0.367188]\n",
      "892 [D loss: 0.483179] [D acc: 0.773438] [G loss: 1.405932] [G acc: 0.101562]\n",
      "893 [D loss: 0.500940] [D acc: 0.757812] [G loss: 0.872768] [G acc: 0.421875]\n",
      "894 [D loss: 0.614591] [D acc: 0.695312] [G loss: 1.952322] [G acc: 0.000000]\n",
      "895 [D loss: 0.683458] [D acc: 0.644531] [G loss: 0.658059] [G acc: 0.632812]\n",
      "896 [D loss: 0.651331] [D acc: 0.597656] [G loss: 1.352488] [G acc: 0.109375]\n",
      "897 [D loss: 0.515117] [D acc: 0.765625] [G loss: 0.908419] [G acc: 0.304688]\n",
      "898 [D loss: 0.534929] [D acc: 0.718750] [G loss: 1.162725] [G acc: 0.164062]\n",
      "899 [D loss: 0.539311] [D acc: 0.722656] [G loss: 1.041160] [G acc: 0.265625]\n",
      "900 [D loss: 0.508620] [D acc: 0.753906] [G loss: 1.074307] [G acc: 0.226562]\n",
      "901 [D loss: 0.517078] [D acc: 0.738281] [G loss: 1.261737] [G acc: 0.140625]\n",
      "902 [D loss: 0.553851] [D acc: 0.691406] [G loss: 1.031236] [G acc: 0.312500]\n",
      "903 [D loss: 0.591440] [D acc: 0.695312] [G loss: 1.336859] [G acc: 0.101562]\n",
      "904 [D loss: 0.537553] [D acc: 0.722656] [G loss: 0.874894] [G acc: 0.390625]\n",
      "905 [D loss: 0.504322] [D acc: 0.746094] [G loss: 1.587497] [G acc: 0.046875]\n",
      "906 [D loss: 0.535235] [D acc: 0.714844] [G loss: 0.803398] [G acc: 0.484375]\n",
      "907 [D loss: 0.621204] [D acc: 0.679688] [G loss: 1.643925] [G acc: 0.023438]\n",
      "908 [D loss: 0.547093] [D acc: 0.714844] [G loss: 0.796300] [G acc: 0.390625]\n",
      "909 [D loss: 0.579181] [D acc: 0.687500] [G loss: 1.603043] [G acc: 0.046875]\n",
      "910 [D loss: 0.550462] [D acc: 0.683594] [G loss: 0.815389] [G acc: 0.476562]\n",
      "911 [D loss: 0.616738] [D acc: 0.679688] [G loss: 1.288299] [G acc: 0.132812]\n",
      "912 [D loss: 0.508011] [D acc: 0.765625] [G loss: 1.069784] [G acc: 0.273438]\n",
      "913 [D loss: 0.506106] [D acc: 0.738281] [G loss: 1.330484] [G acc: 0.117188]\n",
      "914 [D loss: 0.520945] [D acc: 0.718750] [G loss: 0.964562] [G acc: 0.351562]\n",
      "915 [D loss: 0.482498] [D acc: 0.785156] [G loss: 1.194809] [G acc: 0.148438]\n",
      "916 [D loss: 0.455180] [D acc: 0.824219] [G loss: 1.457707] [G acc: 0.109375]\n",
      "917 [D loss: 0.557254] [D acc: 0.699219] [G loss: 1.124073] [G acc: 0.273438]\n",
      "918 [D loss: 0.552824] [D acc: 0.667969] [G loss: 1.507444] [G acc: 0.070312]\n",
      "919 [D loss: 0.512743] [D acc: 0.726562] [G loss: 0.715689] [G acc: 0.570312]\n",
      "920 [D loss: 0.646274] [D acc: 0.660156] [G loss: 1.739610] [G acc: 0.046875]\n",
      "921 [D loss: 0.622237] [D acc: 0.644531] [G loss: 0.600566] [G acc: 0.640625]\n",
      "922 [D loss: 0.628518] [D acc: 0.636719] [G loss: 1.232021] [G acc: 0.140625]\n",
      "923 [D loss: 0.547507] [D acc: 0.714844] [G loss: 1.116682] [G acc: 0.164062]\n",
      "924 [D loss: 0.500293] [D acc: 0.750000] [G loss: 1.098501] [G acc: 0.273438]\n",
      "925 [D loss: 0.537436] [D acc: 0.761719] [G loss: 1.189049] [G acc: 0.171875]\n",
      "926 [D loss: 0.449167] [D acc: 0.800781] [G loss: 1.200545] [G acc: 0.179688]\n",
      "927 [D loss: 0.494360] [D acc: 0.769531] [G loss: 1.527058] [G acc: 0.078125]\n",
      "928 [D loss: 0.555833] [D acc: 0.722656] [G loss: 0.795333] [G acc: 0.500000]\n",
      "929 [D loss: 0.551597] [D acc: 0.718750] [G loss: 1.730313] [G acc: 0.046875]\n",
      "930 [D loss: 0.585945] [D acc: 0.671875] [G loss: 0.737497] [G acc: 0.515625]\n",
      "931 [D loss: 0.647436] [D acc: 0.617188] [G loss: 1.403145] [G acc: 0.070312]\n",
      "932 [D loss: 0.578326] [D acc: 0.703125] [G loss: 0.984406] [G acc: 0.304688]\n",
      "933 [D loss: 0.512019] [D acc: 0.746094] [G loss: 1.219425] [G acc: 0.164062]\n",
      "934 [D loss: 0.529688] [D acc: 0.734375] [G loss: 1.189698] [G acc: 0.187500]\n",
      "935 [D loss: 0.491945] [D acc: 0.746094] [G loss: 1.379687] [G acc: 0.156250]\n",
      "936 [D loss: 0.446800] [D acc: 0.808594] [G loss: 0.994153] [G acc: 0.335938]\n",
      "937 [D loss: 0.531501] [D acc: 0.722656] [G loss: 1.873906] [G acc: 0.046875]\n",
      "938 [D loss: 0.528276] [D acc: 0.734375] [G loss: 0.605723] [G acc: 0.640625]\n",
      "939 [D loss: 0.731165] [D acc: 0.601562] [G loss: 1.719037] [G acc: 0.039062]\n",
      "940 [D loss: 0.616362] [D acc: 0.656250] [G loss: 0.879663] [G acc: 0.343750]\n",
      "941 [D loss: 0.507724] [D acc: 0.773438] [G loss: 1.148519] [G acc: 0.195312]\n",
      "942 [D loss: 0.499683] [D acc: 0.785156] [G loss: 1.187093] [G acc: 0.210938]\n",
      "943 [D loss: 0.473946] [D acc: 0.753906] [G loss: 1.113420] [G acc: 0.289062]\n",
      "944 [D loss: 0.503803] [D acc: 0.734375] [G loss: 1.410057] [G acc: 0.085938]\n",
      "945 [D loss: 0.496290] [D acc: 0.753906] [G loss: 1.392925] [G acc: 0.156250]\n",
      "946 [D loss: 0.516108] [D acc: 0.753906] [G loss: 1.229401] [G acc: 0.195312]\n",
      "947 [D loss: 0.492619] [D acc: 0.757812] [G loss: 1.916092] [G acc: 0.054688]\n",
      "948 [D loss: 0.495870] [D acc: 0.742188] [G loss: 0.656018] [G acc: 0.601562]\n",
      "949 [D loss: 0.667837] [D acc: 0.648438] [G loss: 1.744881] [G acc: 0.039062]\n",
      "950 [D loss: 0.631011] [D acc: 0.660156] [G loss: 0.863116] [G acc: 0.312500]\n",
      "951 [D loss: 0.538302] [D acc: 0.730469] [G loss: 1.452020] [G acc: 0.093750]\n",
      "952 [D loss: 0.582154] [D acc: 0.699219] [G loss: 0.892659] [G acc: 0.406250]\n",
      "953 [D loss: 0.579812] [D acc: 0.671875] [G loss: 1.345476] [G acc: 0.117188]\n",
      "954 [D loss: 0.522408] [D acc: 0.718750] [G loss: 0.912801] [G acc: 0.328125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955 [D loss: 0.506821] [D acc: 0.750000] [G loss: 1.294474] [G acc: 0.156250]\n",
      "956 [D loss: 0.574256] [D acc: 0.695312] [G loss: 0.985399] [G acc: 0.296875]\n",
      "957 [D loss: 0.469816] [D acc: 0.796875] [G loss: 1.360988] [G acc: 0.117188]\n",
      "958 [D loss: 0.470052] [D acc: 0.781250] [G loss: 1.266369] [G acc: 0.203125]\n",
      "959 [D loss: 0.525798] [D acc: 0.707031] [G loss: 1.230961] [G acc: 0.234375]\n",
      "960 [D loss: 0.535492] [D acc: 0.726562] [G loss: 1.172941] [G acc: 0.210938]\n",
      "961 [D loss: 0.487431] [D acc: 0.761719] [G loss: 2.052067] [G acc: 0.031250]\n",
      "962 [D loss: 0.614990] [D acc: 0.664062] [G loss: 0.507547] [G acc: 0.750000]\n",
      "963 [D loss: 0.803355] [D acc: 0.585938] [G loss: 1.405712] [G acc: 0.085938]\n",
      "964 [D loss: 0.612493] [D acc: 0.640625] [G loss: 0.870338] [G acc: 0.421875]\n",
      "965 [D loss: 0.560059] [D acc: 0.726562] [G loss: 1.169303] [G acc: 0.156250]\n",
      "966 [D loss: 0.521690] [D acc: 0.714844] [G loss: 1.096796] [G acc: 0.195312]\n",
      "967 [D loss: 0.542965] [D acc: 0.726562] [G loss: 1.173765] [G acc: 0.140625]\n",
      "968 [D loss: 0.508345] [D acc: 0.730469] [G loss: 1.147553] [G acc: 0.195312]\n",
      "969 [D loss: 0.504450] [D acc: 0.753906] [G loss: 1.384806] [G acc: 0.101562]\n",
      "970 [D loss: 0.508886] [D acc: 0.738281] [G loss: 1.089336] [G acc: 0.265625]\n",
      "971 [D loss: 0.483732] [D acc: 0.746094] [G loss: 1.567685] [G acc: 0.101562]\n",
      "972 [D loss: 0.495703] [D acc: 0.773438] [G loss: 0.966705] [G acc: 0.382812]\n",
      "973 [D loss: 0.545221] [D acc: 0.734375] [G loss: 2.048453] [G acc: 0.007812]\n",
      "974 [D loss: 0.628209] [D acc: 0.695312] [G loss: 0.779136] [G acc: 0.507812]\n",
      "975 [D loss: 0.559933] [D acc: 0.679688] [G loss: 1.304080] [G acc: 0.140625]\n",
      "976 [D loss: 0.535990] [D acc: 0.753906] [G loss: 1.068666] [G acc: 0.273438]\n",
      "977 [D loss: 0.473029] [D acc: 0.777344] [G loss: 1.310385] [G acc: 0.179688]\n",
      "978 [D loss: 0.502723] [D acc: 0.757812] [G loss: 1.013561] [G acc: 0.304688]\n",
      "979 [D loss: 0.507137] [D acc: 0.750000] [G loss: 1.689166] [G acc: 0.046875]\n",
      "980 [D loss: 0.461890] [D acc: 0.750000] [G loss: 0.880495] [G acc: 0.421875]\n",
      "981 [D loss: 0.579527] [D acc: 0.714844] [G loss: 1.868561] [G acc: 0.054688]\n",
      "982 [D loss: 0.564802] [D acc: 0.703125] [G loss: 0.701587] [G acc: 0.570312]\n",
      "983 [D loss: 0.567090] [D acc: 0.703125] [G loss: 1.743644] [G acc: 0.031250]\n",
      "984 [D loss: 0.632406] [D acc: 0.652344] [G loss: 0.745492] [G acc: 0.492188]\n",
      "985 [D loss: 0.603359] [D acc: 0.695312] [G loss: 1.356692] [G acc: 0.078125]\n",
      "986 [D loss: 0.544351] [D acc: 0.703125] [G loss: 0.894199] [G acc: 0.351562]\n",
      "987 [D loss: 0.541602] [D acc: 0.714844] [G loss: 1.340058] [G acc: 0.125000]\n",
      "988 [D loss: 0.495702] [D acc: 0.765625] [G loss: 0.998646] [G acc: 0.351562]\n",
      "989 [D loss: 0.484673] [D acc: 0.757812] [G loss: 1.361127] [G acc: 0.171875]\n",
      "990 [D loss: 0.495631] [D acc: 0.757812] [G loss: 1.098813] [G acc: 0.250000]\n",
      "991 [D loss: 0.521887] [D acc: 0.757812] [G loss: 1.320339] [G acc: 0.171875]\n",
      "992 [D loss: 0.494387] [D acc: 0.753906] [G loss: 1.355379] [G acc: 0.148438]\n",
      "993 [D loss: 0.479274] [D acc: 0.773438] [G loss: 1.346661] [G acc: 0.171875]\n",
      "994 [D loss: 0.490221] [D acc: 0.750000] [G loss: 1.521043] [G acc: 0.132812]\n",
      "995 [D loss: 0.512145] [D acc: 0.750000] [G loss: 1.119733] [G acc: 0.273438]\n",
      "996 [D loss: 0.497241] [D acc: 0.777344] [G loss: 1.395491] [G acc: 0.148438]\n",
      "997 [D loss: 0.499090] [D acc: 0.738281] [G loss: 1.344273] [G acc: 0.210938]\n",
      "998 [D loss: 0.514510] [D acc: 0.734375] [G loss: 1.782017] [G acc: 0.101562]\n",
      "999 [D loss: 0.539105] [D acc: 0.730469] [G loss: 0.551631] [G acc: 0.664062]\n",
      "1000 [D loss: 0.783335] [D acc: 0.574219] [G loss: 2.006309] [G acc: 0.023438]\n",
      "1001 [D loss: 0.689134] [D acc: 0.617188] [G loss: 0.689146] [G acc: 0.570312]\n",
      "1002 [D loss: 0.534019] [D acc: 0.722656] [G loss: 1.033859] [G acc: 0.195312]\n",
      "1003 [D loss: 0.537678] [D acc: 0.718750] [G loss: 1.171869] [G acc: 0.187500]\n",
      "1004 [D loss: 0.506736] [D acc: 0.773438] [G loss: 1.184925] [G acc: 0.179688]\n",
      "1005 [D loss: 0.486561] [D acc: 0.777344] [G loss: 1.300331] [G acc: 0.171875]\n",
      "1006 [D loss: 0.493167] [D acc: 0.765625] [G loss: 1.221007] [G acc: 0.164062]\n",
      "1007 [D loss: 0.495173] [D acc: 0.753906] [G loss: 1.306885] [G acc: 0.156250]\n",
      "1008 [D loss: 0.503431] [D acc: 0.789062] [G loss: 1.191308] [G acc: 0.234375]\n",
      "1009 [D loss: 0.519491] [D acc: 0.757812] [G loss: 1.266364] [G acc: 0.195312]\n",
      "1010 [D loss: 0.477675] [D acc: 0.781250] [G loss: 1.306761] [G acc: 0.210938]\n",
      "1011 [D loss: 0.501564] [D acc: 0.757812] [G loss: 1.055814] [G acc: 0.343750]\n",
      "1012 [D loss: 0.546279] [D acc: 0.730469] [G loss: 2.085593] [G acc: 0.031250]\n",
      "1013 [D loss: 0.553056] [D acc: 0.699219] [G loss: 0.474596] [G acc: 0.781250]\n",
      "1014 [D loss: 0.807989] [D acc: 0.574219] [G loss: 1.650455] [G acc: 0.109375]\n",
      "1015 [D loss: 0.583253] [D acc: 0.691406] [G loss: 0.957446] [G acc: 0.382812]\n",
      "1016 [D loss: 0.471460] [D acc: 0.800781] [G loss: 1.284523] [G acc: 0.171875]\n",
      "1017 [D loss: 0.477918] [D acc: 0.781250] [G loss: 1.217901] [G acc: 0.203125]\n",
      "1018 [D loss: 0.557047] [D acc: 0.703125] [G loss: 1.139603] [G acc: 0.273438]\n",
      "1019 [D loss: 0.539622] [D acc: 0.718750] [G loss: 1.364234] [G acc: 0.140625]\n",
      "1020 [D loss: 0.443522] [D acc: 0.824219] [G loss: 1.206142] [G acc: 0.234375]\n",
      "1021 [D loss: 0.515153] [D acc: 0.753906] [G loss: 1.797464] [G acc: 0.117188]\n",
      "1022 [D loss: 0.547446] [D acc: 0.710938] [G loss: 0.734498] [G acc: 0.546875]\n",
      "1023 [D loss: 0.701177] [D acc: 0.613281] [G loss: 1.797960] [G acc: 0.031250]\n",
      "1024 [D loss: 0.620482] [D acc: 0.640625] [G loss: 0.762801] [G acc: 0.515625]\n",
      "1025 [D loss: 0.558908] [D acc: 0.691406] [G loss: 1.377531] [G acc: 0.109375]\n",
      "1026 [D loss: 0.559647] [D acc: 0.699219] [G loss: 1.023180] [G acc: 0.296875]\n",
      "1027 [D loss: 0.487955] [D acc: 0.804688] [G loss: 1.339913] [G acc: 0.156250]\n",
      "1028 [D loss: 0.554157] [D acc: 0.714844] [G loss: 1.061617] [G acc: 0.273438]\n",
      "1029 [D loss: 0.501061] [D acc: 0.773438] [G loss: 1.453725] [G acc: 0.132812]\n",
      "1030 [D loss: 0.501802] [D acc: 0.769531] [G loss: 0.970025] [G acc: 0.343750]\n",
      "1031 [D loss: 0.507688] [D acc: 0.761719] [G loss: 1.540116] [G acc: 0.148438]\n",
      "1032 [D loss: 0.543125] [D acc: 0.730469] [G loss: 0.716175] [G acc: 0.523438]\n",
      "1033 [D loss: 0.592179] [D acc: 0.660156] [G loss: 1.746193] [G acc: 0.062500]\n",
      "1034 [D loss: 0.594601] [D acc: 0.718750] [G loss: 0.908862] [G acc: 0.320312]\n",
      "1035 [D loss: 0.566503] [D acc: 0.722656] [G loss: 1.480964] [G acc: 0.117188]\n",
      "1036 [D loss: 0.548001] [D acc: 0.710938] [G loss: 1.076737] [G acc: 0.320312]\n",
      "1037 [D loss: 0.474678] [D acc: 0.769531] [G loss: 1.494323] [G acc: 0.109375]\n",
      "1038 [D loss: 0.505400] [D acc: 0.750000] [G loss: 1.088608] [G acc: 0.265625]\n",
      "1039 [D loss: 0.540869] [D acc: 0.726562] [G loss: 1.706709] [G acc: 0.117188]\n",
      "1040 [D loss: 0.521819] [D acc: 0.738281] [G loss: 0.670591] [G acc: 0.632812]\n",
      "1041 [D loss: 0.674517] [D acc: 0.636719] [G loss: 1.865067] [G acc: 0.078125]\n",
      "1042 [D loss: 0.583643] [D acc: 0.695312] [G loss: 0.789369] [G acc: 0.453125]\n",
      "1043 [D loss: 0.548257] [D acc: 0.714844] [G loss: 1.174075] [G acc: 0.195312]\n",
      "1044 [D loss: 0.519492] [D acc: 0.734375] [G loss: 1.048367] [G acc: 0.226562]\n",
      "1045 [D loss: 0.524246] [D acc: 0.730469] [G loss: 1.337351] [G acc: 0.125000]\n",
      "1046 [D loss: 0.472602] [D acc: 0.773438] [G loss: 0.840067] [G acc: 0.460938]\n",
      "1047 [D loss: 0.570431] [D acc: 0.648438] [G loss: 1.453708] [G acc: 0.125000]\n",
      "1048 [D loss: 0.576356] [D acc: 0.695312] [G loss: 0.837228] [G acc: 0.429688]\n",
      "1049 [D loss: 0.598595] [D acc: 0.679688] [G loss: 1.439985] [G acc: 0.085938]\n",
      "1050 [D loss: 0.542471] [D acc: 0.726562] [G loss: 0.765142] [G acc: 0.500000]\n",
      "1051 [D loss: 0.533902] [D acc: 0.703125] [G loss: 1.505913] [G acc: 0.070312]\n",
      "1052 [D loss: 0.495217] [D acc: 0.757812] [G loss: 1.008755] [G acc: 0.281250]\n",
      "1053 [D loss: 0.514721] [D acc: 0.753906] [G loss: 1.816320] [G acc: 0.046875]\n",
      "1054 [D loss: 0.621080] [D acc: 0.640625] [G loss: 0.709098] [G acc: 0.554688]\n",
      "1055 [D loss: 0.670256] [D acc: 0.644531] [G loss: 1.253159] [G acc: 0.164062]\n",
      "1056 [D loss: 0.529680] [D acc: 0.726562] [G loss: 1.034557] [G acc: 0.265625]\n",
      "1057 [D loss: 0.531051] [D acc: 0.726562] [G loss: 1.160817] [G acc: 0.171875]\n",
      "1058 [D loss: 0.492844] [D acc: 0.789062] [G loss: 1.207499] [G acc: 0.210938]\n",
      "1059 [D loss: 0.471647] [D acc: 0.773438] [G loss: 1.132496] [G acc: 0.289062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060 [D loss: 0.519291] [D acc: 0.718750] [G loss: 1.800378] [G acc: 0.070312]\n",
      "1061 [D loss: 0.510421] [D acc: 0.722656] [G loss: 0.933839] [G acc: 0.382812]\n",
      "1062 [D loss: 0.544036] [D acc: 0.710938] [G loss: 1.612724] [G acc: 0.062500]\n",
      "1063 [D loss: 0.582636] [D acc: 0.691406] [G loss: 0.936687] [G acc: 0.335938]\n",
      "1064 [D loss: 0.538155] [D acc: 0.710938] [G loss: 1.551934] [G acc: 0.054688]\n",
      "1065 [D loss: 0.512861] [D acc: 0.753906] [G loss: 1.009134] [G acc: 0.296875]\n",
      "1066 [D loss: 0.517672] [D acc: 0.714844] [G loss: 1.558798] [G acc: 0.117188]\n",
      "1067 [D loss: 0.576206] [D acc: 0.703125] [G loss: 0.838961] [G acc: 0.437500]\n",
      "1068 [D loss: 0.545354] [D acc: 0.726562] [G loss: 1.272980] [G acc: 0.164062]\n",
      "1069 [D loss: 0.494642] [D acc: 0.761719] [G loss: 1.160807] [G acc: 0.218750]\n",
      "1070 [D loss: 0.475358] [D acc: 0.777344] [G loss: 1.351381] [G acc: 0.171875]\n",
      "1071 [D loss: 0.508562] [D acc: 0.773438] [G loss: 1.119861] [G acc: 0.265625]\n",
      "1072 [D loss: 0.524855] [D acc: 0.734375] [G loss: 1.615337] [G acc: 0.070312]\n",
      "1073 [D loss: 0.508198] [D acc: 0.746094] [G loss: 0.652498] [G acc: 0.593750]\n",
      "1074 [D loss: 0.619509] [D acc: 0.667969] [G loss: 1.925985] [G acc: 0.023438]\n",
      "1075 [D loss: 0.652460] [D acc: 0.648438] [G loss: 0.722764] [G acc: 0.546875]\n",
      "1076 [D loss: 0.632913] [D acc: 0.632812] [G loss: 1.263112] [G acc: 0.093750]\n",
      "1077 [D loss: 0.567528] [D acc: 0.707031] [G loss: 1.025890] [G acc: 0.289062]\n",
      "1078 [D loss: 0.508152] [D acc: 0.742188] [G loss: 1.205376] [G acc: 0.132812]\n",
      "1079 [D loss: 0.499994] [D acc: 0.773438] [G loss: 1.120218] [G acc: 0.281250]\n",
      "1080 [D loss: 0.495021] [D acc: 0.750000] [G loss: 1.408754] [G acc: 0.148438]\n",
      "1081 [D loss: 0.496095] [D acc: 0.777344] [G loss: 1.272499] [G acc: 0.203125]\n",
      "1082 [D loss: 0.483544] [D acc: 0.785156] [G loss: 1.579552] [G acc: 0.085938]\n",
      "1083 [D loss: 0.488638] [D acc: 0.781250] [G loss: 1.139138] [G acc: 0.273438]\n",
      "1084 [D loss: 0.516558] [D acc: 0.734375] [G loss: 1.776304] [G acc: 0.046875]\n",
      "1085 [D loss: 0.504980] [D acc: 0.722656] [G loss: 0.797373] [G acc: 0.539062]\n",
      "1086 [D loss: 0.599012] [D acc: 0.652344] [G loss: 2.010237] [G acc: 0.015625]\n",
      "1087 [D loss: 0.640572] [D acc: 0.667969] [G loss: 0.748203] [G acc: 0.507812]\n",
      "1088 [D loss: 0.610824] [D acc: 0.648438] [G loss: 1.290067] [G acc: 0.062500]\n",
      "1089 [D loss: 0.546426] [D acc: 0.722656] [G loss: 1.137719] [G acc: 0.234375]\n",
      "1090 [D loss: 0.498249] [D acc: 0.785156] [G loss: 1.224938] [G acc: 0.195312]\n",
      "1091 [D loss: 0.510293] [D acc: 0.765625] [G loss: 1.226647] [G acc: 0.226562]\n",
      "1092 [D loss: 0.515517] [D acc: 0.750000] [G loss: 1.372967] [G acc: 0.156250]\n",
      "1093 [D loss: 0.474591] [D acc: 0.781250] [G loss: 1.022194] [G acc: 0.335938]\n",
      "1094 [D loss: 0.498006] [D acc: 0.746094] [G loss: 1.509527] [G acc: 0.109375]\n",
      "1095 [D loss: 0.554199] [D acc: 0.667969] [G loss: 1.054716] [G acc: 0.335938]\n",
      "1096 [D loss: 0.505359] [D acc: 0.757812] [G loss: 1.900570] [G acc: 0.085938]\n",
      "1097 [D loss: 0.536002] [D acc: 0.714844] [G loss: 0.532003] [G acc: 0.726562]\n",
      "1098 [D loss: 0.764565] [D acc: 0.585938] [G loss: 1.609799] [G acc: 0.039062]\n",
      "1099 [D loss: 0.492182] [D acc: 0.734375] [G loss: 1.077851] [G acc: 0.250000]\n",
      "1100 [D loss: 0.547401] [D acc: 0.722656] [G loss: 1.336290] [G acc: 0.117188]\n",
      "1101 [D loss: 0.536742] [D acc: 0.734375] [G loss: 1.187664] [G acc: 0.187500]\n",
      "1102 [D loss: 0.488948] [D acc: 0.785156] [G loss: 1.335044] [G acc: 0.218750]\n",
      "1103 [D loss: 0.448449] [D acc: 0.812500] [G loss: 1.447214] [G acc: 0.179688]\n",
      "1104 [D loss: 0.494099] [D acc: 0.781250] [G loss: 1.218573] [G acc: 0.257812]\n",
      "1105 [D loss: 0.512363] [D acc: 0.742188] [G loss: 1.439632] [G acc: 0.164062]\n",
      "1106 [D loss: 0.495410] [D acc: 0.757812] [G loss: 0.834898] [G acc: 0.453125]\n",
      "1107 [D loss: 0.618064] [D acc: 0.683594] [G loss: 1.767502] [G acc: 0.046875]\n",
      "1108 [D loss: 0.602868] [D acc: 0.683594] [G loss: 0.625483] [G acc: 0.585938]\n",
      "1109 [D loss: 0.656159] [D acc: 0.667969] [G loss: 1.363665] [G acc: 0.093750]\n",
      "1110 [D loss: 0.531391] [D acc: 0.738281] [G loss: 1.079179] [G acc: 0.234375]\n",
      "1111 [D loss: 0.487619] [D acc: 0.765625] [G loss: 1.359898] [G acc: 0.148438]\n",
      "1112 [D loss: 0.537410] [D acc: 0.722656] [G loss: 1.056766] [G acc: 0.289062]\n",
      "1113 [D loss: 0.559196] [D acc: 0.699219] [G loss: 1.309658] [G acc: 0.109375]\n",
      "1114 [D loss: 0.528080] [D acc: 0.742188] [G loss: 1.105897] [G acc: 0.242188]\n",
      "1115 [D loss: 0.478222] [D acc: 0.789062] [G loss: 1.574285] [G acc: 0.132812]\n",
      "1116 [D loss: 0.509913] [D acc: 0.734375] [G loss: 0.836986] [G acc: 0.437500]\n",
      "1117 [D loss: 0.531437] [D acc: 0.710938] [G loss: 2.071789] [G acc: 0.062500]\n",
      "1118 [D loss: 0.641779] [D acc: 0.656250] [G loss: 0.705684] [G acc: 0.546875]\n",
      "1119 [D loss: 0.601142] [D acc: 0.613281] [G loss: 1.279554] [G acc: 0.093750]\n",
      "1120 [D loss: 0.580587] [D acc: 0.675781] [G loss: 1.042442] [G acc: 0.304688]\n",
      "1121 [D loss: 0.535397] [D acc: 0.738281] [G loss: 1.142599] [G acc: 0.164062]\n",
      "1122 [D loss: 0.542580] [D acc: 0.718750] [G loss: 1.123487] [G acc: 0.195312]\n",
      "1123 [D loss: 0.519553] [D acc: 0.714844] [G loss: 1.248302] [G acc: 0.117188]\n",
      "1124 [D loss: 0.490621] [D acc: 0.773438] [G loss: 1.017479] [G acc: 0.296875]\n",
      "1125 [D loss: 0.511358] [D acc: 0.734375] [G loss: 1.425109] [G acc: 0.062500]\n",
      "1126 [D loss: 0.508358] [D acc: 0.753906] [G loss: 0.919743] [G acc: 0.367188]\n",
      "1127 [D loss: 0.546773] [D acc: 0.722656] [G loss: 1.945180] [G acc: 0.046875]\n",
      "1128 [D loss: 0.577784] [D acc: 0.695312] [G loss: 0.696986] [G acc: 0.554688]\n",
      "1129 [D loss: 0.641506] [D acc: 0.671875] [G loss: 1.487896] [G acc: 0.054688]\n",
      "1130 [D loss: 0.491536] [D acc: 0.777344] [G loss: 1.044018] [G acc: 0.250000]\n",
      "1131 [D loss: 0.466642] [D acc: 0.808594] [G loss: 1.225063] [G acc: 0.187500]\n",
      "1132 [D loss: 0.486916] [D acc: 0.769531] [G loss: 1.324667] [G acc: 0.140625]\n",
      "1133 [D loss: 0.479148] [D acc: 0.765625] [G loss: 1.342549] [G acc: 0.187500]\n",
      "1134 [D loss: 0.478998] [D acc: 0.773438] [G loss: 1.445476] [G acc: 0.148438]\n",
      "1135 [D loss: 0.449614] [D acc: 0.789062] [G loss: 1.294472] [G acc: 0.195312]\n",
      "1136 [D loss: 0.487821] [D acc: 0.761719] [G loss: 2.141259] [G acc: 0.031250]\n",
      "1137 [D loss: 0.576535] [D acc: 0.734375] [G loss: 0.565807] [G acc: 0.710938]\n",
      "1138 [D loss: 0.634312] [D acc: 0.628906] [G loss: 1.557217] [G acc: 0.054688]\n",
      "1139 [D loss: 0.517908] [D acc: 0.750000] [G loss: 1.018288] [G acc: 0.320312]\n",
      "1140 [D loss: 0.529226] [D acc: 0.773438] [G loss: 1.374324] [G acc: 0.125000]\n",
      "1141 [D loss: 0.539553] [D acc: 0.726562] [G loss: 1.035673] [G acc: 0.312500]\n",
      "1142 [D loss: 0.536182] [D acc: 0.730469] [G loss: 1.494718] [G acc: 0.140625]\n",
      "1143 [D loss: 0.488114] [D acc: 0.742188] [G loss: 0.799821] [G acc: 0.437500]\n",
      "1144 [D loss: 0.583068] [D acc: 0.707031] [G loss: 1.779954] [G acc: 0.015625]\n",
      "1145 [D loss: 0.603279] [D acc: 0.699219] [G loss: 0.792662] [G acc: 0.429688]\n",
      "1146 [D loss: 0.543098] [D acc: 0.710938] [G loss: 1.272253] [G acc: 0.156250]\n",
      "1147 [D loss: 0.513210] [D acc: 0.718750] [G loss: 1.305054] [G acc: 0.164062]\n",
      "1148 [D loss: 0.496652] [D acc: 0.777344] [G loss: 1.240627] [G acc: 0.210938]\n",
      "1149 [D loss: 0.488786] [D acc: 0.738281] [G loss: 1.268506] [G acc: 0.210938]\n",
      "1150 [D loss: 0.479281] [D acc: 0.785156] [G loss: 1.116802] [G acc: 0.296875]\n",
      "1151 [D loss: 0.509762] [D acc: 0.714844] [G loss: 1.928018] [G acc: 0.023438]\n",
      "1152 [D loss: 0.562611] [D acc: 0.699219] [G loss: 0.745511] [G acc: 0.515625]\n",
      "1153 [D loss: 0.639971] [D acc: 0.660156] [G loss: 1.552887] [G acc: 0.062500]\n",
      "1154 [D loss: 0.547433] [D acc: 0.718750] [G loss: 1.010339] [G acc: 0.328125]\n",
      "1155 [D loss: 0.539116] [D acc: 0.730469] [G loss: 1.316800] [G acc: 0.140625]\n",
      "1156 [D loss: 0.508631] [D acc: 0.746094] [G loss: 1.002272] [G acc: 0.312500]\n",
      "1157 [D loss: 0.558387] [D acc: 0.730469] [G loss: 1.734548] [G acc: 0.062500]\n",
      "1158 [D loss: 0.559791] [D acc: 0.683594] [G loss: 0.684000] [G acc: 0.585938]\n",
      "1159 [D loss: 0.601336] [D acc: 0.640625] [G loss: 1.331159] [G acc: 0.093750]\n",
      "1160 [D loss: 0.509377] [D acc: 0.765625] [G loss: 0.997423] [G acc: 0.343750]\n",
      "1161 [D loss: 0.480271] [D acc: 0.750000] [G loss: 1.243062] [G acc: 0.187500]\n",
      "1162 [D loss: 0.497257] [D acc: 0.742188] [G loss: 1.264290] [G acc: 0.218750]\n",
      "1163 [D loss: 0.501460] [D acc: 0.734375] [G loss: 1.275571] [G acc: 0.140625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164 [D loss: 0.493353] [D acc: 0.777344] [G loss: 1.185856] [G acc: 0.265625]\n",
      "1165 [D loss: 0.522731] [D acc: 0.718750] [G loss: 1.432462] [G acc: 0.109375]\n",
      "1166 [D loss: 0.473488] [D acc: 0.781250] [G loss: 1.132032] [G acc: 0.304688]\n",
      "1167 [D loss: 0.449004] [D acc: 0.812500] [G loss: 1.824825] [G acc: 0.054688]\n",
      "1168 [D loss: 0.486811] [D acc: 0.781250] [G loss: 0.992885] [G acc: 0.343750]\n",
      "1169 [D loss: 0.635346] [D acc: 0.695312] [G loss: 2.232758] [G acc: 0.031250]\n",
      "1170 [D loss: 0.745989] [D acc: 0.601562] [G loss: 0.611760] [G acc: 0.656250]\n",
      "1171 [D loss: 0.618592] [D acc: 0.648438] [G loss: 1.098907] [G acc: 0.187500]\n",
      "1172 [D loss: 0.503422] [D acc: 0.765625] [G loss: 1.131317] [G acc: 0.156250]\n",
      "1173 [D loss: 0.563472] [D acc: 0.699219] [G loss: 1.154264] [G acc: 0.171875]\n",
      "1174 [D loss: 0.532002] [D acc: 0.714844] [G loss: 1.191335] [G acc: 0.234375]\n",
      "1175 [D loss: 0.480589] [D acc: 0.769531] [G loss: 1.428706] [G acc: 0.140625]\n",
      "1176 [D loss: 0.512912] [D acc: 0.765625] [G loss: 0.797330] [G acc: 0.484375]\n",
      "1177 [D loss: 0.630930] [D acc: 0.679688] [G loss: 1.711826] [G acc: 0.046875]\n",
      "1178 [D loss: 0.655383] [D acc: 0.675781] [G loss: 0.806525] [G acc: 0.460938]\n",
      "1179 [D loss: 0.571408] [D acc: 0.679688] [G loss: 1.267484] [G acc: 0.101562]\n",
      "1180 [D loss: 0.515944] [D acc: 0.738281] [G loss: 1.077259] [G acc: 0.218750]\n",
      "1181 [D loss: 0.543466] [D acc: 0.746094] [G loss: 1.181644] [G acc: 0.218750]\n",
      "1182 [D loss: 0.517329] [D acc: 0.730469] [G loss: 1.139633] [G acc: 0.234375]\n",
      "1183 [D loss: 0.504945] [D acc: 0.757812] [G loss: 1.250590] [G acc: 0.242188]\n",
      "1184 [D loss: 0.475003] [D acc: 0.777344] [G loss: 1.370174] [G acc: 0.164062]\n",
      "1185 [D loss: 0.598325] [D acc: 0.718750] [G loss: 1.193128] [G acc: 0.234375]\n",
      "1186 [D loss: 0.484207] [D acc: 0.761719] [G loss: 1.180888] [G acc: 0.234375]\n",
      "1187 [D loss: 0.533242] [D acc: 0.726562] [G loss: 1.566094] [G acc: 0.132812]\n",
      "1188 [D loss: 0.593555] [D acc: 0.683594] [G loss: 0.706380] [G acc: 0.554688]\n",
      "1189 [D loss: 0.638028] [D acc: 0.667969] [G loss: 1.703116] [G acc: 0.046875]\n",
      "1190 [D loss: 0.608382] [D acc: 0.683594] [G loss: 0.784267] [G acc: 0.492188]\n",
      "1191 [D loss: 0.577044] [D acc: 0.687500] [G loss: 1.427124] [G acc: 0.101562]\n",
      "1192 [D loss: 0.567612] [D acc: 0.691406] [G loss: 0.845813] [G acc: 0.429688]\n",
      "1193 [D loss: 0.505480] [D acc: 0.750000] [G loss: 1.203007] [G acc: 0.203125]\n",
      "1194 [D loss: 0.540404] [D acc: 0.722656] [G loss: 1.138928] [G acc: 0.226562]\n",
      "1195 [D loss: 0.516098] [D acc: 0.769531] [G loss: 1.352366] [G acc: 0.093750]\n",
      "1196 [D loss: 0.463181] [D acc: 0.781250] [G loss: 1.144123] [G acc: 0.242188]\n",
      "1197 [D loss: 0.500280] [D acc: 0.742188] [G loss: 1.634513] [G acc: 0.085938]\n",
      "1198 [D loss: 0.532451] [D acc: 0.742188] [G loss: 0.847818] [G acc: 0.445312]\n",
      "1199 [D loss: 0.538436] [D acc: 0.710938] [G loss: 1.723822] [G acc: 0.062500]\n",
      "1200 [D loss: 0.552522] [D acc: 0.746094] [G loss: 0.731673] [G acc: 0.507812]\n",
      "1201 [D loss: 0.685074] [D acc: 0.632812] [G loss: 1.646263] [G acc: 0.031250]\n",
      "1202 [D loss: 0.544762] [D acc: 0.734375] [G loss: 0.943092] [G acc: 0.312500]\n",
      "1203 [D loss: 0.532744] [D acc: 0.730469] [G loss: 1.219565] [G acc: 0.140625]\n",
      "1204 [D loss: 0.524831] [D acc: 0.753906] [G loss: 1.165051] [G acc: 0.203125]\n",
      "1205 [D loss: 0.482661] [D acc: 0.785156] [G loss: 1.231072] [G acc: 0.257812]\n",
      "1206 [D loss: 0.488509] [D acc: 0.761719] [G loss: 1.433854] [G acc: 0.140625]\n",
      "1207 [D loss: 0.481003] [D acc: 0.765625] [G loss: 1.423800] [G acc: 0.140625]\n",
      "1208 [D loss: 0.526749] [D acc: 0.730469] [G loss: 1.683276] [G acc: 0.093750]\n",
      "1209 [D loss: 0.516774] [D acc: 0.730469] [G loss: 0.781688] [G acc: 0.445312]\n",
      "1210 [D loss: 0.647637] [D acc: 0.605469] [G loss: 2.077635] [G acc: 0.015625]\n",
      "1211 [D loss: 0.621529] [D acc: 0.664062] [G loss: 0.842726] [G acc: 0.398438]\n",
      "1212 [D loss: 0.614329] [D acc: 0.652344] [G loss: 1.224332] [G acc: 0.171875]\n",
      "1213 [D loss: 0.538525] [D acc: 0.714844] [G loss: 1.117204] [G acc: 0.171875]\n",
      "1214 [D loss: 0.530874] [D acc: 0.707031] [G loss: 1.264669] [G acc: 0.148438]\n",
      "1215 [D loss: 0.484983] [D acc: 0.769531] [G loss: 1.148971] [G acc: 0.210938]\n",
      "1216 [D loss: 0.486305] [D acc: 0.761719] [G loss: 1.202456] [G acc: 0.226562]\n",
      "1217 [D loss: 0.559560] [D acc: 0.703125] [G loss: 1.558663] [G acc: 0.093750]\n",
      "1218 [D loss: 0.492337] [D acc: 0.765625] [G loss: 0.678048] [G acc: 0.554688]\n",
      "1219 [D loss: 0.597519] [D acc: 0.695312] [G loss: 1.869497] [G acc: 0.023438]\n",
      "1220 [D loss: 0.643962] [D acc: 0.652344] [G loss: 0.702563] [G acc: 0.539062]\n",
      "1221 [D loss: 0.613732] [D acc: 0.613281] [G loss: 1.069348] [G acc: 0.250000]\n",
      "1222 [D loss: 0.541685] [D acc: 0.722656] [G loss: 0.959523] [G acc: 0.328125]\n",
      "1223 [D loss: 0.522392] [D acc: 0.738281] [G loss: 1.176580] [G acc: 0.210938]\n",
      "1224 [D loss: 0.527872] [D acc: 0.722656] [G loss: 1.176401] [G acc: 0.164062]\n",
      "1225 [D loss: 0.501896] [D acc: 0.773438] [G loss: 1.250894] [G acc: 0.171875]\n",
      "1226 [D loss: 0.544134] [D acc: 0.714844] [G loss: 1.361856] [G acc: 0.109375]\n",
      "1227 [D loss: 0.564323] [D acc: 0.718750] [G loss: 1.122991] [G acc: 0.179688]\n",
      "1228 [D loss: 0.521695] [D acc: 0.742188] [G loss: 1.708416] [G acc: 0.085938]\n",
      "1229 [D loss: 0.610940] [D acc: 0.703125] [G loss: 0.768775] [G acc: 0.429688]\n",
      "1230 [D loss: 0.620719] [D acc: 0.664062] [G loss: 1.328379] [G acc: 0.085938]\n",
      "1231 [D loss: 0.527935] [D acc: 0.730469] [G loss: 0.940703] [G acc: 0.234375]\n",
      "1232 [D loss: 0.508349] [D acc: 0.757812] [G loss: 1.155153] [G acc: 0.226562]\n",
      "1233 [D loss: 0.490353] [D acc: 0.792969] [G loss: 1.382862] [G acc: 0.101562]\n",
      "1234 [D loss: 0.536004] [D acc: 0.710938] [G loss: 1.197100] [G acc: 0.203125]\n",
      "1235 [D loss: 0.479787] [D acc: 0.773438] [G loss: 1.678249] [G acc: 0.070312]\n",
      "1236 [D loss: 0.512844] [D acc: 0.753906] [G loss: 0.763119] [G acc: 0.492188]\n",
      "1237 [D loss: 0.631160] [D acc: 0.660156] [G loss: 1.746981] [G acc: 0.039062]\n",
      "1238 [D loss: 0.647241] [D acc: 0.628906] [G loss: 0.859667] [G acc: 0.375000]\n",
      "1239 [D loss: 0.519621] [D acc: 0.714844] [G loss: 1.169929] [G acc: 0.148438]\n",
      "1240 [D loss: 0.539649] [D acc: 0.707031] [G loss: 1.154063] [G acc: 0.210938]\n",
      "1241 [D loss: 0.535151] [D acc: 0.714844] [G loss: 1.537994] [G acc: 0.070312]\n",
      "1242 [D loss: 0.500947] [D acc: 0.742188] [G loss: 0.685759] [G acc: 0.570312]\n",
      "1243 [D loss: 0.556614] [D acc: 0.679688] [G loss: 1.541261] [G acc: 0.109375]\n",
      "1244 [D loss: 0.532422] [D acc: 0.730469] [G loss: 0.794296] [G acc: 0.429688]\n",
      "1245 [D loss: 0.564866] [D acc: 0.699219] [G loss: 1.590776] [G acc: 0.101562]\n",
      "1246 [D loss: 0.552680] [D acc: 0.742188] [G loss: 0.867907] [G acc: 0.375000]\n",
      "1247 [D loss: 0.550047] [D acc: 0.699219] [G loss: 1.319075] [G acc: 0.156250]\n",
      "1248 [D loss: 0.503905] [D acc: 0.757812] [G loss: 1.059183] [G acc: 0.265625]\n",
      "1249 [D loss: 0.497831] [D acc: 0.746094] [G loss: 1.481472] [G acc: 0.101562]\n",
      "1250 [D loss: 0.479095] [D acc: 0.777344] [G loss: 1.188411] [G acc: 0.210938]\n",
      "1251 [D loss: 0.527457] [D acc: 0.710938] [G loss: 1.668455] [G acc: 0.085938]\n",
      "1252 [D loss: 0.541778] [D acc: 0.726562] [G loss: 0.834094] [G acc: 0.468750]\n",
      "1253 [D loss: 0.579042] [D acc: 0.656250] [G loss: 1.707471] [G acc: 0.039062]\n",
      "1254 [D loss: 0.601989] [D acc: 0.675781] [G loss: 0.883183] [G acc: 0.382812]\n",
      "1255 [D loss: 0.530005] [D acc: 0.761719] [G loss: 1.499154] [G acc: 0.101562]\n",
      "1256 [D loss: 0.578976] [D acc: 0.648438] [G loss: 0.980102] [G acc: 0.289062]\n",
      "1257 [D loss: 0.484577] [D acc: 0.750000] [G loss: 1.376050] [G acc: 0.132812]\n",
      "1258 [D loss: 0.484668] [D acc: 0.777344] [G loss: 1.212397] [G acc: 0.210938]\n",
      "1259 [D loss: 0.484455] [D acc: 0.773438] [G loss: 1.363477] [G acc: 0.195312]\n",
      "1260 [D loss: 0.451098] [D acc: 0.773438] [G loss: 1.178035] [G acc: 0.296875]\n",
      "1261 [D loss: 0.547330] [D acc: 0.691406] [G loss: 2.618089] [G acc: 0.000000]\n",
      "1262 [D loss: 0.717717] [D acc: 0.613281] [G loss: 0.596116] [G acc: 0.617188]\n",
      "1263 [D loss: 0.760367] [D acc: 0.601562] [G loss: 1.292644] [G acc: 0.140625]\n",
      "1264 [D loss: 0.567191] [D acc: 0.679688] [G loss: 1.034416] [G acc: 0.242188]\n",
      "1265 [D loss: 0.556449] [D acc: 0.722656] [G loss: 1.171003] [G acc: 0.140625]\n",
      "1266 [D loss: 0.555003] [D acc: 0.707031] [G loss: 0.984916] [G acc: 0.242188]\n",
      "1267 [D loss: 0.551311] [D acc: 0.703125] [G loss: 1.206100] [G acc: 0.148438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1268 [D loss: 0.524478] [D acc: 0.734375] [G loss: 1.047341] [G acc: 0.257812]\n",
      "1269 [D loss: 0.461549] [D acc: 0.777344] [G loss: 1.455620] [G acc: 0.085938]\n",
      "1270 [D loss: 0.491290] [D acc: 0.765625] [G loss: 1.414897] [G acc: 0.164062]\n",
      "1271 [D loss: 0.495865] [D acc: 0.730469] [G loss: 1.226028] [G acc: 0.218750]\n",
      "1272 [D loss: 0.544672] [D acc: 0.703125] [G loss: 1.474642] [G acc: 0.132812]\n",
      "1273 [D loss: 0.509345] [D acc: 0.734375] [G loss: 0.835049] [G acc: 0.515625]\n",
      "1274 [D loss: 0.548527] [D acc: 0.710938] [G loss: 1.805041] [G acc: 0.093750]\n",
      "1275 [D loss: 0.539251] [D acc: 0.726562] [G loss: 0.672027] [G acc: 0.539062]\n",
      "1276 [D loss: 0.632314] [D acc: 0.617188] [G loss: 1.440877] [G acc: 0.078125]\n",
      "1277 [D loss: 0.532865] [D acc: 0.714844] [G loss: 1.028270] [G acc: 0.273438]\n",
      "1278 [D loss: 0.519274] [D acc: 0.789062] [G loss: 1.193068] [G acc: 0.218750]\n",
      "1279 [D loss: 0.511146] [D acc: 0.738281] [G loss: 1.200401] [G acc: 0.203125]\n",
      "1280 [D loss: 0.492879] [D acc: 0.750000] [G loss: 1.238472] [G acc: 0.210938]\n",
      "1281 [D loss: 0.497705] [D acc: 0.753906] [G loss: 1.647652] [G acc: 0.039062]\n",
      "1282 [D loss: 0.516037] [D acc: 0.738281] [G loss: 0.984342] [G acc: 0.328125]\n",
      "1283 [D loss: 0.584529] [D acc: 0.675781] [G loss: 1.950236] [G acc: 0.070312]\n",
      "1284 [D loss: 0.599750] [D acc: 0.699219] [G loss: 0.577978] [G acc: 0.695312]\n",
      "1285 [D loss: 0.680098] [D acc: 0.636719] [G loss: 1.349607] [G acc: 0.070312]\n",
      "1286 [D loss: 0.573300] [D acc: 0.707031] [G loss: 0.984773] [G acc: 0.343750]\n",
      "1287 [D loss: 0.534969] [D acc: 0.718750] [G loss: 1.147046] [G acc: 0.171875]\n",
      "1288 [D loss: 0.505804] [D acc: 0.734375] [G loss: 1.199618] [G acc: 0.156250]\n",
      "1289 [D loss: 0.478388] [D acc: 0.816406] [G loss: 1.245510] [G acc: 0.156250]\n",
      "1290 [D loss: 0.451193] [D acc: 0.796875] [G loss: 1.427935] [G acc: 0.117188]\n",
      "1291 [D loss: 0.494794] [D acc: 0.734375] [G loss: 1.372553] [G acc: 0.140625]\n",
      "1292 [D loss: 0.514562] [D acc: 0.750000] [G loss: 1.425694] [G acc: 0.148438]\n",
      "1293 [D loss: 0.485173] [D acc: 0.773438] [G loss: 0.970066] [G acc: 0.367188]\n",
      "1294 [D loss: 0.585802] [D acc: 0.683594] [G loss: 1.785865] [G acc: 0.062500]\n",
      "1295 [D loss: 0.538668] [D acc: 0.722656] [G loss: 0.881348] [G acc: 0.421875]\n",
      "1296 [D loss: 0.525317] [D acc: 0.714844] [G loss: 1.827673] [G acc: 0.039062]\n",
      "1297 [D loss: 0.574443] [D acc: 0.714844] [G loss: 0.707647] [G acc: 0.531250]\n",
      "1298 [D loss: 0.614733] [D acc: 0.671875] [G loss: 1.556617] [G acc: 0.054688]\n",
      "1299 [D loss: 0.593681] [D acc: 0.675781] [G loss: 0.816051] [G acc: 0.390625]\n",
      "1300 [D loss: 0.514874] [D acc: 0.761719] [G loss: 1.238989] [G acc: 0.164062]\n",
      "1301 [D loss: 0.528713] [D acc: 0.714844] [G loss: 1.300917] [G acc: 0.117188]\n",
      "1302 [D loss: 0.555434] [D acc: 0.695312] [G loss: 1.152343] [G acc: 0.210938]\n",
      "1303 [D loss: 0.518403] [D acc: 0.746094] [G loss: 1.131795] [G acc: 0.195312]\n",
      "1304 [D loss: 0.493559] [D acc: 0.769531] [G loss: 1.489334] [G acc: 0.109375]\n",
      "1305 [D loss: 0.560121] [D acc: 0.710938] [G loss: 1.149485] [G acc: 0.242188]\n",
      "1306 [D loss: 0.504453] [D acc: 0.738281] [G loss: 1.572780] [G acc: 0.070312]\n",
      "1307 [D loss: 0.468826] [D acc: 0.765625] [G loss: 0.647859] [G acc: 0.617188]\n",
      "1308 [D loss: 0.684134] [D acc: 0.636719] [G loss: 2.014828] [G acc: 0.015625]\n",
      "1309 [D loss: 0.712434] [D acc: 0.621094] [G loss: 0.801335] [G acc: 0.406250]\n",
      "1310 [D loss: 0.526325] [D acc: 0.707031] [G loss: 1.005617] [G acc: 0.257812]\n",
      "1311 [D loss: 0.503698] [D acc: 0.761719] [G loss: 1.080410] [G acc: 0.281250]\n",
      "1312 [D loss: 0.486505] [D acc: 0.769531] [G loss: 1.209212] [G acc: 0.203125]\n",
      "1313 [D loss: 0.489947] [D acc: 0.781250] [G loss: 1.111011] [G acc: 0.242188]\n",
      "1314 [D loss: 0.457547] [D acc: 0.785156] [G loss: 1.378784] [G acc: 0.156250]\n",
      "1315 [D loss: 0.508961] [D acc: 0.746094] [G loss: 1.407879] [G acc: 0.187500]\n",
      "1316 [D loss: 0.456763] [D acc: 0.757812] [G loss: 1.274179] [G acc: 0.171875]\n",
      "1317 [D loss: 0.498922] [D acc: 0.769531] [G loss: 1.491569] [G acc: 0.148438]\n",
      "1318 [D loss: 0.482485] [D acc: 0.773438] [G loss: 1.097992] [G acc: 0.296875]\n",
      "1319 [D loss: 0.507412] [D acc: 0.726562] [G loss: 2.020094] [G acc: 0.023438]\n",
      "1320 [D loss: 0.597301] [D acc: 0.683594] [G loss: 0.627891] [G acc: 0.609375]\n",
      "1321 [D loss: 0.631137] [D acc: 0.625000] [G loss: 1.301196] [G acc: 0.109375]\n",
      "1322 [D loss: 0.531596] [D acc: 0.738281] [G loss: 1.155438] [G acc: 0.218750]\n",
      "1323 [D loss: 0.455407] [D acc: 0.777344] [G loss: 1.153103] [G acc: 0.203125]\n",
      "1324 [D loss: 0.471732] [D acc: 0.792969] [G loss: 1.342425] [G acc: 0.210938]\n",
      "1325 [D loss: 0.520773] [D acc: 0.753906] [G loss: 1.258301] [G acc: 0.210938]\n",
      "1326 [D loss: 0.494169] [D acc: 0.773438] [G loss: 1.367410] [G acc: 0.195312]\n",
      "1327 [D loss: 0.461822] [D acc: 0.765625] [G loss: 1.187228] [G acc: 0.234375]\n",
      "1328 [D loss: 0.478728] [D acc: 0.757812] [G loss: 2.145670] [G acc: 0.023438]\n",
      "1329 [D loss: 0.613373] [D acc: 0.707031] [G loss: 0.526816] [G acc: 0.718750]\n",
      "1330 [D loss: 0.782278] [D acc: 0.601562] [G loss: 1.620686] [G acc: 0.039062]\n",
      "1331 [D loss: 0.565922] [D acc: 0.683594] [G loss: 0.964043] [G acc: 0.296875]\n",
      "1332 [D loss: 0.537374] [D acc: 0.726562] [G loss: 1.126698] [G acc: 0.179688]\n",
      "1333 [D loss: 0.468499] [D acc: 0.785156] [G loss: 1.213291] [G acc: 0.218750]\n",
      "1334 [D loss: 0.479164] [D acc: 0.761719] [G loss: 1.104227] [G acc: 0.242188]\n",
      "1335 [D loss: 0.492003] [D acc: 0.753906] [G loss: 1.303016] [G acc: 0.171875]\n",
      "1336 [D loss: 0.458129] [D acc: 0.785156] [G loss: 1.198393] [G acc: 0.187500]\n",
      "1337 [D loss: 0.539957] [D acc: 0.746094] [G loss: 1.744744] [G acc: 0.078125]\n",
      "1338 [D loss: 0.517194] [D acc: 0.750000] [G loss: 0.783239] [G acc: 0.523438]\n",
      "1339 [D loss: 0.571285] [D acc: 0.652344] [G loss: 1.740272] [G acc: 0.046875]\n",
      "1340 [D loss: 0.529875] [D acc: 0.738281] [G loss: 0.948775] [G acc: 0.359375]\n",
      "1341 [D loss: 0.476935] [D acc: 0.808594] [G loss: 1.531548] [G acc: 0.117188]\n",
      "1342 [D loss: 0.502538] [D acc: 0.769531] [G loss: 1.037209] [G acc: 0.304688]\n",
      "1343 [D loss: 0.550020] [D acc: 0.710938] [G loss: 1.812671] [G acc: 0.046875]\n",
      "1344 [D loss: 0.509267] [D acc: 0.742188] [G loss: 0.956607] [G acc: 0.359375]\n",
      "1345 [D loss: 0.553426] [D acc: 0.687500] [G loss: 1.412537] [G acc: 0.085938]\n",
      "1346 [D loss: 0.443054] [D acc: 0.808594] [G loss: 1.215389] [G acc: 0.218750]\n",
      "1347 [D loss: 0.508797] [D acc: 0.734375] [G loss: 1.484584] [G acc: 0.140625]\n",
      "1348 [D loss: 0.496219] [D acc: 0.753906] [G loss: 1.270380] [G acc: 0.195312]\n",
      "1349 [D loss: 0.486144] [D acc: 0.773438] [G loss: 1.435011] [G acc: 0.140625]\n",
      "1350 [D loss: 0.442773] [D acc: 0.835938] [G loss: 1.298494] [G acc: 0.250000]\n",
      "1351 [D loss: 0.515804] [D acc: 0.738281] [G loss: 2.765363] [G acc: 0.000000]\n",
      "1352 [D loss: 0.715049] [D acc: 0.628906] [G loss: 0.551091] [G acc: 0.679688]\n",
      "1353 [D loss: 0.825632] [D acc: 0.570312] [G loss: 1.491854] [G acc: 0.023438]\n",
      "1354 [D loss: 0.582597] [D acc: 0.671875] [G loss: 1.054922] [G acc: 0.164062]\n",
      "1355 [D loss: 0.496005] [D acc: 0.757812] [G loss: 1.188595] [G acc: 0.156250]\n",
      "1356 [D loss: 0.534265] [D acc: 0.714844] [G loss: 1.297969] [G acc: 0.140625]\n",
      "1357 [D loss: 0.477463] [D acc: 0.769531] [G loss: 1.143598] [G acc: 0.218750]\n",
      "1358 [D loss: 0.507958] [D acc: 0.765625] [G loss: 1.078730] [G acc: 0.328125]\n",
      "1359 [D loss: 0.506046] [D acc: 0.722656] [G loss: 1.185442] [G acc: 0.195312]\n",
      "1360 [D loss: 0.467110] [D acc: 0.792969] [G loss: 1.378295] [G acc: 0.156250]\n",
      "1361 [D loss: 0.452652] [D acc: 0.789062] [G loss: 1.530776] [G acc: 0.093750]\n",
      "1362 [D loss: 0.470306] [D acc: 0.781250] [G loss: 1.172240] [G acc: 0.304688]\n",
      "1363 [D loss: 0.449916] [D acc: 0.800781] [G loss: 2.214050] [G acc: 0.039062]\n",
      "1364 [D loss: 0.583568] [D acc: 0.726562] [G loss: 0.662639] [G acc: 0.617188]\n",
      "1365 [D loss: 0.703124] [D acc: 0.625000] [G loss: 1.769433] [G acc: 0.023438]\n",
      "1366 [D loss: 0.557678] [D acc: 0.699219] [G loss: 0.900926] [G acc: 0.367188]\n",
      "1367 [D loss: 0.490287] [D acc: 0.757812] [G loss: 1.364697] [G acc: 0.132812]\n",
      "1368 [D loss: 0.483629] [D acc: 0.750000] [G loss: 1.203123] [G acc: 0.179688]\n",
      "1369 [D loss: 0.542083] [D acc: 0.695312] [G loss: 1.387376] [G acc: 0.148438]\n",
      "1370 [D loss: 0.482823] [D acc: 0.757812] [G loss: 1.431746] [G acc: 0.226562]\n",
      "1371 [D loss: 0.523311] [D acc: 0.742188] [G loss: 1.377747] [G acc: 0.164062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372 [D loss: 0.504624] [D acc: 0.746094] [G loss: 1.546036] [G acc: 0.085938]\n",
      "1373 [D loss: 0.544017] [D acc: 0.718750] [G loss: 0.978857] [G acc: 0.351562]\n",
      "1374 [D loss: 0.513242] [D acc: 0.757812] [G loss: 1.549143] [G acc: 0.125000]\n",
      "1375 [D loss: 0.454053] [D acc: 0.769531] [G loss: 0.944073] [G acc: 0.335938]\n",
      "1376 [D loss: 0.528510] [D acc: 0.722656] [G loss: 2.035305] [G acc: 0.039062]\n",
      "1377 [D loss: 0.562762] [D acc: 0.675781] [G loss: 0.694201] [G acc: 0.539062]\n",
      "1378 [D loss: 0.610316] [D acc: 0.675781] [G loss: 1.366823] [G acc: 0.117188]\n",
      "1379 [D loss: 0.519675] [D acc: 0.742188] [G loss: 1.124276] [G acc: 0.226562]\n",
      "1380 [D loss: 0.492359] [D acc: 0.750000] [G loss: 1.258793] [G acc: 0.187500]\n",
      "1381 [D loss: 0.527141] [D acc: 0.730469] [G loss: 1.278991] [G acc: 0.179688]\n",
      "1382 [D loss: 0.464493] [D acc: 0.757812] [G loss: 1.341078] [G acc: 0.195312]\n",
      "1383 [D loss: 0.487166] [D acc: 0.769531] [G loss: 1.708089] [G acc: 0.101562]\n",
      "1384 [D loss: 0.512617] [D acc: 0.738281] [G loss: 0.935642] [G acc: 0.343750]\n",
      "1385 [D loss: 0.536824] [D acc: 0.730469] [G loss: 1.905173] [G acc: 0.039062]\n",
      "1386 [D loss: 0.538269] [D acc: 0.699219] [G loss: 0.749437] [G acc: 0.476562]\n",
      "1387 [D loss: 0.563623] [D acc: 0.722656] [G loss: 1.714930] [G acc: 0.062500]\n",
      "1388 [D loss: 0.533819] [D acc: 0.730469] [G loss: 0.925543] [G acc: 0.359375]\n",
      "1389 [D loss: 0.554196] [D acc: 0.734375] [G loss: 1.551209] [G acc: 0.109375]\n",
      "1390 [D loss: 0.488487] [D acc: 0.753906] [G loss: 1.074993] [G acc: 0.296875]\n",
      "1391 [D loss: 0.553648] [D acc: 0.687500] [G loss: 1.470180] [G acc: 0.109375]\n",
      "1392 [D loss: 0.493147] [D acc: 0.753906] [G loss: 1.185628] [G acc: 0.210938]\n",
      "1393 [D loss: 0.500646] [D acc: 0.765625] [G loss: 1.661816] [G acc: 0.070312]\n",
      "1394 [D loss: 0.490341] [D acc: 0.765625] [G loss: 1.193378] [G acc: 0.187500]\n",
      "1395 [D loss: 0.509871] [D acc: 0.726562] [G loss: 1.935376] [G acc: 0.031250]\n",
      "1396 [D loss: 0.568211] [D acc: 0.707031] [G loss: 0.721755] [G acc: 0.484375]\n",
      "1397 [D loss: 0.692657] [D acc: 0.613281] [G loss: 1.635696] [G acc: 0.070312]\n",
      "1398 [D loss: 0.609637] [D acc: 0.671875] [G loss: 1.002962] [G acc: 0.289062]\n",
      "1399 [D loss: 0.518705] [D acc: 0.710938] [G loss: 1.185326] [G acc: 0.187500]\n",
      "1400 [D loss: 0.523850] [D acc: 0.703125] [G loss: 1.269597] [G acc: 0.187500]\n",
      "1401 [D loss: 0.467527] [D acc: 0.789062] [G loss: 1.266604] [G acc: 0.210938]\n",
      "1402 [D loss: 0.541602] [D acc: 0.746094] [G loss: 1.408428] [G acc: 0.148438]\n",
      "1403 [D loss: 0.461247] [D acc: 0.777344] [G loss: 1.043582] [G acc: 0.335938]\n",
      "1404 [D loss: 0.569829] [D acc: 0.679688] [G loss: 1.749490] [G acc: 0.093750]\n",
      "1405 [D loss: 0.516826] [D acc: 0.703125] [G loss: 0.606189] [G acc: 0.664062]\n",
      "1406 [D loss: 0.621765] [D acc: 0.664062] [G loss: 1.809914] [G acc: 0.031250]\n",
      "1407 [D loss: 0.619106] [D acc: 0.652344] [G loss: 0.800833] [G acc: 0.421875]\n",
      "1408 [D loss: 0.563326] [D acc: 0.691406] [G loss: 1.161402] [G acc: 0.210938]\n",
      "1409 [D loss: 0.505182] [D acc: 0.796875] [G loss: 1.174983] [G acc: 0.179688]\n",
      "1410 [D loss: 0.498153] [D acc: 0.765625] [G loss: 1.211718] [G acc: 0.187500]\n",
      "1411 [D loss: 0.451107] [D acc: 0.812500] [G loss: 1.253601] [G acc: 0.226562]\n",
      "1412 [D loss: 0.506323] [D acc: 0.746094] [G loss: 1.609978] [G acc: 0.101562]\n",
      "1413 [D loss: 0.524023] [D acc: 0.742188] [G loss: 0.990355] [G acc: 0.343750]\n",
      "1414 [D loss: 0.581356] [D acc: 0.675781] [G loss: 1.568512] [G acc: 0.039062]\n",
      "1415 [D loss: 0.508542] [D acc: 0.730469] [G loss: 0.826098] [G acc: 0.421875]\n",
      "1416 [D loss: 0.515242] [D acc: 0.769531] [G loss: 1.496819] [G acc: 0.140625]\n",
      "1417 [D loss: 0.535617] [D acc: 0.714844] [G loss: 0.941923] [G acc: 0.382812]\n",
      "1418 [D loss: 0.494606] [D acc: 0.789062] [G loss: 1.617144] [G acc: 0.109375]\n",
      "1419 [D loss: 0.538515] [D acc: 0.738281] [G loss: 0.863003] [G acc: 0.453125]\n",
      "1420 [D loss: 0.600361] [D acc: 0.707031] [G loss: 1.775312] [G acc: 0.046875]\n",
      "1421 [D loss: 0.569458] [D acc: 0.707031] [G loss: 0.818195] [G acc: 0.421875]\n",
      "1422 [D loss: 0.549596] [D acc: 0.722656] [G loss: 1.368830] [G acc: 0.132812]\n",
      "1423 [D loss: 0.496680] [D acc: 0.750000] [G loss: 1.269512] [G acc: 0.203125]\n",
      "1424 [D loss: 0.471694] [D acc: 0.757812] [G loss: 1.176191] [G acc: 0.273438]\n",
      "1425 [D loss: 0.400546] [D acc: 0.839844] [G loss: 1.271154] [G acc: 0.257812]\n",
      "1426 [D loss: 0.491451] [D acc: 0.742188] [G loss: 1.472401] [G acc: 0.179688]\n",
      "1427 [D loss: 0.453487] [D acc: 0.769531] [G loss: 1.272000] [G acc: 0.250000]\n",
      "1428 [D loss: 0.481396] [D acc: 0.734375] [G loss: 2.170669] [G acc: 0.039062]\n",
      "1429 [D loss: 0.579079] [D acc: 0.691406] [G loss: 0.546354] [G acc: 0.718750]\n",
      "1430 [D loss: 0.854156] [D acc: 0.562500] [G loss: 1.533217] [G acc: 0.085938]\n",
      "1431 [D loss: 0.558392] [D acc: 0.691406] [G loss: 1.086305] [G acc: 0.210938]\n",
      "1432 [D loss: 0.536667] [D acc: 0.699219] [G loss: 1.128102] [G acc: 0.203125]\n",
      "1433 [D loss: 0.474049] [D acc: 0.777344] [G loss: 1.339483] [G acc: 0.179688]\n",
      "1434 [D loss: 0.481934] [D acc: 0.757812] [G loss: 1.250827] [G acc: 0.218750]\n",
      "1435 [D loss: 0.536851] [D acc: 0.726562] [G loss: 1.582230] [G acc: 0.070312]\n",
      "1436 [D loss: 0.490189] [D acc: 0.746094] [G loss: 1.012231] [G acc: 0.328125]\n",
      "1437 [D loss: 0.547375] [D acc: 0.691406] [G loss: 1.511345] [G acc: 0.101562]\n",
      "1438 [D loss: 0.477618] [D acc: 0.750000] [G loss: 1.127104] [G acc: 0.335938]\n",
      "1439 [D loss: 0.510662] [D acc: 0.761719] [G loss: 1.425426] [G acc: 0.179688]\n",
      "1440 [D loss: 0.441461] [D acc: 0.781250] [G loss: 1.256881] [G acc: 0.226562]\n",
      "1441 [D loss: 0.497111] [D acc: 0.750000] [G loss: 2.121490] [G acc: 0.054688]\n",
      "1442 [D loss: 0.478358] [D acc: 0.785156] [G loss: 0.773002] [G acc: 0.515625]\n",
      "1443 [D loss: 0.587357] [D acc: 0.683594] [G loss: 1.842416] [G acc: 0.062500]\n",
      "1444 [D loss: 0.523518] [D acc: 0.714844] [G loss: 1.011469] [G acc: 0.359375]\n",
      "1445 [D loss: 0.503289] [D acc: 0.722656] [G loss: 1.480330] [G acc: 0.164062]\n",
      "1446 [D loss: 0.532255] [D acc: 0.761719] [G loss: 0.975196] [G acc: 0.351562]\n",
      "1447 [D loss: 0.551627] [D acc: 0.738281] [G loss: 1.612921] [G acc: 0.109375]\n",
      "1448 [D loss: 0.485481] [D acc: 0.777344] [G loss: 1.216265] [G acc: 0.234375]\n",
      "1449 [D loss: 0.560149] [D acc: 0.699219] [G loss: 1.610970] [G acc: 0.156250]\n",
      "1450 [D loss: 0.496905] [D acc: 0.777344] [G loss: 1.133024] [G acc: 0.234375]\n",
      "1451 [D loss: 0.462256] [D acc: 0.761719] [G loss: 1.402988] [G acc: 0.156250]\n",
      "1452 [D loss: 0.447060] [D acc: 0.804688] [G loss: 1.102522] [G acc: 0.296875]\n",
      "1453 [D loss: 0.497824] [D acc: 0.769531] [G loss: 1.821385] [G acc: 0.078125]\n",
      "1454 [D loss: 0.506443] [D acc: 0.710938] [G loss: 0.730378] [G acc: 0.585938]\n",
      "1455 [D loss: 0.630423] [D acc: 0.648438] [G loss: 1.783247] [G acc: 0.054688]\n",
      "1456 [D loss: 0.594087] [D acc: 0.707031] [G loss: 0.910387] [G acc: 0.289062]\n",
      "1457 [D loss: 0.504965] [D acc: 0.746094] [G loss: 1.276708] [G acc: 0.156250]\n",
      "1458 [D loss: 0.513761] [D acc: 0.769531] [G loss: 1.330455] [G acc: 0.179688]\n",
      "1459 [D loss: 0.501868] [D acc: 0.730469] [G loss: 1.383453] [G acc: 0.203125]\n",
      "1460 [D loss: 0.501401] [D acc: 0.765625] [G loss: 1.212550] [G acc: 0.242188]\n",
      "1461 [D loss: 0.521363] [D acc: 0.726562] [G loss: 1.509142] [G acc: 0.117188]\n",
      "1462 [D loss: 0.522061] [D acc: 0.726562] [G loss: 0.931008] [G acc: 0.375000]\n",
      "1463 [D loss: 0.528996] [D acc: 0.722656] [G loss: 1.409983] [G acc: 0.164062]\n",
      "1464 [D loss: 0.525734] [D acc: 0.722656] [G loss: 0.900331] [G acc: 0.398438]\n",
      "1465 [D loss: 0.512992] [D acc: 0.738281] [G loss: 1.792840] [G acc: 0.046875]\n",
      "1466 [D loss: 0.568992] [D acc: 0.707031] [G loss: 0.686140] [G acc: 0.585938]\n",
      "1467 [D loss: 0.565336] [D acc: 0.710938] [G loss: 1.409508] [G acc: 0.132812]\n",
      "1468 [D loss: 0.535533] [D acc: 0.699219] [G loss: 1.086657] [G acc: 0.250000]\n",
      "1469 [D loss: 0.504035] [D acc: 0.750000] [G loss: 1.592378] [G acc: 0.078125]\n",
      "1470 [D loss: 0.501105] [D acc: 0.742188] [G loss: 0.976041] [G acc: 0.335938]\n",
      "1471 [D loss: 0.515673] [D acc: 0.726562] [G loss: 1.510993] [G acc: 0.078125]\n",
      "1472 [D loss: 0.464026] [D acc: 0.800781] [G loss: 1.116998] [G acc: 0.289062]\n",
      "1473 [D loss: 0.444258] [D acc: 0.800781] [G loss: 1.840089] [G acc: 0.085938]\n",
      "1474 [D loss: 0.506040] [D acc: 0.722656] [G loss: 1.004399] [G acc: 0.398438]\n",
      "1475 [D loss: 0.512705] [D acc: 0.730469] [G loss: 2.091224] [G acc: 0.039062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1476 [D loss: 0.610792] [D acc: 0.710938] [G loss: 0.650557] [G acc: 0.593750]\n",
      "1477 [D loss: 0.631812] [D acc: 0.640625] [G loss: 1.383916] [G acc: 0.117188]\n",
      "1478 [D loss: 0.560928] [D acc: 0.683594] [G loss: 1.089461] [G acc: 0.250000]\n",
      "1479 [D loss: 0.508751] [D acc: 0.722656] [G loss: 1.344093] [G acc: 0.125000]\n",
      "1480 [D loss: 0.476771] [D acc: 0.789062] [G loss: 1.362858] [G acc: 0.125000]\n",
      "1481 [D loss: 0.515905] [D acc: 0.722656] [G loss: 1.033533] [G acc: 0.304688]\n",
      "1482 [D loss: 0.432444] [D acc: 0.804688] [G loss: 1.646548] [G acc: 0.078125]\n",
      "1483 [D loss: 0.487613] [D acc: 0.773438] [G loss: 1.130878] [G acc: 0.320312]\n",
      "1484 [D loss: 0.471459] [D acc: 0.781250] [G loss: 1.777419] [G acc: 0.070312]\n",
      "1485 [D loss: 0.554914] [D acc: 0.742188] [G loss: 0.811972] [G acc: 0.476562]\n",
      "1486 [D loss: 0.566758] [D acc: 0.695312] [G loss: 1.823238] [G acc: 0.031250]\n",
      "1487 [D loss: 0.574593] [D acc: 0.714844] [G loss: 0.943572] [G acc: 0.390625]\n",
      "1488 [D loss: 0.579546] [D acc: 0.695312] [G loss: 1.383358] [G acc: 0.164062]\n",
      "1489 [D loss: 0.507885] [D acc: 0.726562] [G loss: 1.109323] [G acc: 0.273438]\n",
      "1490 [D loss: 0.528378] [D acc: 0.718750] [G loss: 1.502792] [G acc: 0.085938]\n",
      "1491 [D loss: 0.456682] [D acc: 0.808594] [G loss: 0.894972] [G acc: 0.382812]\n",
      "1492 [D loss: 0.547480] [D acc: 0.707031] [G loss: 1.670496] [G acc: 0.070312]\n",
      "1493 [D loss: 0.576477] [D acc: 0.699219] [G loss: 0.749914] [G acc: 0.515625]\n",
      "1494 [D loss: 0.606626] [D acc: 0.652344] [G loss: 1.235228] [G acc: 0.117188]\n",
      "1495 [D loss: 0.517155] [D acc: 0.753906] [G loss: 1.138443] [G acc: 0.148438]\n",
      "1496 [D loss: 0.491814] [D acc: 0.769531] [G loss: 1.280775] [G acc: 0.179688]\n",
      "1497 [D loss: 0.502667] [D acc: 0.773438] [G loss: 1.302249] [G acc: 0.179688]\n",
      "1498 [D loss: 0.482295] [D acc: 0.742188] [G loss: 1.462946] [G acc: 0.125000]\n",
      "1499 [D loss: 0.507376] [D acc: 0.734375] [G loss: 1.146318] [G acc: 0.257812]\n",
      "1500 [D loss: 0.513470] [D acc: 0.750000] [G loss: 1.821219] [G acc: 0.054688]\n",
      "1501 [D loss: 0.543663] [D acc: 0.718750] [G loss: 0.885160] [G acc: 0.429688]\n",
      "1502 [D loss: 0.599596] [D acc: 0.734375] [G loss: 1.797258] [G acc: 0.046875]\n",
      "1503 [D loss: 0.552906] [D acc: 0.683594] [G loss: 0.990038] [G acc: 0.335938]\n",
      "1504 [D loss: 0.512664] [D acc: 0.742188] [G loss: 1.555446] [G acc: 0.125000]\n",
      "1505 [D loss: 0.510748] [D acc: 0.753906] [G loss: 0.865969] [G acc: 0.476562]\n",
      "1506 [D loss: 0.547459] [D acc: 0.703125] [G loss: 1.647096] [G acc: 0.078125]\n",
      "1507 [D loss: 0.511859] [D acc: 0.757812] [G loss: 1.036012] [G acc: 0.289062]\n",
      "1508 [D loss: 0.522483] [D acc: 0.757812] [G loss: 1.825214] [G acc: 0.039062]\n",
      "1509 [D loss: 0.582847] [D acc: 0.710938] [G loss: 0.838670] [G acc: 0.445312]\n",
      "1510 [D loss: 0.568159] [D acc: 0.710938] [G loss: 1.462262] [G acc: 0.078125]\n",
      "1511 [D loss: 0.534564] [D acc: 0.765625] [G loss: 1.231186] [G acc: 0.140625]\n",
      "1512 [D loss: 0.506470] [D acc: 0.750000] [G loss: 1.221635] [G acc: 0.179688]\n",
      "1513 [D loss: 0.473972] [D acc: 0.761719] [G loss: 1.132141] [G acc: 0.265625]\n",
      "1514 [D loss: 0.489132] [D acc: 0.761719] [G loss: 1.311706] [G acc: 0.179688]\n",
      "1515 [D loss: 0.494481] [D acc: 0.785156] [G loss: 1.187445] [G acc: 0.265625]\n",
      "1516 [D loss: 0.479273] [D acc: 0.792969] [G loss: 1.729647] [G acc: 0.132812]\n",
      "1517 [D loss: 0.463786] [D acc: 0.781250] [G loss: 1.143260] [G acc: 0.335938]\n",
      "1518 [D loss: 0.456717] [D acc: 0.769531] [G loss: 1.782195] [G acc: 0.132812]\n",
      "1519 [D loss: 0.533127] [D acc: 0.730469] [G loss: 0.749326] [G acc: 0.554688]\n",
      "1520 [D loss: 0.575373] [D acc: 0.660156] [G loss: 2.537009] [G acc: 0.023438]\n",
      "1521 [D loss: 0.780215] [D acc: 0.636719] [G loss: 0.628352] [G acc: 0.609375]\n",
      "1522 [D loss: 0.615155] [D acc: 0.652344] [G loss: 1.009883] [G acc: 0.171875]\n",
      "1523 [D loss: 0.543458] [D acc: 0.738281] [G loss: 1.127265] [G acc: 0.195312]\n",
      "1524 [D loss: 0.517671] [D acc: 0.730469] [G loss: 1.175828] [G acc: 0.187500]\n",
      "1525 [D loss: 0.489872] [D acc: 0.789062] [G loss: 1.122952] [G acc: 0.179688]\n",
      "1526 [D loss: 0.507042] [D acc: 0.734375] [G loss: 1.290492] [G acc: 0.187500]\n",
      "1527 [D loss: 0.461034] [D acc: 0.753906] [G loss: 1.245731] [G acc: 0.140625]\n",
      "1528 [D loss: 0.442332] [D acc: 0.820312] [G loss: 1.384608] [G acc: 0.171875]\n",
      "1529 [D loss: 0.487434] [D acc: 0.773438] [G loss: 1.399956] [G acc: 0.148438]\n",
      "1530 [D loss: 0.440642] [D acc: 0.808594] [G loss: 1.472141] [G acc: 0.148438]\n",
      "1531 [D loss: 0.431965] [D acc: 0.792969] [G loss: 1.462307] [G acc: 0.140625]\n",
      "1532 [D loss: 0.471617] [D acc: 0.785156] [G loss: 1.453673] [G acc: 0.203125]\n",
      "1533 [D loss: 0.430514] [D acc: 0.804688] [G loss: 1.239805] [G acc: 0.273438]\n",
      "1534 [D loss: 0.534911] [D acc: 0.726562] [G loss: 2.247303] [G acc: 0.070312]\n",
      "1535 [D loss: 0.601171] [D acc: 0.683594] [G loss: 0.565357] [G acc: 0.648438]\n",
      "1536 [D loss: 0.805407] [D acc: 0.597656] [G loss: 1.641001] [G acc: 0.007812]\n",
      "1537 [D loss: 0.581190] [D acc: 0.703125] [G loss: 0.908928] [G acc: 0.351562]\n",
      "1538 [D loss: 0.503387] [D acc: 0.718750] [G loss: 1.307019] [G acc: 0.109375]\n",
      "1539 [D loss: 0.512020] [D acc: 0.726562] [G loss: 1.228614] [G acc: 0.218750]\n",
      "1540 [D loss: 0.509797] [D acc: 0.753906] [G loss: 1.307396] [G acc: 0.226562]\n",
      "1541 [D loss: 0.456072] [D acc: 0.820312] [G loss: 1.397502] [G acc: 0.156250]\n",
      "1542 [D loss: 0.469892] [D acc: 0.777344] [G loss: 1.251589] [G acc: 0.226562]\n",
      "1543 [D loss: 0.537007] [D acc: 0.726562] [G loss: 1.584325] [G acc: 0.125000]\n",
      "1544 [D loss: 0.569975] [D acc: 0.714844] [G loss: 0.947175] [G acc: 0.398438]\n",
      "1545 [D loss: 0.544283] [D acc: 0.699219] [G loss: 1.492773] [G acc: 0.101562]\n",
      "1546 [D loss: 0.511903] [D acc: 0.757812] [G loss: 0.987030] [G acc: 0.320312]\n",
      "1547 [D loss: 0.520748] [D acc: 0.718750] [G loss: 1.443115] [G acc: 0.109375]\n",
      "1548 [D loss: 0.520096] [D acc: 0.746094] [G loss: 1.045137] [G acc: 0.328125]\n",
      "1549 [D loss: 0.566041] [D acc: 0.703125] [G loss: 1.891368] [G acc: 0.039062]\n",
      "1550 [D loss: 0.631887] [D acc: 0.671875] [G loss: 0.806255] [G acc: 0.429688]\n",
      "1551 [D loss: 0.573634] [D acc: 0.699219] [G loss: 1.370535] [G acc: 0.101562]\n",
      "1552 [D loss: 0.498314] [D acc: 0.765625] [G loss: 1.026160] [G acc: 0.273438]\n",
      "1553 [D loss: 0.440700] [D acc: 0.804688] [G loss: 1.275927] [G acc: 0.140625]\n",
      "1554 [D loss: 0.453573] [D acc: 0.812500] [G loss: 1.392844] [G acc: 0.171875]\n",
      "1555 [D loss: 0.524889] [D acc: 0.726562] [G loss: 1.339363] [G acc: 0.203125]\n",
      "1556 [D loss: 0.513860] [D acc: 0.718750] [G loss: 1.161412] [G acc: 0.304688]\n",
      "1557 [D loss: 0.507365] [D acc: 0.722656] [G loss: 1.739000] [G acc: 0.085938]\n",
      "1558 [D loss: 0.543239] [D acc: 0.707031] [G loss: 0.693598] [G acc: 0.578125]\n",
      "1559 [D loss: 0.725305] [D acc: 0.625000] [G loss: 1.773180] [G acc: 0.015625]\n",
      "1560 [D loss: 0.630976] [D acc: 0.648438] [G loss: 0.841145] [G acc: 0.421875]\n",
      "1561 [D loss: 0.529091] [D acc: 0.703125] [G loss: 1.267186] [G acc: 0.171875]\n",
      "1562 [D loss: 0.495127] [D acc: 0.750000] [G loss: 1.291171] [G acc: 0.156250]\n",
      "1563 [D loss: 0.534520] [D acc: 0.734375] [G loss: 1.329308] [G acc: 0.156250]\n",
      "1564 [D loss: 0.487404] [D acc: 0.765625] [G loss: 1.416211] [G acc: 0.085938]\n",
      "1565 [D loss: 0.488332] [D acc: 0.742188] [G loss: 1.240851] [G acc: 0.195312]\n",
      "1566 [D loss: 0.435482] [D acc: 0.804688] [G loss: 1.311977] [G acc: 0.195312]\n",
      "1567 [D loss: 0.508605] [D acc: 0.746094] [G loss: 1.127738] [G acc: 0.265625]\n",
      "1568 [D loss: 0.502038] [D acc: 0.718750] [G loss: 1.688838] [G acc: 0.070312]\n",
      "1569 [D loss: 0.533418] [D acc: 0.722656] [G loss: 0.810433] [G acc: 0.500000]\n",
      "1570 [D loss: 0.601204] [D acc: 0.671875] [G loss: 1.947036] [G acc: 0.031250]\n",
      "1571 [D loss: 0.550448] [D acc: 0.695312] [G loss: 0.746850] [G acc: 0.523438]\n",
      "1572 [D loss: 0.552536] [D acc: 0.707031] [G loss: 1.479141] [G acc: 0.093750]\n",
      "1573 [D loss: 0.513726] [D acc: 0.730469] [G loss: 1.183319] [G acc: 0.242188]\n",
      "1574 [D loss: 0.484042] [D acc: 0.746094] [G loss: 1.484031] [G acc: 0.125000]\n",
      "1575 [D loss: 0.468304] [D acc: 0.769531] [G loss: 1.119944] [G acc: 0.242188]\n",
      "1576 [D loss: 0.525151] [D acc: 0.722656] [G loss: 1.514546] [G acc: 0.125000]\n",
      "1577 [D loss: 0.507295] [D acc: 0.750000] [G loss: 0.812168] [G acc: 0.468750]\n",
      "1578 [D loss: 0.579403] [D acc: 0.691406] [G loss: 1.793265] [G acc: 0.070312]\n",
      "1579 [D loss: 0.603165] [D acc: 0.671875] [G loss: 0.763900] [G acc: 0.476562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580 [D loss: 0.599619] [D acc: 0.679688] [G loss: 1.200144] [G acc: 0.140625]\n",
      "1581 [D loss: 0.501205] [D acc: 0.746094] [G loss: 1.164296] [G acc: 0.250000]\n",
      "1582 [D loss: 0.534078] [D acc: 0.742188] [G loss: 1.400831] [G acc: 0.164062]\n",
      "1583 [D loss: 0.547867] [D acc: 0.718750] [G loss: 1.079996] [G acc: 0.273438]\n",
      "1584 [D loss: 0.473930] [D acc: 0.804688] [G loss: 1.422299] [G acc: 0.125000]\n",
      "1585 [D loss: 0.517524] [D acc: 0.742188] [G loss: 0.980685] [G acc: 0.320312]\n",
      "1586 [D loss: 0.474709] [D acc: 0.773438] [G loss: 1.490695] [G acc: 0.109375]\n",
      "1587 [D loss: 0.537687] [D acc: 0.722656] [G loss: 1.060905] [G acc: 0.296875]\n",
      "1588 [D loss: 0.457684] [D acc: 0.785156] [G loss: 1.473726] [G acc: 0.117188]\n",
      "1589 [D loss: 0.452888] [D acc: 0.796875] [G loss: 1.344980] [G acc: 0.179688]\n",
      "1590 [D loss: 0.455355] [D acc: 0.796875] [G loss: 1.489369] [G acc: 0.140625]\n",
      "1591 [D loss: 0.470429] [D acc: 0.761719] [G loss: 1.103088] [G acc: 0.328125]\n",
      "1592 [D loss: 0.467279] [D acc: 0.804688] [G loss: 1.863496] [G acc: 0.101562]\n",
      "1593 [D loss: 0.482599] [D acc: 0.750000] [G loss: 0.592787] [G acc: 0.656250]\n",
      "1594 [D loss: 0.744948] [D acc: 0.636719] [G loss: 1.971679] [G acc: 0.039062]\n",
      "1595 [D loss: 0.702862] [D acc: 0.644531] [G loss: 0.839727] [G acc: 0.390625]\n",
      "1596 [D loss: 0.590767] [D acc: 0.675781] [G loss: 1.172871] [G acc: 0.195312]\n",
      "1597 [D loss: 0.533215] [D acc: 0.730469] [G loss: 0.926428] [G acc: 0.304688]\n",
      "1598 [D loss: 0.540104] [D acc: 0.707031] [G loss: 1.011856] [G acc: 0.281250]\n",
      "1599 [D loss: 0.486244] [D acc: 0.804688] [G loss: 1.460281] [G acc: 0.156250]\n",
      "1600 [D loss: 0.494948] [D acc: 0.757812] [G loss: 1.131684] [G acc: 0.304688]\n",
      "1601 [D loss: 0.573181] [D acc: 0.667969] [G loss: 1.751992] [G acc: 0.070312]\n",
      "1602 [D loss: 0.583903] [D acc: 0.691406] [G loss: 0.740227] [G acc: 0.531250]\n",
      "1603 [D loss: 0.592733] [D acc: 0.675781] [G loss: 1.464077] [G acc: 0.085938]\n",
      "1604 [D loss: 0.510268] [D acc: 0.726562] [G loss: 0.996603] [G acc: 0.265625]\n",
      "1605 [D loss: 0.547580] [D acc: 0.710938] [G loss: 1.346035] [G acc: 0.109375]\n",
      "1606 [D loss: 0.473078] [D acc: 0.761719] [G loss: 1.165862] [G acc: 0.218750]\n",
      "1607 [D loss: 0.459220] [D acc: 0.796875] [G loss: 1.243203] [G acc: 0.250000]\n",
      "1608 [D loss: 0.482556] [D acc: 0.757812] [G loss: 1.468780] [G acc: 0.101562]\n",
      "1609 [D loss: 0.437552] [D acc: 0.785156] [G loss: 1.406779] [G acc: 0.242188]\n",
      "1610 [D loss: 0.484789] [D acc: 0.785156] [G loss: 1.365415] [G acc: 0.179688]\n",
      "1611 [D loss: 0.481190] [D acc: 0.746094] [G loss: 1.173474] [G acc: 0.234375]\n",
      "1612 [D loss: 0.498979] [D acc: 0.765625] [G loss: 1.864157] [G acc: 0.062500]\n",
      "1613 [D loss: 0.493246] [D acc: 0.738281] [G loss: 0.959740] [G acc: 0.367188]\n",
      "1614 [D loss: 0.545946] [D acc: 0.722656] [G loss: 2.153796] [G acc: 0.015625]\n",
      "1615 [D loss: 0.639555] [D acc: 0.683594] [G loss: 0.658839] [G acc: 0.554688]\n",
      "1616 [D loss: 0.610451] [D acc: 0.671875] [G loss: 1.310874] [G acc: 0.164062]\n",
      "1617 [D loss: 0.512653] [D acc: 0.730469] [G loss: 1.201875] [G acc: 0.242188]\n",
      "1618 [D loss: 0.512214] [D acc: 0.746094] [G loss: 1.117410] [G acc: 0.250000]\n",
      "1619 [D loss: 0.535204] [D acc: 0.730469] [G loss: 1.303877] [G acc: 0.125000]\n",
      "1620 [D loss: 0.501813] [D acc: 0.769531] [G loss: 1.137093] [G acc: 0.257812]\n",
      "1621 [D loss: 0.479935] [D acc: 0.753906] [G loss: 1.394268] [G acc: 0.148438]\n",
      "1622 [D loss: 0.487458] [D acc: 0.769531] [G loss: 1.237178] [G acc: 0.171875]\n",
      "1623 [D loss: 0.505748] [D acc: 0.734375] [G loss: 1.310609] [G acc: 0.226562]\n",
      "1624 [D loss: 0.438637] [D acc: 0.777344] [G loss: 1.439406] [G acc: 0.203125]\n",
      "1625 [D loss: 0.470976] [D acc: 0.757812] [G loss: 1.165067] [G acc: 0.296875]\n",
      "1626 [D loss: 0.509487] [D acc: 0.734375] [G loss: 1.839763] [G acc: 0.085938]\n",
      "1627 [D loss: 0.515295] [D acc: 0.742188] [G loss: 0.770392] [G acc: 0.515625]\n",
      "1628 [D loss: 0.614893] [D acc: 0.648438] [G loss: 1.716206] [G acc: 0.062500]\n",
      "1629 [D loss: 0.519257] [D acc: 0.726562] [G loss: 1.033356] [G acc: 0.289062]\n",
      "1630 [D loss: 0.512821] [D acc: 0.738281] [G loss: 1.305642] [G acc: 0.148438]\n",
      "1631 [D loss: 0.465977] [D acc: 0.746094] [G loss: 1.220757] [G acc: 0.210938]\n",
      "1632 [D loss: 0.511240] [D acc: 0.738281] [G loss: 1.360001] [G acc: 0.179688]\n",
      "1633 [D loss: 0.473728] [D acc: 0.773438] [G loss: 1.134865] [G acc: 0.257812]\n",
      "1634 [D loss: 0.484273] [D acc: 0.742188] [G loss: 1.397000] [G acc: 0.148438]\n",
      "1635 [D loss: 0.465741] [D acc: 0.765625] [G loss: 0.988930] [G acc: 0.375000]\n",
      "1636 [D loss: 0.524761] [D acc: 0.726562] [G loss: 1.843432] [G acc: 0.085938]\n",
      "1637 [D loss: 0.495752] [D acc: 0.738281] [G loss: 0.767695] [G acc: 0.554688]\n",
      "1638 [D loss: 0.600621] [D acc: 0.664062] [G loss: 2.174801] [G acc: 0.007812]\n",
      "1639 [D loss: 0.606954] [D acc: 0.652344] [G loss: 0.823130] [G acc: 0.421875]\n",
      "1640 [D loss: 0.580483] [D acc: 0.707031] [G loss: 1.477268] [G acc: 0.062500]\n",
      "1641 [D loss: 0.524478] [D acc: 0.738281] [G loss: 1.158335] [G acc: 0.218750]\n",
      "1642 [D loss: 0.511841] [D acc: 0.753906] [G loss: 1.529148] [G acc: 0.148438]\n",
      "1643 [D loss: 0.480351] [D acc: 0.777344] [G loss: 1.084391] [G acc: 0.250000]\n",
      "1644 [D loss: 0.516730] [D acc: 0.738281] [G loss: 1.620146] [G acc: 0.101562]\n",
      "1645 [D loss: 0.500094] [D acc: 0.750000] [G loss: 1.117329] [G acc: 0.265625]\n",
      "1646 [D loss: 0.533582] [D acc: 0.714844] [G loss: 1.380278] [G acc: 0.125000]\n",
      "1647 [D loss: 0.500030] [D acc: 0.726562] [G loss: 1.325220] [G acc: 0.210938]\n",
      "1648 [D loss: 0.556893] [D acc: 0.695312] [G loss: 1.381622] [G acc: 0.203125]\n",
      "1649 [D loss: 0.437820] [D acc: 0.816406] [G loss: 1.153527] [G acc: 0.289062]\n",
      "1650 [D loss: 0.495841] [D acc: 0.757812] [G loss: 2.103763] [G acc: 0.062500]\n",
      "1651 [D loss: 0.534500] [D acc: 0.703125] [G loss: 0.768246] [G acc: 0.476562]\n",
      "1652 [D loss: 0.575815] [D acc: 0.667969] [G loss: 1.607189] [G acc: 0.093750]\n",
      "1653 [D loss: 0.460678] [D acc: 0.781250] [G loss: 1.335870] [G acc: 0.210938]\n",
      "1654 [D loss: 0.495727] [D acc: 0.777344] [G loss: 1.149048] [G acc: 0.242188]\n",
      "1655 [D loss: 0.475242] [D acc: 0.750000] [G loss: 1.264017] [G acc: 0.234375]\n",
      "1656 [D loss: 0.461783] [D acc: 0.781250] [G loss: 1.444652] [G acc: 0.148438]\n",
      "1657 [D loss: 0.459541] [D acc: 0.800781] [G loss: 1.273904] [G acc: 0.242188]\n",
      "1658 [D loss: 0.535616] [D acc: 0.730469] [G loss: 1.712680] [G acc: 0.140625]\n",
      "1659 [D loss: 0.431616] [D acc: 0.804688] [G loss: 1.121255] [G acc: 0.343750]\n",
      "1660 [D loss: 0.549265] [D acc: 0.695312] [G loss: 2.543694] [G acc: 0.031250]\n",
      "1661 [D loss: 0.815849] [D acc: 0.609375] [G loss: 0.460559] [G acc: 0.812500]\n",
      "1662 [D loss: 0.775354] [D acc: 0.632812] [G loss: 1.054946] [G acc: 0.179688]\n",
      "1663 [D loss: 0.517962] [D acc: 0.722656] [G loss: 1.066558] [G acc: 0.210938]\n",
      "1664 [D loss: 0.501158] [D acc: 0.746094] [G loss: 1.169343] [G acc: 0.234375]\n",
      "1665 [D loss: 0.525205] [D acc: 0.714844] [G loss: 1.198268] [G acc: 0.203125]\n",
      "1666 [D loss: 0.519865] [D acc: 0.738281] [G loss: 1.039541] [G acc: 0.250000]\n",
      "1667 [D loss: 0.468860] [D acc: 0.785156] [G loss: 1.420899] [G acc: 0.140625]\n",
      "1668 [D loss: 0.545020] [D acc: 0.730469] [G loss: 0.976282] [G acc: 0.320312]\n",
      "1669 [D loss: 0.509166] [D acc: 0.746094] [G loss: 1.546394] [G acc: 0.132812]\n",
      "1670 [D loss: 0.451633] [D acc: 0.785156] [G loss: 1.094777] [G acc: 0.335938]\n",
      "1671 [D loss: 0.570258] [D acc: 0.695312] [G loss: 1.374233] [G acc: 0.195312]\n",
      "1672 [D loss: 0.451231] [D acc: 0.808594] [G loss: 1.168419] [G acc: 0.265625]\n",
      "1673 [D loss: 0.532149] [D acc: 0.710938] [G loss: 1.571224] [G acc: 0.156250]\n",
      "1674 [D loss: 0.573449] [D acc: 0.671875] [G loss: 0.900831] [G acc: 0.406250]\n",
      "1675 [D loss: 0.558271] [D acc: 0.714844] [G loss: 1.997529] [G acc: 0.062500]\n",
      "1676 [D loss: 0.586486] [D acc: 0.691406] [G loss: 0.734245] [G acc: 0.515625]\n",
      "1677 [D loss: 0.633505] [D acc: 0.636719] [G loss: 1.242573] [G acc: 0.164062]\n",
      "1678 [D loss: 0.516964] [D acc: 0.742188] [G loss: 1.179754] [G acc: 0.195312]\n",
      "1679 [D loss: 0.480720] [D acc: 0.757812] [G loss: 1.373586] [G acc: 0.148438]\n",
      "1680 [D loss: 0.525279] [D acc: 0.734375] [G loss: 1.010352] [G acc: 0.343750]\n",
      "1681 [D loss: 0.505613] [D acc: 0.769531] [G loss: 1.495923] [G acc: 0.117188]\n",
      "1682 [D loss: 0.525358] [D acc: 0.703125] [G loss: 1.079754] [G acc: 0.257812]\n",
      "1683 [D loss: 0.481911] [D acc: 0.796875] [G loss: 1.922806] [G acc: 0.054688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1684 [D loss: 0.634037] [D acc: 0.660156] [G loss: 0.692900] [G acc: 0.500000]\n",
      "1685 [D loss: 0.611928] [D acc: 0.644531] [G loss: 1.297092] [G acc: 0.093750]\n",
      "1686 [D loss: 0.560796] [D acc: 0.726562] [G loss: 0.980162] [G acc: 0.273438]\n",
      "1687 [D loss: 0.558975] [D acc: 0.703125] [G loss: 1.184989] [G acc: 0.148438]\n",
      "1688 [D loss: 0.556326] [D acc: 0.703125] [G loss: 0.947779] [G acc: 0.312500]\n",
      "1689 [D loss: 0.534788] [D acc: 0.761719] [G loss: 1.284430] [G acc: 0.148438]\n",
      "1690 [D loss: 0.497459] [D acc: 0.722656] [G loss: 1.022070] [G acc: 0.289062]\n",
      "1691 [D loss: 0.519876] [D acc: 0.722656] [G loss: 1.707104] [G acc: 0.078125]\n",
      "1692 [D loss: 0.535538] [D acc: 0.703125] [G loss: 0.872320] [G acc: 0.375000]\n",
      "1693 [D loss: 0.619689] [D acc: 0.648438] [G loss: 1.547407] [G acc: 0.070312]\n",
      "1694 [D loss: 0.595905] [D acc: 0.664062] [G loss: 0.900370] [G acc: 0.390625]\n",
      "1695 [D loss: 0.564486] [D acc: 0.742188] [G loss: 1.386419] [G acc: 0.078125]\n",
      "1696 [D loss: 0.503115] [D acc: 0.757812] [G loss: 0.992620] [G acc: 0.289062]\n",
      "1697 [D loss: 0.474398] [D acc: 0.765625] [G loss: 1.366649] [G acc: 0.125000]\n",
      "1698 [D loss: 0.466710] [D acc: 0.757812] [G loss: 1.098758] [G acc: 0.242188]\n",
      "1699 [D loss: 0.463485] [D acc: 0.796875] [G loss: 1.504846] [G acc: 0.093750]\n",
      "1700 [D loss: 0.502983] [D acc: 0.718750] [G loss: 0.978663] [G acc: 0.367188]\n",
      "1701 [D loss: 0.494420] [D acc: 0.734375] [G loss: 1.953077] [G acc: 0.085938]\n",
      "1702 [D loss: 0.519070] [D acc: 0.726562] [G loss: 0.843149] [G acc: 0.414062]\n",
      "1703 [D loss: 0.582461] [D acc: 0.695312] [G loss: 1.832805] [G acc: 0.023438]\n",
      "1704 [D loss: 0.532855] [D acc: 0.750000] [G loss: 0.951641] [G acc: 0.312500]\n",
      "1705 [D loss: 0.497054] [D acc: 0.785156] [G loss: 1.644271] [G acc: 0.062500]\n",
      "1706 [D loss: 0.532369] [D acc: 0.695312] [G loss: 0.925691] [G acc: 0.406250]\n",
      "1707 [D loss: 0.502903] [D acc: 0.750000] [G loss: 1.373673] [G acc: 0.109375]\n",
      "1708 [D loss: 0.546592] [D acc: 0.703125] [G loss: 1.149820] [G acc: 0.156250]\n",
      "1709 [D loss: 0.504435] [D acc: 0.765625] [G loss: 1.465230] [G acc: 0.125000]\n",
      "1710 [D loss: 0.556347] [D acc: 0.695312] [G loss: 0.999640] [G acc: 0.281250]\n",
      "1711 [D loss: 0.499721] [D acc: 0.722656] [G loss: 1.375791] [G acc: 0.156250]\n",
      "1712 [D loss: 0.513426] [D acc: 0.769531] [G loss: 1.308756] [G acc: 0.195312]\n",
      "1713 [D loss: 0.474615] [D acc: 0.781250] [G loss: 1.140638] [G acc: 0.304688]\n",
      "1714 [D loss: 0.505917] [D acc: 0.746094] [G loss: 1.463262] [G acc: 0.156250]\n",
      "1715 [D loss: 0.461236] [D acc: 0.796875] [G loss: 1.208905] [G acc: 0.234375]\n",
      "1716 [D loss: 0.537763] [D acc: 0.710938] [G loss: 1.932531] [G acc: 0.015625]\n",
      "1717 [D loss: 0.492980] [D acc: 0.734375] [G loss: 0.803840] [G acc: 0.492188]\n",
      "1718 [D loss: 0.610265] [D acc: 0.656250] [G loss: 1.824760] [G acc: 0.054688]\n",
      "1719 [D loss: 0.627939] [D acc: 0.640625] [G loss: 0.915265] [G acc: 0.328125]\n",
      "1720 [D loss: 0.543648] [D acc: 0.722656] [G loss: 1.365622] [G acc: 0.132812]\n",
      "1721 [D loss: 0.484070] [D acc: 0.773438] [G loss: 0.975710] [G acc: 0.296875]\n",
      "1722 [D loss: 0.478219] [D acc: 0.761719] [G loss: 1.620741] [G acc: 0.101562]\n",
      "1723 [D loss: 0.518998] [D acc: 0.695312] [G loss: 0.838896] [G acc: 0.398438]\n",
      "1724 [D loss: 0.508067] [D acc: 0.761719] [G loss: 1.390983] [G acc: 0.171875]\n",
      "1725 [D loss: 0.499497] [D acc: 0.750000] [G loss: 1.073166] [G acc: 0.289062]\n",
      "1726 [D loss: 0.491704] [D acc: 0.738281] [G loss: 1.811509] [G acc: 0.093750]\n",
      "1727 [D loss: 0.515827] [D acc: 0.757812] [G loss: 0.821735] [G acc: 0.476562]\n",
      "1728 [D loss: 0.528147] [D acc: 0.722656] [G loss: 2.019001] [G acc: 0.023438]\n",
      "1729 [D loss: 0.603336] [D acc: 0.683594] [G loss: 0.704240] [G acc: 0.531250]\n",
      "1730 [D loss: 0.631756] [D acc: 0.632812] [G loss: 1.379304] [G acc: 0.085938]\n",
      "1731 [D loss: 0.513022] [D acc: 0.753906] [G loss: 0.973388] [G acc: 0.375000]\n",
      "1732 [D loss: 0.538237] [D acc: 0.730469] [G loss: 1.257736] [G acc: 0.171875]\n",
      "1733 [D loss: 0.483223] [D acc: 0.765625] [G loss: 1.377954] [G acc: 0.140625]\n",
      "1734 [D loss: 0.514879] [D acc: 0.753906] [G loss: 1.210448] [G acc: 0.210938]\n",
      "1735 [D loss: 0.456034] [D acc: 0.773438] [G loss: 1.390299] [G acc: 0.171875]\n",
      "1736 [D loss: 0.473955] [D acc: 0.746094] [G loss: 1.560201] [G acc: 0.078125]\n",
      "1737 [D loss: 0.527119] [D acc: 0.765625] [G loss: 1.025791] [G acc: 0.351562]\n",
      "1738 [D loss: 0.482402] [D acc: 0.753906] [G loss: 1.941317] [G acc: 0.054688]\n",
      "1739 [D loss: 0.619578] [D acc: 0.695312] [G loss: 0.739967] [G acc: 0.546875]\n",
      "1740 [D loss: 0.555207] [D acc: 0.753906] [G loss: 1.477041] [G acc: 0.093750]\n",
      "1741 [D loss: 0.539642] [D acc: 0.710938] [G loss: 0.955329] [G acc: 0.320312]\n",
      "1742 [D loss: 0.533317] [D acc: 0.738281] [G loss: 1.396255] [G acc: 0.085938]\n",
      "1743 [D loss: 0.519411] [D acc: 0.718750] [G loss: 1.183992] [G acc: 0.210938]\n",
      "1744 [D loss: 0.457616] [D acc: 0.808594] [G loss: 1.323833] [G acc: 0.218750]\n",
      "1745 [D loss: 0.472774] [D acc: 0.765625] [G loss: 1.226962] [G acc: 0.226562]\n",
      "1746 [D loss: 0.504113] [D acc: 0.730469] [G loss: 1.596558] [G acc: 0.109375]\n",
      "1747 [D loss: 0.506207] [D acc: 0.785156] [G loss: 1.132381] [G acc: 0.312500]\n",
      "1748 [D loss: 0.511259] [D acc: 0.730469] [G loss: 1.613096] [G acc: 0.109375]\n",
      "1749 [D loss: 0.535175] [D acc: 0.722656] [G loss: 0.719002] [G acc: 0.531250]\n",
      "1750 [D loss: 0.659963] [D acc: 0.660156] [G loss: 1.726080] [G acc: 0.062500]\n",
      "1751 [D loss: 0.549357] [D acc: 0.722656] [G loss: 1.004698] [G acc: 0.335938]\n",
      "1752 [D loss: 0.521700] [D acc: 0.765625] [G loss: 1.212598] [G acc: 0.218750]\n",
      "1753 [D loss: 0.463053] [D acc: 0.792969] [G loss: 1.286604] [G acc: 0.179688]\n",
      "1754 [D loss: 0.539846] [D acc: 0.746094] [G loss: 1.165263] [G acc: 0.226562]\n",
      "1755 [D loss: 0.456794] [D acc: 0.789062] [G loss: 1.315842] [G acc: 0.148438]\n",
      "1756 [D loss: 0.472432] [D acc: 0.785156] [G loss: 1.472609] [G acc: 0.195312]\n",
      "1757 [D loss: 0.470215] [D acc: 0.777344] [G loss: 1.447799] [G acc: 0.117188]\n",
      "1758 [D loss: 0.480580] [D acc: 0.738281] [G loss: 1.595608] [G acc: 0.101562]\n",
      "1759 [D loss: 0.523911] [D acc: 0.742188] [G loss: 0.915864] [G acc: 0.398438]\n",
      "1760 [D loss: 0.540644] [D acc: 0.691406] [G loss: 2.108478] [G acc: 0.054688]\n",
      "1761 [D loss: 0.647782] [D acc: 0.660156] [G loss: 0.724448] [G acc: 0.539062]\n",
      "1762 [D loss: 0.632195] [D acc: 0.648438] [G loss: 1.217613] [G acc: 0.179688]\n",
      "1763 [D loss: 0.498420] [D acc: 0.753906] [G loss: 1.146413] [G acc: 0.234375]\n",
      "1764 [D loss: 0.482806] [D acc: 0.769531] [G loss: 1.272897] [G acc: 0.164062]\n",
      "1765 [D loss: 0.463966] [D acc: 0.773438] [G loss: 1.286085] [G acc: 0.171875]\n",
      "1766 [D loss: 0.492758] [D acc: 0.757812] [G loss: 1.575993] [G acc: 0.140625]\n",
      "1767 [D loss: 0.465147] [D acc: 0.808594] [G loss: 1.600234] [G acc: 0.132812]\n",
      "1768 [D loss: 0.502606] [D acc: 0.730469] [G loss: 0.970762] [G acc: 0.375000]\n",
      "1769 [D loss: 0.593695] [D acc: 0.699219] [G loss: 1.763429] [G acc: 0.054688]\n",
      "1770 [D loss: 0.582806] [D acc: 0.695312] [G loss: 0.892599] [G acc: 0.414062]\n",
      "1771 [D loss: 0.531068] [D acc: 0.722656] [G loss: 1.366122] [G acc: 0.132812]\n",
      "1772 [D loss: 0.510287] [D acc: 0.722656] [G loss: 1.053478] [G acc: 0.304688]\n",
      "1773 [D loss: 0.563534] [D acc: 0.683594] [G loss: 1.865418] [G acc: 0.062500]\n",
      "1774 [D loss: 0.491374] [D acc: 0.734375] [G loss: 0.895776] [G acc: 0.398438]\n",
      "1775 [D loss: 0.557656] [D acc: 0.699219] [G loss: 1.542415] [G acc: 0.054688]\n",
      "1776 [D loss: 0.539642] [D acc: 0.734375] [G loss: 0.980963] [G acc: 0.320312]\n",
      "1777 [D loss: 0.469515] [D acc: 0.761719] [G loss: 1.462239] [G acc: 0.093750]\n",
      "1778 [D loss: 0.486490] [D acc: 0.777344] [G loss: 0.942557] [G acc: 0.343750]\n",
      "1779 [D loss: 0.574832] [D acc: 0.722656] [G loss: 1.809845] [G acc: 0.023438]\n",
      "1780 [D loss: 0.559327] [D acc: 0.726562] [G loss: 0.929314] [G acc: 0.335938]\n",
      "1781 [D loss: 0.508407] [D acc: 0.738281] [G loss: 1.276045] [G acc: 0.179688]\n",
      "1782 [D loss: 0.493844] [D acc: 0.753906] [G loss: 1.325756] [G acc: 0.164062]\n",
      "1783 [D loss: 0.499149] [D acc: 0.746094] [G loss: 1.221718] [G acc: 0.226562]\n",
      "1784 [D loss: 0.534960] [D acc: 0.722656] [G loss: 1.710388] [G acc: 0.070312]\n",
      "1785 [D loss: 0.451845] [D acc: 0.804688] [G loss: 1.043250] [G acc: 0.296875]\n",
      "1786 [D loss: 0.532829] [D acc: 0.714844] [G loss: 1.744477] [G acc: 0.070312]\n",
      "1787 [D loss: 0.581322] [D acc: 0.675781] [G loss: 0.698648] [G acc: 0.546875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788 [D loss: 0.604228] [D acc: 0.648438] [G loss: 1.562740] [G acc: 0.070312]\n",
      "1789 [D loss: 0.558781] [D acc: 0.707031] [G loss: 1.149711] [G acc: 0.210938]\n",
      "1790 [D loss: 0.487610] [D acc: 0.757812] [G loss: 1.287995] [G acc: 0.148438]\n",
      "1791 [D loss: 0.546409] [D acc: 0.718750] [G loss: 1.230445] [G acc: 0.203125]\n",
      "1792 [D loss: 0.493274] [D acc: 0.765625] [G loss: 1.379976] [G acc: 0.148438]\n",
      "1793 [D loss: 0.471401] [D acc: 0.792969] [G loss: 1.483499] [G acc: 0.101562]\n",
      "1794 [D loss: 0.531813] [D acc: 0.734375] [G loss: 1.441350] [G acc: 0.171875]\n",
      "1795 [D loss: 0.477785] [D acc: 0.746094] [G loss: 1.284606] [G acc: 0.250000]\n",
      "1796 [D loss: 0.483711] [D acc: 0.734375] [G loss: 1.654579] [G acc: 0.101562]\n",
      "1797 [D loss: 0.494285] [D acc: 0.746094] [G loss: 0.916210] [G acc: 0.359375]\n",
      "1798 [D loss: 0.556931] [D acc: 0.718750] [G loss: 2.077568] [G acc: 0.039062]\n",
      "1799 [D loss: 0.654880] [D acc: 0.683594] [G loss: 0.711594] [G acc: 0.546875]\n",
      "1800 [D loss: 0.630486] [D acc: 0.648438] [G loss: 1.393914] [G acc: 0.117188]\n",
      "1801 [D loss: 0.519502] [D acc: 0.742188] [G loss: 0.981183] [G acc: 0.289062]\n",
      "1802 [D loss: 0.525544] [D acc: 0.710938] [G loss: 1.286521] [G acc: 0.187500]\n",
      "1803 [D loss: 0.444402] [D acc: 0.804688] [G loss: 1.187671] [G acc: 0.234375]\n",
      "1804 [D loss: 0.464250] [D acc: 0.773438] [G loss: 1.398075] [G acc: 0.148438]\n",
      "1805 [D loss: 0.448279] [D acc: 0.804688] [G loss: 1.572562] [G acc: 0.117188]\n",
      "1806 [D loss: 0.505315] [D acc: 0.761719] [G loss: 0.949755] [G acc: 0.359375]\n",
      "1807 [D loss: 0.553627] [D acc: 0.722656] [G loss: 2.006988] [G acc: 0.054688]\n",
      "1808 [D loss: 0.693502] [D acc: 0.628906] [G loss: 0.775542] [G acc: 0.421875]\n",
      "1809 [D loss: 0.530963] [D acc: 0.734375] [G loss: 1.155206] [G acc: 0.125000]\n",
      "1810 [D loss: 0.516309] [D acc: 0.726562] [G loss: 1.098415] [G acc: 0.281250]\n",
      "1811 [D loss: 0.497322] [D acc: 0.761719] [G loss: 1.191430] [G acc: 0.171875]\n",
      "1812 [D loss: 0.483824] [D acc: 0.761719] [G loss: 1.280628] [G acc: 0.187500]\n",
      "1813 [D loss: 0.507948] [D acc: 0.746094] [G loss: 1.309599] [G acc: 0.195312]\n",
      "1814 [D loss: 0.491803] [D acc: 0.761719] [G loss: 1.230300] [G acc: 0.203125]\n",
      "1815 [D loss: 0.482083] [D acc: 0.746094] [G loss: 1.478114] [G acc: 0.140625]\n",
      "1816 [D loss: 0.444202] [D acc: 0.800781] [G loss: 1.067726] [G acc: 0.296875]\n",
      "1817 [D loss: 0.543541] [D acc: 0.707031] [G loss: 1.811207] [G acc: 0.078125]\n",
      "1818 [D loss: 0.494342] [D acc: 0.746094] [G loss: 0.819443] [G acc: 0.468750]\n",
      "1819 [D loss: 0.586834] [D acc: 0.679688] [G loss: 1.855883] [G acc: 0.046875]\n",
      "1820 [D loss: 0.564705] [D acc: 0.710938] [G loss: 0.994030] [G acc: 0.265625]\n",
      "1821 [D loss: 0.527550] [D acc: 0.726562] [G loss: 1.357578] [G acc: 0.140625]\n",
      "1822 [D loss: 0.529446] [D acc: 0.773438] [G loss: 1.096954] [G acc: 0.210938]\n",
      "1823 [D loss: 0.491375] [D acc: 0.777344] [G loss: 1.164313] [G acc: 0.210938]\n",
      "1824 [D loss: 0.439184] [D acc: 0.796875] [G loss: 1.449797] [G acc: 0.132812]\n",
      "1825 [D loss: 0.514793] [D acc: 0.730469] [G loss: 1.245354] [G acc: 0.195312]\n",
      "1826 [D loss: 0.481634] [D acc: 0.753906] [G loss: 1.661507] [G acc: 0.046875]\n",
      "1827 [D loss: 0.551425] [D acc: 0.687500] [G loss: 1.137319] [G acc: 0.281250]\n",
      "1828 [D loss: 0.444420] [D acc: 0.781250] [G loss: 1.592071] [G acc: 0.093750]\n",
      "1829 [D loss: 0.458416] [D acc: 0.781250] [G loss: 1.198822] [G acc: 0.265625]\n",
      "1830 [D loss: 0.447917] [D acc: 0.792969] [G loss: 1.669343] [G acc: 0.093750]\n",
      "1831 [D loss: 0.501770] [D acc: 0.750000] [G loss: 1.071209] [G acc: 0.335938]\n",
      "1832 [D loss: 0.544012] [D acc: 0.699219] [G loss: 1.860577] [G acc: 0.046875]\n",
      "1833 [D loss: 0.605790] [D acc: 0.707031] [G loss: 0.544156] [G acc: 0.656250]\n",
      "1834 [D loss: 0.734934] [D acc: 0.628906] [G loss: 1.689852] [G acc: 0.054688]\n",
      "1835 [D loss: 0.547081] [D acc: 0.707031] [G loss: 1.089739] [G acc: 0.218750]\n",
      "1836 [D loss: 0.472442] [D acc: 0.781250] [G loss: 1.170305] [G acc: 0.242188]\n",
      "1837 [D loss: 0.501350] [D acc: 0.730469] [G loss: 1.213028] [G acc: 0.195312]\n",
      "1838 [D loss: 0.493598] [D acc: 0.761719] [G loss: 1.067140] [G acc: 0.304688]\n",
      "1839 [D loss: 0.491706] [D acc: 0.738281] [G loss: 1.606441] [G acc: 0.093750]\n",
      "1840 [D loss: 0.492463] [D acc: 0.761719] [G loss: 0.923038] [G acc: 0.359375]\n",
      "1841 [D loss: 0.607699] [D acc: 0.687500] [G loss: 1.804383] [G acc: 0.023438]\n",
      "1842 [D loss: 0.556915] [D acc: 0.714844] [G loss: 0.957312] [G acc: 0.367188]\n",
      "1843 [D loss: 0.475360] [D acc: 0.761719] [G loss: 1.323588] [G acc: 0.140625]\n",
      "1844 [D loss: 0.517604] [D acc: 0.757812] [G loss: 1.097377] [G acc: 0.242188]\n",
      "1845 [D loss: 0.500638] [D acc: 0.769531] [G loss: 1.400047] [G acc: 0.125000]\n",
      "1846 [D loss: 0.530555] [D acc: 0.726562] [G loss: 1.163410] [G acc: 0.265625]\n",
      "1847 [D loss: 0.481773] [D acc: 0.753906] [G loss: 1.608000] [G acc: 0.117188]\n",
      "1848 [D loss: 0.517804] [D acc: 0.718750] [G loss: 1.066024] [G acc: 0.281250]\n",
      "1849 [D loss: 0.518481] [D acc: 0.738281] [G loss: 1.590605] [G acc: 0.070312]\n",
      "1850 [D loss: 0.513924] [D acc: 0.730469] [G loss: 0.916630] [G acc: 0.414062]\n",
      "1851 [D loss: 0.537483] [D acc: 0.726562] [G loss: 1.558928] [G acc: 0.070312]\n",
      "1852 [D loss: 0.496953] [D acc: 0.761719] [G loss: 1.007869] [G acc: 0.304688]\n",
      "1853 [D loss: 0.509465] [D acc: 0.750000] [G loss: 2.080818] [G acc: 0.046875]\n",
      "1854 [D loss: 0.479702] [D acc: 0.753906] [G loss: 0.842690] [G acc: 0.414062]\n",
      "1855 [D loss: 0.516399] [D acc: 0.750000] [G loss: 1.573331] [G acc: 0.070312]\n",
      "1856 [D loss: 0.517321] [D acc: 0.750000] [G loss: 0.891703] [G acc: 0.398438]\n",
      "1857 [D loss: 0.556595] [D acc: 0.695312] [G loss: 1.405077] [G acc: 0.101562]\n",
      "1858 [D loss: 0.460618] [D acc: 0.777344] [G loss: 1.191439] [G acc: 0.148438]\n",
      "1859 [D loss: 0.491419] [D acc: 0.792969] [G loss: 1.524414] [G acc: 0.109375]\n",
      "1860 [D loss: 0.535775] [D acc: 0.726562] [G loss: 1.196484] [G acc: 0.218750]\n",
      "1861 [D loss: 0.471456] [D acc: 0.777344] [G loss: 1.646561] [G acc: 0.109375]\n",
      "1862 [D loss: 0.570698] [D acc: 0.687500] [G loss: 0.753498] [G acc: 0.492188]\n",
      "1863 [D loss: 0.616473] [D acc: 0.664062] [G loss: 1.538814] [G acc: 0.085938]\n",
      "1864 [D loss: 0.560029] [D acc: 0.710938] [G loss: 0.937464] [G acc: 0.328125]\n",
      "1865 [D loss: 0.564011] [D acc: 0.699219] [G loss: 1.166174] [G acc: 0.171875]\n",
      "1866 [D loss: 0.499617] [D acc: 0.753906] [G loss: 1.105600] [G acc: 0.226562]\n",
      "1867 [D loss: 0.505097] [D acc: 0.765625] [G loss: 1.321088] [G acc: 0.171875]\n",
      "1868 [D loss: 0.500193] [D acc: 0.746094] [G loss: 1.312934] [G acc: 0.218750]\n",
      "1869 [D loss: 0.485803] [D acc: 0.765625] [G loss: 1.339242] [G acc: 0.179688]\n",
      "1870 [D loss: 0.460312] [D acc: 0.785156] [G loss: 1.317510] [G acc: 0.195312]\n",
      "1871 [D loss: 0.481332] [D acc: 0.746094] [G loss: 1.409313] [G acc: 0.171875]\n",
      "1872 [D loss: 0.518442] [D acc: 0.718750] [G loss: 1.593589] [G acc: 0.125000]\n",
      "1873 [D loss: 0.515426] [D acc: 0.734375] [G loss: 0.656250] [G acc: 0.593750]\n",
      "1874 [D loss: 0.741254] [D acc: 0.632812] [G loss: 2.264030] [G acc: 0.015625]\n",
      "1875 [D loss: 0.670295] [D acc: 0.652344] [G loss: 0.878684] [G acc: 0.351562]\n",
      "1876 [D loss: 0.540302] [D acc: 0.722656] [G loss: 1.084332] [G acc: 0.210938]\n",
      "1877 [D loss: 0.542829] [D acc: 0.753906] [G loss: 1.088644] [G acc: 0.226562]\n",
      "1878 [D loss: 0.506404] [D acc: 0.781250] [G loss: 1.147501] [G acc: 0.171875]\n",
      "1879 [D loss: 0.499684] [D acc: 0.765625] [G loss: 1.269838] [G acc: 0.148438]\n",
      "1880 [D loss: 0.491593] [D acc: 0.750000] [G loss: 1.270913] [G acc: 0.164062]\n",
      "1881 [D loss: 0.479174] [D acc: 0.773438] [G loss: 1.164098] [G acc: 0.273438]\n",
      "1882 [D loss: 0.491743] [D acc: 0.757812] [G loss: 1.395538] [G acc: 0.132812]\n",
      "1883 [D loss: 0.462149] [D acc: 0.773438] [G loss: 1.255121] [G acc: 0.226562]\n",
      "1884 [D loss: 0.544693] [D acc: 0.722656] [G loss: 1.333278] [G acc: 0.156250]\n",
      "1885 [D loss: 0.435519] [D acc: 0.816406] [G loss: 1.382292] [G acc: 0.156250]\n",
      "1886 [D loss: 0.521695] [D acc: 0.734375] [G loss: 1.191344] [G acc: 0.210938]\n",
      "1887 [D loss: 0.526092] [D acc: 0.683594] [G loss: 1.686215] [G acc: 0.101562]\n",
      "1888 [D loss: 0.540608] [D acc: 0.726562] [G loss: 0.718078] [G acc: 0.546875]\n",
      "1889 [D loss: 0.524854] [D acc: 0.757812] [G loss: 2.073106] [G acc: 0.039062]\n",
      "1890 [D loss: 0.659874] [D acc: 0.656250] [G loss: 0.683368] [G acc: 0.562500]\n",
      "1891 [D loss: 0.605126] [D acc: 0.632812] [G loss: 1.273916] [G acc: 0.101562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1892 [D loss: 0.491942] [D acc: 0.769531] [G loss: 1.061782] [G acc: 0.265625]\n",
      "1893 [D loss: 0.488946] [D acc: 0.769531] [G loss: 1.301575] [G acc: 0.125000]\n",
      "1894 [D loss: 0.494847] [D acc: 0.769531] [G loss: 1.204088] [G acc: 0.265625]\n",
      "1895 [D loss: 0.433101] [D acc: 0.816406] [G loss: 1.326168] [G acc: 0.210938]\n",
      "1896 [D loss: 0.457563] [D acc: 0.773438] [G loss: 1.693527] [G acc: 0.117188]\n",
      "1897 [D loss: 0.476200] [D acc: 0.750000] [G loss: 0.879972] [G acc: 0.476562]\n",
      "1898 [D loss: 0.594836] [D acc: 0.652344] [G loss: 2.211488] [G acc: 0.062500]\n",
      "1899 [D loss: 0.586698] [D acc: 0.703125] [G loss: 0.662940] [G acc: 0.593750]\n",
      "1900 [D loss: 0.608059] [D acc: 0.660156] [G loss: 1.454830] [G acc: 0.101562]\n",
      "1901 [D loss: 0.552501] [D acc: 0.726562] [G loss: 1.040115] [G acc: 0.281250]\n",
      "1902 [D loss: 0.553476] [D acc: 0.703125] [G loss: 1.228151] [G acc: 0.156250]\n",
      "1903 [D loss: 0.498310] [D acc: 0.757812] [G loss: 1.193736] [G acc: 0.203125]\n",
      "1904 [D loss: 0.494334] [D acc: 0.757812] [G loss: 1.238039] [G acc: 0.156250]\n",
      "1905 [D loss: 0.461531] [D acc: 0.812500] [G loss: 1.158629] [G acc: 0.257812]\n",
      "1906 [D loss: 0.472338] [D acc: 0.773438] [G loss: 1.735590] [G acc: 0.054688]\n",
      "1907 [D loss: 0.508409] [D acc: 0.726562] [G loss: 0.924602] [G acc: 0.375000]\n",
      "1908 [D loss: 0.495316] [D acc: 0.757812] [G loss: 1.587596] [G acc: 0.093750]\n",
      "1909 [D loss: 0.530124] [D acc: 0.718750] [G loss: 0.937884] [G acc: 0.375000]\n",
      "1910 [D loss: 0.507916] [D acc: 0.730469] [G loss: 1.760830] [G acc: 0.078125]\n",
      "1911 [D loss: 0.498710] [D acc: 0.746094] [G loss: 0.873676] [G acc: 0.367188]\n",
      "1912 [D loss: 0.484297] [D acc: 0.761719] [G loss: 1.393729] [G acc: 0.117188]\n",
      "1913 [D loss: 0.533659] [D acc: 0.726562] [G loss: 1.026178] [G acc: 0.328125]\n",
      "1914 [D loss: 0.496445] [D acc: 0.761719] [G loss: 1.412665] [G acc: 0.140625]\n",
      "1915 [D loss: 0.512266] [D acc: 0.722656] [G loss: 1.183029] [G acc: 0.250000]\n",
      "1916 [D loss: 0.520410] [D acc: 0.753906] [G loss: 1.291726] [G acc: 0.164062]\n",
      "1917 [D loss: 0.459036] [D acc: 0.781250] [G loss: 1.460495] [G acc: 0.125000]\n",
      "1918 [D loss: 0.433996] [D acc: 0.781250] [G loss: 1.838577] [G acc: 0.085938]\n",
      "1919 [D loss: 0.476715] [D acc: 0.777344] [G loss: 1.128451] [G acc: 0.296875]\n",
      "1920 [D loss: 0.494799] [D acc: 0.765625] [G loss: 2.250830] [G acc: 0.023438]\n",
      "1921 [D loss: 0.592889] [D acc: 0.714844] [G loss: 0.639389] [G acc: 0.632812]\n",
      "1922 [D loss: 0.740899] [D acc: 0.593750] [G loss: 1.619305] [G acc: 0.031250]\n",
      "1923 [D loss: 0.552697] [D acc: 0.718750] [G loss: 1.089960] [G acc: 0.226562]\n",
      "1924 [D loss: 0.488950] [D acc: 0.757812] [G loss: 1.393034] [G acc: 0.101562]\n",
      "1925 [D loss: 0.480962] [D acc: 0.785156] [G loss: 1.096467] [G acc: 0.242188]\n",
      "1926 [D loss: 0.511626] [D acc: 0.734375] [G loss: 1.436711] [G acc: 0.117188]\n",
      "1927 [D loss: 0.489753] [D acc: 0.757812] [G loss: 1.231195] [G acc: 0.195312]\n",
      "1928 [D loss: 0.487409] [D acc: 0.753906] [G loss: 1.461148] [G acc: 0.125000]\n",
      "1929 [D loss: 0.570704] [D acc: 0.718750] [G loss: 1.181550] [G acc: 0.203125]\n",
      "1930 [D loss: 0.484982] [D acc: 0.773438] [G loss: 1.553752] [G acc: 0.109375]\n",
      "1931 [D loss: 0.495269] [D acc: 0.750000] [G loss: 0.999661] [G acc: 0.320312]\n",
      "1932 [D loss: 0.525459] [D acc: 0.722656] [G loss: 1.747675] [G acc: 0.054688]\n",
      "1933 [D loss: 0.615084] [D acc: 0.675781] [G loss: 0.823737] [G acc: 0.437500]\n",
      "1934 [D loss: 0.528345] [D acc: 0.718750] [G loss: 1.453561] [G acc: 0.078125]\n",
      "1935 [D loss: 0.545171] [D acc: 0.707031] [G loss: 0.915199] [G acc: 0.359375]\n",
      "1936 [D loss: 0.540181] [D acc: 0.726562] [G loss: 1.349385] [G acc: 0.117188]\n",
      "1937 [D loss: 0.491495] [D acc: 0.765625] [G loss: 0.987072] [G acc: 0.328125]\n",
      "1938 [D loss: 0.487374] [D acc: 0.742188] [G loss: 1.648693] [G acc: 0.046875]\n",
      "1939 [D loss: 0.534440] [D acc: 0.710938] [G loss: 1.091701] [G acc: 0.281250]\n",
      "1940 [D loss: 0.481991] [D acc: 0.769531] [G loss: 1.677267] [G acc: 0.085938]\n",
      "1941 [D loss: 0.481357] [D acc: 0.765625] [G loss: 1.014575] [G acc: 0.320312]\n",
      "1942 [D loss: 0.562465] [D acc: 0.718750] [G loss: 1.825643] [G acc: 0.046875]\n",
      "1943 [D loss: 0.564021] [D acc: 0.699219] [G loss: 0.964280] [G acc: 0.281250]\n",
      "1944 [D loss: 0.555799] [D acc: 0.691406] [G loss: 1.378197] [G acc: 0.132812]\n",
      "1945 [D loss: 0.555552] [D acc: 0.656250] [G loss: 1.101564] [G acc: 0.273438]\n",
      "1946 [D loss: 0.500546] [D acc: 0.750000] [G loss: 1.235840] [G acc: 0.164062]\n",
      "1947 [D loss: 0.484668] [D acc: 0.773438] [G loss: 1.531114] [G acc: 0.156250]\n",
      "1948 [D loss: 0.473743] [D acc: 0.781250] [G loss: 0.977880] [G acc: 0.320312]\n",
      "1949 [D loss: 0.598221] [D acc: 0.664062] [G loss: 1.688047] [G acc: 0.054688]\n",
      "1950 [D loss: 0.517897] [D acc: 0.707031] [G loss: 0.911326] [G acc: 0.312500]\n",
      "1951 [D loss: 0.534433] [D acc: 0.750000] [G loss: 1.582097] [G acc: 0.101562]\n",
      "1952 [D loss: 0.517205] [D acc: 0.722656] [G loss: 0.966726] [G acc: 0.351562]\n",
      "1953 [D loss: 0.525466] [D acc: 0.730469] [G loss: 1.557968] [G acc: 0.125000]\n",
      "1954 [D loss: 0.530805] [D acc: 0.707031] [G loss: 0.872590] [G acc: 0.429688]\n",
      "1955 [D loss: 0.548177] [D acc: 0.703125] [G loss: 1.462082] [G acc: 0.078125]\n",
      "1956 [D loss: 0.498880] [D acc: 0.722656] [G loss: 1.041374] [G acc: 0.312500]\n",
      "1957 [D loss: 0.504444] [D acc: 0.742188] [G loss: 1.388534] [G acc: 0.093750]\n",
      "1958 [D loss: 0.546444] [D acc: 0.699219] [G loss: 1.187390] [G acc: 0.187500]\n",
      "1959 [D loss: 0.490545] [D acc: 0.792969] [G loss: 1.455188] [G acc: 0.187500]\n",
      "1960 [D loss: 0.480603] [D acc: 0.761719] [G loss: 1.260115] [G acc: 0.195312]\n",
      "1961 [D loss: 0.453483] [D acc: 0.796875] [G loss: 1.749982] [G acc: 0.070312]\n",
      "1962 [D loss: 0.505496] [D acc: 0.742188] [G loss: 0.997967] [G acc: 0.281250]\n",
      "1963 [D loss: 0.568830] [D acc: 0.703125] [G loss: 1.638123] [G acc: 0.093750]\n",
      "1964 [D loss: 0.533928] [D acc: 0.734375] [G loss: 0.838397] [G acc: 0.445312]\n",
      "1965 [D loss: 0.511805] [D acc: 0.722656] [G loss: 1.600008] [G acc: 0.085938]\n",
      "1966 [D loss: 0.546889] [D acc: 0.703125] [G loss: 0.768263] [G acc: 0.492188]\n",
      "1967 [D loss: 0.613668] [D acc: 0.656250] [G loss: 1.869907] [G acc: 0.007812]\n",
      "1968 [D loss: 0.587498] [D acc: 0.707031] [G loss: 0.864170] [G acc: 0.367188]\n",
      "1969 [D loss: 0.507743] [D acc: 0.761719] [G loss: 1.165996] [G acc: 0.203125]\n",
      "1970 [D loss: 0.500782] [D acc: 0.726562] [G loss: 1.245965] [G acc: 0.148438]\n",
      "1971 [D loss: 0.504618] [D acc: 0.773438] [G loss: 1.087277] [G acc: 0.226562]\n",
      "1972 [D loss: 0.528443] [D acc: 0.707031] [G loss: 1.346477] [G acc: 0.125000]\n",
      "1973 [D loss: 0.491646] [D acc: 0.781250] [G loss: 1.101963] [G acc: 0.289062]\n",
      "1974 [D loss: 0.484790] [D acc: 0.769531] [G loss: 1.373111] [G acc: 0.164062]\n",
      "1975 [D loss: 0.536446] [D acc: 0.710938] [G loss: 0.933806] [G acc: 0.398438]\n",
      "1976 [D loss: 0.496985] [D acc: 0.742188] [G loss: 1.523156] [G acc: 0.117188]\n",
      "1977 [D loss: 0.513669] [D acc: 0.726562] [G loss: 0.853643] [G acc: 0.429688]\n",
      "1978 [D loss: 0.581944] [D acc: 0.679688] [G loss: 1.591517] [G acc: 0.070312]\n",
      "1979 [D loss: 0.536079] [D acc: 0.707031] [G loss: 0.819184] [G acc: 0.421875]\n",
      "1980 [D loss: 0.506608] [D acc: 0.757812] [G loss: 1.397446] [G acc: 0.078125]\n",
      "1981 [D loss: 0.498167] [D acc: 0.738281] [G loss: 1.350143] [G acc: 0.125000]\n",
      "1982 [D loss: 0.464489] [D acc: 0.789062] [G loss: 1.415285] [G acc: 0.109375]\n",
      "1983 [D loss: 0.459414] [D acc: 0.781250] [G loss: 1.765072] [G acc: 0.093750]\n",
      "1984 [D loss: 0.525854] [D acc: 0.734375] [G loss: 1.020887] [G acc: 0.296875]\n",
      "1985 [D loss: 0.593266] [D acc: 0.691406] [G loss: 1.896225] [G acc: 0.015625]\n",
      "1986 [D loss: 0.554093] [D acc: 0.718750] [G loss: 0.900407] [G acc: 0.359375]\n",
      "1987 [D loss: 0.574321] [D acc: 0.707031] [G loss: 1.656157] [G acc: 0.070312]\n",
      "1988 [D loss: 0.545275] [D acc: 0.738281] [G loss: 1.120840] [G acc: 0.179688]\n",
      "1989 [D loss: 0.535057] [D acc: 0.726562] [G loss: 1.258996] [G acc: 0.156250]\n",
      "1990 [D loss: 0.512500] [D acc: 0.750000] [G loss: 1.037869] [G acc: 0.257812]\n",
      "1991 [D loss: 0.470474] [D acc: 0.765625] [G loss: 1.474706] [G acc: 0.125000]\n",
      "1992 [D loss: 0.485070] [D acc: 0.785156] [G loss: 0.885496] [G acc: 0.382812]\n",
      "1993 [D loss: 0.548655] [D acc: 0.675781] [G loss: 1.926115] [G acc: 0.015625]\n",
      "1994 [D loss: 0.578140] [D acc: 0.675781] [G loss: 0.836084] [G acc: 0.398438]\n",
      "1995 [D loss: 0.588800] [D acc: 0.656250] [G loss: 1.403993] [G acc: 0.070312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996 [D loss: 0.534380] [D acc: 0.722656] [G loss: 1.153309] [G acc: 0.195312]\n",
      "1997 [D loss: 0.513838] [D acc: 0.742188] [G loss: 1.301659] [G acc: 0.148438]\n",
      "1998 [D loss: 0.463077] [D acc: 0.789062] [G loss: 1.236676] [G acc: 0.179688]\n",
      "1999 [D loss: 0.481916] [D acc: 0.753906] [G loss: 1.298502] [G acc: 0.179688]\n"
     ]
    }
   ],
   "source": [
    "d_losses, g_losses, d_accs, g_accs = gan.train(     \n",
    "    x_train\n",
    "    , batch_size = 128\n",
    "    , epochs = 2000\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = 10\n",
    "    , initial_epoch = 0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x183b91320>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(g_accs, color='orange', linewidth=1)\n",
    "plt.plot(d_accs, color='green', linewidth=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34817725]\n",
      "[0.45862654]\n",
      "[0.9232486]\n",
      "[0.47461298]\n",
      "[0.30895877]\n",
      "[0.21166903]\n",
      "[0.88487977]\n",
      "[0.56153405]\n",
      "[0.5665859]\n",
      "[0.44828114]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(gan.discriminator.predict(np.array([x_train[i]]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.27551252], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAADyJJREFUeJzt3X+QVfV5x/HPw7KwYcEIKrggKRAZO5Q2WLfEVqfRockgdQo6GStpLSS0OBOttUkzdWw7sZ12xkmNJJ3J2JK4EdsEkk5CZaZME8O0MsZIXVF+KFGUgILLLzHlR2RZdp/+scfMRvd873XvufdcfN6vmZ299zzn7Hn2zH723HvPj6+5uwDEM6rsBgCUg/ADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwhqdCNXNsbGepvaG7lKIJTTOqUz3mvVzFtT+M1soaQvS2qR9DV3vzc1f5va9WFbUMsqASRs8U1Vzzvil/1m1iLpK5KukzRH0lIzmzPSnwegsWp5zz9f0kvuvsfdz0haJ2lxMW0BqLdawj9N0qtDnu/Ppv0CM1tpZt1m1t2n3hpWB6BIdf+0391Xu3unu3e2amy9VwegSrWE/4Ck6UOeX5JNA3AOqCX8T0mabWYzzWyMpJslbSimLQD1NuJDfe5+1sxul/Q9DR7q63L35wrrDEBd1XSc3903StpYUC8AGojTe4GgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiqplF6zWyvpBOS+iWddffOIpoCUH81hT9zrbsfLeDnAGggXvYDQdUafpf0fTN72sxWFtEQgMao9WX/1e5+wMwmS3rUzH7s7puHzpD9U1gpSW0aV+PqABSlpj2/ux/Ivh+WtF7S/GHmWe3une7e2aqxtawOQIFGHH4zazezCW89lvQxSTuLagxAfdXysn+KpPVm9tbP+aa7/1chXQGouxGH3933SPpQgb0AaCAO9QFBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Iq4u69QH0M3iti5NyL6eM9ij0/EBThB4Ii/EBQhB8IivADQRF+ICjCDwTFcX7UxMamR2HqveZXc2sDrel9T/uLr6dXfuyn6fr55+WWvL0tuajtey1dnzQxWT8984Jkfey2vbm1/qMVfu+CsOcHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAqHuc3sy5J10s67O5zs2mTJH1L0gxJeyXd5O5v1K9N1IuNrvAn8KHLkuVRJ04n622P7cytDZxOL9ufrEoa1ZIsj24fl1s7NX1Cctn2n6TvBbDnlqnJ+r8t/1Ky/np/e25t1dwrkstW2m7VqmbP/5CkhW+bdpekTe4+W9Km7DmAc0jF8Lv7ZknH3jZ5saQ12eM1kpYU3BeAOhvpe/4p7t6TPT4oaUpB/QBokJo/8HN3l5T7BsnMVppZt5l196m31tUBKMhIw3/IzDokKft+OG9Gd1/t7p3u3tmq9EUgABpnpOHfIGlZ9niZpEeKaQdAo1QMv5mtlfQjSZeZ2X4zWyHpXkkfNbPdkn4new7gHFLxOL+7L80pLSi4l4pSx6RtzqXJZQe2/7jodopT4f70NmZMst4y8fzc2isPpK8r/88rVifrff4/yfod1/5hst7fW7/PeUZPuShZ33/D9PxlFxxNLjv+mfHplVcYEqDVBpL1i0efyC+2pM9fKApn+AFBEX4gKMIPBEX4gaAIPxAU4QeCaq5bd1c45DX9h/lnCH5haldy2Y+/cHOy3ra8wgWkY1pzS69dl7688zeXb03W75y8KVmfWsOhnyP9Z5P1v3hlcbK+e236kt6Lj+xI1kdPy9823pY+hKnD6VtYH/j4rGR9yScfy62tmLglueyCOz6XrNtA+ljfks2fTi9/LP93v/Rn6d6Kwp4fCIrwA0ERfiAowg8ERfiBoAg/EBThB4JqquP8lS5dPXI6/zLLVkv/H/u7Wf+RrH/+0j9O1vfcmH+c/8kb/jG57OSW/Ns0D6pUH7kfvJm+NPXkivRQ0x0/fSm9gksuTpYvX5t/KfUfTXwyueytL34iWb9nxsPJ+sG+9+fWNp5Mn79Q6ZLd2f/0k2R94Hjikl1JA28mbr/tFVZeEPb8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUUx3n9zNnkvUdz8zMrZ2Ymb5u/Ss91yXrL/9+elPct2Btbm285Z8DUI1vn8w/Hi1Jf701fc39+iv/Jbe26jMrk8uO27c9We+9+leS9cl/mz7evfT8/82t9Xl63/PJ6T9M1j/SljtQlCRp26if5dZOefqckn+48ZvJ+kP3dSbrA6dOJevNgD0/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRV8Ti/mXVJul7SYXefm027R9KfSDqSzXa3u2+suZsK1zHPWp8/3PMTi9L3zt92MF3/3qJVyfruvvyhru8/9mvJZV/rzR9CW5L23Xhhsj7++vT1/lOvyt9uZ9vSYyGMSgzvLUnn/c3+ZH3RBenzBG779B25tXEvv5Fcdv/vTk7Wuxa+lqyPH5P/9/J7U7Yll/3lsemfrbPp80rOBdXs+R+StHCY6avcfV72VXvwATRUxfC7+2ZJxxrQC4AGquU9/+1mtt3MuswsfS8oAE1npOF/QNIHJc2T1CPpi3kzmtlKM+s2s+4+5b8HA9BYIwq/ux9y9353H5D0VUnzE/OudvdOd+9sVf5AmwAaa0ThN7OOIU9vkLSzmHYANEo1h/rWSrpG0oVmtl/S5yVdY2bzNHiD472Sbq1jjwDqoGL43X3pMJMfrEMvFY167JncWtdHfiu57IzR/5es3zbr9mS95cnnc2veW+mzjMQ92iVJ6WPpk3alj3ef8oHc2vufSV/z7udPSNa/NuuhZP1Lr1+ZrLc9mjie3v6+5LKzFqfvfb99x4xkvf2Vltza168dl1y2o/14sj7wZrp+LuAMPyAowg8ERfiBoAg/EBThB4Ii/EBQTXXr7lqc7TlY0/KjXk0fbmvMoMnDG7s7/bu9kBiKemBieojuPTem6+Mq3JZ83fNXJOuXeuL8r470IczPTP/3ZP3Pj9+UrPdOze/9Ex/oTi77G+/bk6x/6nN/mqxP//snkvVmwJ4fCIrwA0ERfiAowg8ERfiBoAg/EBThB4J6zxznfy/rP5S+LLfV+nNr13Y9mVx2+ZijyfraEx9I1i+7O31v17OpW1z3pH+v5Y9/KlkfPSZ9++yB/fmX7a5blx6y/euT0rc8v2R78w/BXQl7fiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IyrzCsNhFOs8m+YdtQcPWF8WoCfm33z70B3OTy16wK31b8ZYnnkvWve9Mso7G2uKbdNyPpU9SyLDnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgKl7Pb2bTJT0saYoGb1+/2t2/bGaTJH1L0gxJeyXd5O5v1K9V5Bk4kT+U9UX//KOafnaZ4xWgvqrZ85+V9Fl3nyPpSkm3mdkcSXdJ2uTusyVtyp4DOEdUDL+797j71uzxCUm7JE2TtFjSmmy2NZKW1KtJAMV7V+/5zWyGpMslbZE0xd17stJBDb4tAHCOqDr8ZjZe0nck3enux4fWfPACgWHfHprZSjPrNrPuPvXW1CyA4lQVfjNr1WDwv+Hu380mHzKzjqzeIWnYuzG6+2p373T3zlaNLaJnAAWoGH4zM0kPStrl7vcPKW2QtCx7vEzSI8W3B6Beqrl191WSbpG0w8yezabdLeleSd82sxWS9klKj5cMoKlUDL+7Py4p7/pgLs4HzlGc4QcERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IqmL4zWy6mf23mT1vZs+Z2Z9l0+8xswNm9mz2taj+7QIoyugq5jkr6bPuvtXMJkh62swezWqr3P2++rUHoF4qht/deyT1ZI9PmNkuSdPq3RiA+npX7/nNbIakyyVtySbdbmbbzazLzCbmLLPSzLrNrLtPvTU1C6A4VYffzMZL+o6kO939uKQHJH1Q0jwNvjL44nDLuftqd+90985WjS2gZQBFqCr8ZtaqweB/w92/K0nufsjd+919QNJXJc2vX5sAilbNp/0m6UFJu9z9/iHTO4bMdoOkncW3B6Beqvm0/ypJt0jaYWbPZtPulrTUzOZJckl7Jd1alw4B1EU1n/Y/LsmGKW0svh0AjcIZfkBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaDM3Ru3MrMjkvYNmXShpKMNa+DdadbemrUvid5GqsjefsndL6pmxoaG/x0rN+t2987SGkho1t6atS+J3kaqrN542Q8ERfiBoMoO/+qS15/SrL01a18SvY1UKb2V+p4fQHnK3vMDKEkp4TezhWb2gpm9ZGZ3ldFDHjPba2Y7spGHu0vupcvMDpvZziHTJpnZo2a2O/s+7DBpJfXWFCM3J0aWLnXbNduI1w1/2W9mLZJelPRRSfslPSVpqbs/39BGcpjZXkmd7l76MWEz+21JJyU97O5zs2lfkHTM3e/N/nFOdPe/bJLe7pF0suyRm7MBZTqGjiwtaYmk5Spx2yX6ukklbLcy9vzzJb3k7nvc/YykdZIWl9BH03P3zZKOvW3yYklrssdrNPjH03A5vTUFd+9x963Z4xOS3hpZutRtl+irFGWEf5qkV4c836/mGvLbJX3fzJ42s5VlNzOMKdmw6ZJ0UNKUMpsZRsWRmxvpbSNLN822G8mI10XjA793utrdf13SdZJuy17eNiUffM/WTIdrqhq5uVGGGVn658rcdiMd8bpoZYT/gKTpQ55fkk1rCu5+IPt+WNJ6Nd/ow4feGiQ1+3645H5+rplGbh5uZGk1wbZrphGvywj/U5Jmm9lMMxsj6WZJG0ro4x3MrD37IEZm1i7pY2q+0Yc3SFqWPV4m6ZESe/kFzTJyc97I0ip52zXdiNfu3vAvSYs0+In/y5L+qowecvqaJWlb9vVc2b1JWqvBl4F9GvxsZIWkCyRtkrRb0g8kTWqi3v5V0g5J2zUYtI6Sertagy/pt0t6NvtaVPa2S/RVynbjDD8gKD7wA4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q1P8DjIeHElg3d8MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = np.random.uniform(-1, 1, 100)\n",
    "img = gan.generator.predict(np.array([noise]))[0]\n",
    "\n",
    "print(img.shape)\n",
    "plt.imshow(img[:,:,0])\n",
    "\n",
    "gan.discriminator.predict(np.array([img]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gan.discriminator.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "for x, y in enumerate(gan.discriminator.layers):\n",
    "    \n",
    "    print(y)\n",
    "    print(y.trainable)\n",
    "    for i in gan.discriminator.layers[x].get_weights():\n",
    "        \n",
    "        print(pointer)\n",
    "        print(i.shape)\n",
    "        pointer+=1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gan.discriminator.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.get_weights()[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.model.save_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
