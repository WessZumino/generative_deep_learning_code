{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from utils.loaders import load_safari\n",
    "import pickle as pkl\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers.merge import _Merge\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import RandomNormal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = '0029'\n",
    "RUN_FOLDER = os.path.join(\"./run\", RUN_ID)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = load_safari('camel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13d56ec88>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[200,:,:,0], cmap = 'gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = (28,28,1)\n",
    "discriminator_conv_filters = [64,64,128,128]\n",
    "discriminator_conv_kernel_size = [5,5,5,5]\n",
    "discriminator_conv_strides = [2,2,2,1]\n",
    "discriminator_kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "\n",
    "discriminator_input = Input(shape=INPUT_DIM, name='discriminator_input')\n",
    "x = discriminator_input\n",
    "\n",
    "for i in range(len(discriminator_conv_filters)):\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters = discriminator_conv_filters[i]\n",
    "        , kernel_size = discriminator_conv_kernel_size[i]\n",
    "        , strides = discriminator_conv_strides[i]\n",
    "        , padding = 'same'\n",
    "        , name = 'discriminator_conv_' + str(i)\n",
    "        , kernel_initializer = discriminator_kernel_initializer\n",
    "        )(x)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(rate = 0.4)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(1, kernel_initializer = discriminator_kernel_initializer)(x)\n",
    "discriminator_output = Activation('sigmoid')(x)\n",
    "\n",
    "discriminator = Model(discriminator_input, discriminator_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "\n",
    "generator_initial_dense_layer_size = (7, 7, 64)\n",
    "generator_upsample = [2,2, 1, 1]\n",
    "generator_conv_filters = [128,64, 64,1]\n",
    "generator_conv_kernel_size = [5,5,5,5]\n",
    "generator_kernel_initializer = RandomNormal(mean=0., stddev=0.02)\n",
    "\n",
    "n_layers_generator = len(generator_conv_filters)\n",
    "\n",
    "generator_input = Input(shape=(z_dim,), name='generator_input')\n",
    "x = generator_input\n",
    "\n",
    "x = Dense(np.prod(generator_initial_dense_layer_size), kernel_initializer = generator_kernel_initializer)(generator_input)\n",
    "x = BatchNormalization(momentum = 0.9)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Reshape(generator_initial_dense_layer_size)(x)\n",
    "\n",
    "for i in range(n_layers_generator):\n",
    "\n",
    "    if generator_upsample[i] == 2:\n",
    "        x = UpSampling2D()(x)\n",
    "    x = Conv2D(\n",
    "        filters = generator_conv_filters[i]\n",
    "        , kernel_size = generator_conv_kernel_size[i]\n",
    "        , padding = 'same'\n",
    "        , name = 'generator_conv_' + str(i)\n",
    "        , kernel_initializer = generator_kernel_initializer\n",
    "        )(x)\n",
    "\n",
    "    if i < n_layers_generator - 1:\n",
    "        x = BatchNormalization(momentum = 0.9)(x)\n",
    "        x = Activation('relu')(x)\n",
    "    else:\n",
    "        x = Activation('tanh')(x)\n",
    "\n",
    "\n",
    "generator_output = x\n",
    "generator = Model(generator_input, generator_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 720,833\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_0 (Conv2D)    (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_1 (Conv2D)    (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_2 (Conv2D)    (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_3 (Conv2D)    (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 844,161\n",
      "Trainable params: 837,377\n",
      "Non-trainable params: 6,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPILE DISCRIMINATOR\n",
    "\n",
    "discriminator.compile(\n",
    "    optimizer= RMSprop(lr=0.0008)\n",
    "    , loss = 'binary_crossentropy'\n",
    "    ,  metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "### COMPILE COMBINED MODEL\n",
    "\n",
    "discriminator.trainable = False\n",
    "model_input = Input(shape=(z_dim,), name='model_input')\n",
    "model_output = discriminator(generator(model_input))\n",
    "model = Model(model_input, model_output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=RMSprop(lr=0.0004) \n",
    "    , loss='binary_crossentropy'\n",
    "    , metrics=['accuracy']\n",
    ")\n",
    "\n",
    "discriminator.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(x_train, batch_size):\n",
    "\n",
    "    valid = np.ones((batch_size,1))\n",
    "    fake = np.zeros((batch_size,1))\n",
    "\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    true_imgs = x_train[idx]\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    d_loss_real, d_acc_real =   discriminator.train_on_batch(true_imgs, valid)\n",
    "    d_loss_fake, d_acc_fake =   discriminator.train_on_batch(gen_imgs, fake)\n",
    "\n",
    "    d_loss =  0.5 * (d_loss_real + d_loss_fake)\n",
    "    d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
    "\n",
    "    return [d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake]\n",
    "\n",
    "def train_generator(batch_size):\n",
    "    valid = np.ones((batch_size,1))\n",
    "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "    return model.train_on_batch(noise, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfoster/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (0.732)(R 0.688, F 0.777)] [D acc: (0.312)(0.625, 0.000)] [G loss: 0.674] [G acc: 1.000]\n",
      "1 [D loss: (1.784)(R 0.643, F 2.924)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.679] [G acc: 1.000]\n",
      "2 [D loss: (0.695)(R 0.676, F 0.714)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.678] [G acc: 1.000]\n",
      "3 [D loss: (0.694)(R 0.675, F 0.713)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.678] [G acc: 1.000]\n",
      "4 [D loss: (0.692)(R 0.674, F 0.710)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.677] [G acc: 1.000]\n",
      "5 [D loss: (0.693)(R 0.673, F 0.714)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.677] [G acc: 1.000]\n",
      "6 [D loss: (0.692)(R 0.670, F 0.713)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.678] [G acc: 1.000]\n",
      "7 [D loss: (0.690)(R 0.665, F 0.715)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.678] [G acc: 1.000]\n",
      "8 [D loss: (0.686)(R 0.654, F 0.719)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.679] [G acc: 1.000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7c2fba685c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_acc_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_acc_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f114165bc4d8>\u001b[0m in \u001b[0;36mtrain_discriminator\u001b[0;34m(x_train, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_acc_real\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0md_loss_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_acc_fake\u001b[0m \u001b[0;34m=\u001b[0m   \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake = train_discriminator(x_train, batch_size)\n",
    "    g_loss, g_acc = train_generator(batch_size)\n",
    "\n",
    "    # Plot the progress\n",
    "    print (\"%d [D loss: (%.3f)(R %.3f, F %.3f)] [D acc: (%.3f)(%.3f, %.3f)] [G loss: %.3f] [G acc: %.3f]\" % (epoch, d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake, g_loss, g_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
