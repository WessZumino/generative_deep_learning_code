{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.GAN_IMP import GAN\n",
    "from utils.loaders import load_fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = '0007'\n",
    "RUN_FOLDER = os.path.join(\"./run\", RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = load_fashion_mnist(28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1205b2a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = (28,28,1)\n",
    "\n",
    "CONV_FILTERS = [64,64,128, 256]\n",
    "CONV_KERNEL_SIZES = [4,4,4,4]\n",
    "CONV_STRIDES = [2,2,2,1]\n",
    "CONV_PADDINGS = ['same', 'same', 'same', 'same']\n",
    "\n",
    "CONV_T_FILTERS = [128,64,1]\n",
    "CONV_T_KERNEL_SIZES = [4,4,4]\n",
    "CONV_T_STRIDES = [1,2,2]\n",
    "CONV_T_PADDINGS = ['same','same','same']\n",
    "\n",
    "Z_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(INPUT_DIM\n",
    "                , CONV_FILTERS\n",
    "                , CONV_KERNEL_SIZES\n",
    "                , CONV_STRIDES\n",
    "                , CONV_PADDINGS\n",
    "                , CONV_T_FILTERS\n",
    "                , CONV_T_KERNEL_SIZES\n",
    "                , CONV_T_STRIDES\n",
    "                , CONV_T_PADDINGS\n",
    "                , Z_DIM\n",
    "                 )\n",
    "\n",
    "gan.save(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 14, 14, 64)        1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 7, 7, 64)          65536     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         131072    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 256)         524288    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 4096      \n",
      "=================================================================\n",
      "Total params: 728,064\n",
      "Trainable params: 727,040\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_0 (Conv2DTr (None, 7, 7, 128)         262144    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "generator_conv_t_1 (Conv2DTr (None, 14, 14, 64)        131072    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "generator_conv_t_2 (Conv2DTr (None, 28, 28, 1)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,480\n",
      "Trainable params: 1,028,096\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40000\n",
    "PRINT_EVERY_N_BATCHES = 10\n",
    "INITIAL_EPOCH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfoster/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: -0.000176] [G loss: 0.000319]\n",
      "1 [D loss: -0.000175] [G loss: 0.000319]\n",
      "2 [D loss: -0.000177] [G loss: 0.000317]\n",
      "3 [D loss: -0.000178] [G loss: 0.000315]\n",
      "4 [D loss: -0.000179] [G loss: 0.000314]\n",
      "5 [D loss: -0.000179] [G loss: 0.000314]\n",
      "6 [D loss: -0.000179] [G loss: 0.000314]\n",
      "7 [D loss: -0.000179] [G loss: 0.000316]\n",
      "8 [D loss: -0.000179] [G loss: 0.000317]\n",
      "9 [D loss: -0.000180] [G loss: 0.000319]\n",
      "10 [D loss: -0.000180] [G loss: 0.000322]\n",
      "11 [D loss: -0.000180] [G loss: 0.000327]\n",
      "12 [D loss: -0.000180] [G loss: 0.000334]\n",
      "13 [D loss: -0.000181] [G loss: 0.000344]\n",
      "14 [D loss: -0.000180] [G loss: 0.000355]\n",
      "15 [D loss: -0.000181] [G loss: 0.000369]\n",
      "16 [D loss: -0.000182] [G loss: 0.000381]\n",
      "17 [D loss: -0.000183] [G loss: 0.000392]\n",
      "18 [D loss: -0.000186] [G loss: 0.000402]\n",
      "19 [D loss: -0.000190] [G loss: 0.000412]\n",
      "20 [D loss: -0.000194] [G loss: 0.000420]\n",
      "21 [D loss: -0.000199] [G loss: 0.000428]\n",
      "22 [D loss: -0.000204] [G loss: 0.000435]\n",
      "23 [D loss: -0.000210] [G loss: 0.000441]\n",
      "24 [D loss: -0.000215] [G loss: 0.000447]\n",
      "25 [D loss: -0.000218] [G loss: 0.000451]\n",
      "26 [D loss: -0.000223] [G loss: 0.000455]\n",
      "27 [D loss: -0.000225] [G loss: 0.000458]\n",
      "28 [D loss: -0.000224] [G loss: 0.000460]\n",
      "29 [D loss: -0.000223] [G loss: 0.000459]\n",
      "30 [D loss: -0.000228] [G loss: 0.000459]\n",
      "31 [D loss: -0.000206] [G loss: 0.000466]\n",
      "32 [D loss: -0.000205] [G loss: 0.000472]\n",
      "33 [D loss: -0.000235] [G loss: 0.000478]\n",
      "34 [D loss: -0.000211] [G loss: 0.000492]\n",
      "35 [D loss: -0.000182] [G loss: 0.000510]\n",
      "36 [D loss: -0.000218] [G loss: 0.000517]\n",
      "37 [D loss: -0.000250] [G loss: 0.000538]\n",
      "38 [D loss: -0.000164] [G loss: 0.000572]\n",
      "39 [D loss: -0.000250] [G loss: 0.000567]\n",
      "40 [D loss: -0.000236] [G loss: 0.000593]\n",
      "41 [D loss: -0.000192] [G loss: 0.000600]\n",
      "42 [D loss: -0.000138] [G loss: 0.000633]\n",
      "43 [D loss: -0.000204] [G loss: 0.000666]\n",
      "44 [D loss: -0.000188] [G loss: 0.000679]\n",
      "45 [D loss: -0.000217] [G loss: 0.000686]\n",
      "46 [D loss: -0.000157] [G loss: 0.000736]\n",
      "47 [D loss: -0.000199] [G loss: 0.000729]\n",
      "48 [D loss: -0.000209] [G loss: 0.000755]\n",
      "49 [D loss: -0.000253] [G loss: 0.000747]\n",
      "50 [D loss: -0.000330] [G loss: 0.000726]\n",
      "51 [D loss: -0.000236] [G loss: 0.000710]\n",
      "52 [D loss: -0.000194] [G loss: 0.000704]\n",
      "53 [D loss: -0.000253] [G loss: 0.000675]\n",
      "54 [D loss: -0.000265] [G loss: 0.000678]\n",
      "55 [D loss: -0.000259] [G loss: 0.000653]\n",
      "56 [D loss: -0.000253] [G loss: 0.000596]\n",
      "57 [D loss: -0.000198] [G loss: 0.000572]\n",
      "58 [D loss: -0.000226] [G loss: 0.000531]\n",
      "59 [D loss: -0.000206] [G loss: 0.000491]\n",
      "60 [D loss: -0.000153] [G loss: 0.000451]\n",
      "61 [D loss: -0.000224] [G loss: 0.000393]\n",
      "62 [D loss: -0.000128] [G loss: 0.000340]\n",
      "63 [D loss: -0.000084] [G loss: 0.000263]\n",
      "64 [D loss: -0.000109] [G loss: 0.000171]\n",
      "65 [D loss: -0.000129] [G loss: 0.000139]\n",
      "66 [D loss: -0.000045] [G loss: -0.000056]\n",
      "67 [D loss: 0.000020] [G loss: -0.000181]\n",
      "68 [D loss: 0.000103] [G loss: -0.000322]\n",
      "69 [D loss: 0.000279] [G loss: -0.000461]\n",
      "70 [D loss: 0.000291] [G loss: -0.000711]\n",
      "71 [D loss: 0.000436] [G loss: -0.000937]\n",
      "72 [D loss: 0.000391] [G loss: -0.001139]\n",
      "73 [D loss: 0.000514] [G loss: -0.001466]\n",
      "74 [D loss: 0.000510] [G loss: -0.001558]\n",
      "75 [D loss: 0.000469] [G loss: -0.001807]\n",
      "76 [D loss: 0.000449] [G loss: -0.002074]\n",
      "77 [D loss: 0.000786] [G loss: -0.002131]\n",
      "78 [D loss: 0.000522] [G loss: -0.002307]\n",
      "79 [D loss: 0.000532] [G loss: -0.002484]\n",
      "80 [D loss: 0.001011] [G loss: -0.002474]\n",
      "81 [D loss: 0.000511] [G loss: -0.002319]\n",
      "82 [D loss: 0.001058] [G loss: -0.002600]\n",
      "83 [D loss: 0.001074] [G loss: -0.002514]\n",
      "84 [D loss: 0.001233] [G loss: -0.002592]\n",
      "85 [D loss: 0.001072] [G loss: -0.002639]\n",
      "86 [D loss: 0.001605] [G loss: -0.002633]\n",
      "87 [D loss: 0.001450] [G loss: -0.002936]\n",
      "88 [D loss: 0.001223] [G loss: -0.002487]\n",
      "89 [D loss: 0.001516] [G loss: -0.002472]\n",
      "90 [D loss: 0.001684] [G loss: -0.002526]\n",
      "91 [D loss: 0.001657] [G loss: -0.001959]\n",
      "92 [D loss: 0.002037] [G loss: -0.002274]\n",
      "93 [D loss: 0.001220] [G loss: -0.002336]\n",
      "94 [D loss: 0.002181] [G loss: -0.002141]\n",
      "95 [D loss: 0.002197] [G loss: -0.002456]\n",
      "96 [D loss: 0.002318] [G loss: -0.002562]\n",
      "97 [D loss: 0.002886] [G loss: -0.002479]\n",
      "98 [D loss: 0.003488] [G loss: -0.002503]\n",
      "99 [D loss: 0.003257] [G loss: -0.002448]\n",
      "100 [D loss: 0.002893] [G loss: -0.002059]\n",
      "101 [D loss: 0.003205] [G loss: -0.002570]\n",
      "102 [D loss: 0.003892] [G loss: -0.002001]\n",
      "103 [D loss: 0.003236] [G loss: -0.002137]\n",
      "104 [D loss: 0.003029] [G loss: -0.001399]\n",
      "105 [D loss: 0.003630] [G loss: -0.001395]\n",
      "106 [D loss: 0.003215] [G loss: -0.000937]\n",
      "107 [D loss: 0.002786] [G loss: -0.000745]\n",
      "108 [D loss: 0.002766] [G loss: -0.000497]\n",
      "109 [D loss: 0.001826] [G loss: -0.000331]\n",
      "110 [D loss: 0.002037] [G loss: -0.001219]\n",
      "111 [D loss: 0.002223] [G loss: -0.000742]\n",
      "112 [D loss: 0.002575] [G loss: -0.001096]\n",
      "113 [D loss: 0.002080] [G loss: -0.000927]\n",
      "114 [D loss: 0.003358] [G loss: -0.000948]\n",
      "115 [D loss: 0.003513] [G loss: -0.002246]\n",
      "116 [D loss: 0.004103] [G loss: -0.002357]\n",
      "117 [D loss: 0.003494] [G loss: -0.002473]\n",
      "118 [D loss: 0.004271] [G loss: -0.003007]\n",
      "119 [D loss: 0.003410] [G loss: -0.003242]\n",
      "120 [D loss: 0.004572] [G loss: -0.003077]\n",
      "121 [D loss: 0.004471] [G loss: -0.003947]\n",
      "122 [D loss: 0.004973] [G loss: -0.004640]\n",
      "123 [D loss: 0.004429] [G loss: -0.003590]\n",
      "124 [D loss: 0.006121] [G loss: -0.005768]\n",
      "125 [D loss: 0.007268] [G loss: -0.004917]\n",
      "126 [D loss: 0.004570] [G loss: -0.005682]\n",
      "127 [D loss: 0.004448] [G loss: -0.006680]\n",
      "128 [D loss: 0.005939] [G loss: -0.005516]\n",
      "129 [D loss: 0.004856] [G loss: -0.005702]\n",
      "130 [D loss: 0.004572] [G loss: -0.006400]\n",
      "131 [D loss: 0.005284] [G loss: -0.005820]\n",
      "132 [D loss: 0.005687] [G loss: -0.006260]\n",
      "133 [D loss: 0.004574] [G loss: -0.005274]\n",
      "134 [D loss: 0.003656] [G loss: -0.005208]\n",
      "135 [D loss: 0.005273] [G loss: -0.005749]\n",
      "136 [D loss: 0.004652] [G loss: -0.006676]\n",
      "137 [D loss: 0.004453] [G loss: -0.004158]\n",
      "138 [D loss: 0.002699] [G loss: -0.006809]\n",
      "139 [D loss: 0.003501] [G loss: -0.005387]\n",
      "140 [D loss: 0.004775] [G loss: -0.004412]\n",
      "141 [D loss: 0.002976] [G loss: -0.004220]\n",
      "142 [D loss: 0.002899] [G loss: -0.003627]\n",
      "143 [D loss: 0.004528] [G loss: -0.004197]\n",
      "144 [D loss: 0.005603] [G loss: -0.003771]\n",
      "145 [D loss: 0.003713] [G loss: -0.003593]\n",
      "146 [D loss: 0.005555] [G loss: -0.003176]\n",
      "147 [D loss: 0.005348] [G loss: -0.003862]\n",
      "148 [D loss: 0.004277] [G loss: -0.002794]\n",
      "149 [D loss: 0.005507] [G loss: -0.002402]\n",
      "150 [D loss: 0.004453] [G loss: -0.001643]\n",
      "151 [D loss: 0.005484] [G loss: -0.001170]\n",
      "152 [D loss: 0.004191] [G loss: -0.000324]\n",
      "153 [D loss: 0.002167] [G loss: 0.000984]\n",
      "154 [D loss: 0.003329] [G loss: 0.000842]\n",
      "155 [D loss: 0.001668] [G loss: 0.002972]\n",
      "156 [D loss: 0.003730] [G loss: 0.003612]\n",
      "157 [D loss: 0.002852] [G loss: 0.003983]\n",
      "158 [D loss: 0.004526] [G loss: 0.003819]\n",
      "159 [D loss: 0.004895] [G loss: 0.003998]\n",
      "160 [D loss: 0.003207] [G loss: 0.004498]\n",
      "161 [D loss: 0.001801] [G loss: 0.004271]\n",
      "162 [D loss: -0.000641] [G loss: 0.004670]\n",
      "163 [D loss: 0.002330] [G loss: 0.004147]\n",
      "164 [D loss: 0.001660] [G loss: 0.004300]\n",
      "165 [D loss: 0.003235] [G loss: 0.004835]\n",
      "166 [D loss: 0.001723] [G loss: 0.004449]\n",
      "167 [D loss: 0.001055] [G loss: 0.004109]\n",
      "168 [D loss: 0.004130] [G loss: 0.004045]\n",
      "169 [D loss: 0.003513] [G loss: 0.003608]\n",
      "170 [D loss: 0.003103] [G loss: 0.003134]\n",
      "171 [D loss: 0.003437] [G loss: 0.002301]\n",
      "172 [D loss: 0.005514] [G loss: 0.001499]\n",
      "173 [D loss: 0.005278] [G loss: 0.001443]\n",
      "174 [D loss: 0.006042] [G loss: -0.000078]\n",
      "175 [D loss: 0.006055] [G loss: -0.000822]\n",
      "176 [D loss: 0.004269] [G loss: -0.000592]\n",
      "177 [D loss: 0.005300] [G loss: -0.001273]\n",
      "178 [D loss: 0.004945] [G loss: -0.001768]\n",
      "179 [D loss: 0.005743] [G loss: -0.004036]\n",
      "180 [D loss: 0.005373] [G loss: -0.003706]\n",
      "181 [D loss: 0.005347] [G loss: -0.004969]\n",
      "182 [D loss: 0.004081] [G loss: -0.003557]\n",
      "183 [D loss: 0.006540] [G loss: -0.007361]\n",
      "184 [D loss: 0.002764] [G loss: -0.004285]\n",
      "185 [D loss: 0.005482] [G loss: -0.005564]\n",
      "186 [D loss: 0.003928] [G loss: -0.005102]\n",
      "187 [D loss: 0.004246] [G loss: -0.004302]\n",
      "188 [D loss: 0.001167] [G loss: -0.005112]\n",
      "189 [D loss: 0.001199] [G loss: -0.005609]\n",
      "190 [D loss: 0.002530] [G loss: -0.004439]\n",
      "191 [D loss: 0.001613] [G loss: -0.006108]\n",
      "192 [D loss: 0.004539] [G loss: -0.005500]\n",
      "193 [D loss: 0.002362] [G loss: -0.003176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 [D loss: -0.000599] [G loss: -0.003894]\n",
      "195 [D loss: 0.001119] [G loss: -0.002901]\n",
      "196 [D loss: -0.000211] [G loss: -0.000118]\n",
      "197 [D loss: -0.000216] [G loss: -0.001035]\n",
      "198 [D loss: 0.004127] [G loss: 0.000069]\n",
      "199 [D loss: 0.002472] [G loss: 0.000171]\n",
      "200 [D loss: 0.001437] [G loss: -0.000967]\n",
      "201 [D loss: 0.001114] [G loss: 0.001608]\n",
      "202 [D loss: 0.000920] [G loss: 0.001252]\n",
      "203 [D loss: 0.000206] [G loss: 0.003629]\n",
      "204 [D loss: 0.003060] [G loss: 0.002655]\n",
      "205 [D loss: 0.002905] [G loss: 0.004553]\n",
      "206 [D loss: 0.002684] [G loss: 0.006228]\n",
      "207 [D loss: 0.000715] [G loss: 0.004804]\n",
      "208 [D loss: 0.000758] [G loss: 0.006826]\n",
      "209 [D loss: 0.001615] [G loss: 0.003890]\n",
      "210 [D loss: 0.001643] [G loss: 0.005552]\n",
      "211 [D loss: -0.000391] [G loss: 0.004838]\n",
      "212 [D loss: 0.002576] [G loss: 0.006570]\n",
      "213 [D loss: 0.002466] [G loss: 0.006075]\n",
      "214 [D loss: 0.001318] [G loss: 0.005534]\n",
      "215 [D loss: 0.004309] [G loss: 0.002437]\n",
      "216 [D loss: 0.001114] [G loss: 0.003097]\n",
      "217 [D loss: 0.004501] [G loss: 0.003349]\n",
      "218 [D loss: 0.002530] [G loss: 0.002407]\n",
      "219 [D loss: 0.003114] [G loss: 0.002690]\n",
      "220 [D loss: 0.004246] [G loss: 0.001281]\n",
      "221 [D loss: -0.000482] [G loss: -0.000164]\n",
      "222 [D loss: 0.002801] [G loss: -0.000120]\n",
      "223 [D loss: 0.000776] [G loss: 0.000047]\n",
      "224 [D loss: 0.001101] [G loss: -0.000288]\n",
      "225 [D loss: 0.001585] [G loss: -0.000285]\n",
      "226 [D loss: -0.002721] [G loss: -0.000274]\n",
      "227 [D loss: 0.000454] [G loss: -0.001378]\n",
      "228 [D loss: -0.002874] [G loss: -0.000473]\n",
      "229 [D loss: -0.002060] [G loss: -0.000330]\n",
      "230 [D loss: -0.000863] [G loss: 0.001509]\n",
      "231 [D loss: -0.000239] [G loss: 0.002155]\n",
      "232 [D loss: -0.001524] [G loss: 0.002918]\n",
      "233 [D loss: -0.001977] [G loss: 0.003218]\n",
      "234 [D loss: 0.000130] [G loss: 0.003486]\n",
      "235 [D loss: 0.000250] [G loss: 0.003504]\n",
      "236 [D loss: 0.000348] [G loss: 0.004027]\n",
      "237 [D loss: 0.001124] [G loss: 0.003190]\n",
      "238 [D loss: -0.001074] [G loss: 0.005355]\n",
      "239 [D loss: -0.000002] [G loss: 0.004906]\n",
      "240 [D loss: 0.001182] [G loss: 0.005498]\n",
      "241 [D loss: 0.003473] [G loss: 0.006152]\n",
      "242 [D loss: -0.000002] [G loss: 0.005566]\n",
      "243 [D loss: -0.001532] [G loss: 0.004337]\n",
      "244 [D loss: 0.001011] [G loss: 0.005490]\n",
      "245 [D loss: 0.001599] [G loss: 0.007099]\n",
      "246 [D loss: 0.001134] [G loss: 0.005002]\n",
      "247 [D loss: -0.000662] [G loss: 0.005599]\n",
      "248 [D loss: -0.002133] [G loss: 0.006076]\n",
      "249 [D loss: -0.000521] [G loss: 0.005987]\n",
      "250 [D loss: 0.002966] [G loss: 0.005684]\n",
      "251 [D loss: -0.000490] [G loss: 0.005513]\n",
      "252 [D loss: 0.000501] [G loss: 0.005165]\n",
      "253 [D loss: -0.001549] [G loss: 0.002889]\n",
      "254 [D loss: 0.000381] [G loss: 0.003849]\n",
      "255 [D loss: 0.000899] [G loss: 0.003544]\n",
      "256 [D loss: -0.000776] [G loss: 0.003302]\n",
      "257 [D loss: -0.002711] [G loss: 0.002684]\n",
      "258 [D loss: 0.000200] [G loss: 0.003452]\n",
      "259 [D loss: -0.000159] [G loss: 0.003668]\n",
      "260 [D loss: 0.000662] [G loss: 0.002981]\n",
      "261 [D loss: -0.000436] [G loss: 0.003130]\n",
      "262 [D loss: 0.000987] [G loss: 0.003017]\n",
      "263 [D loss: 0.000225] [G loss: 0.003860]\n",
      "264 [D loss: -0.001591] [G loss: 0.004153]\n",
      "265 [D loss: 0.000320] [G loss: 0.003161]\n",
      "266 [D loss: -0.000302] [G loss: 0.004057]\n",
      "267 [D loss: 0.001830] [G loss: 0.003297]\n",
      "268 [D loss: 0.000058] [G loss: 0.003837]\n",
      "269 [D loss: -0.002430] [G loss: 0.004993]\n",
      "270 [D loss: -0.001162] [G loss: 0.005282]\n",
      "271 [D loss: 0.000503] [G loss: 0.004313]\n",
      "272 [D loss: -0.001072] [G loss: 0.004298]\n",
      "273 [D loss: -0.001028] [G loss: 0.004772]\n",
      "274 [D loss: -0.000666] [G loss: 0.004808]\n",
      "275 [D loss: -0.000178] [G loss: 0.005877]\n",
      "276 [D loss: 0.000756] [G loss: 0.004476]\n",
      "277 [D loss: -0.000428] [G loss: 0.005215]\n",
      "278 [D loss: 0.001460] [G loss: 0.004941]\n",
      "279 [D loss: 0.000698] [G loss: 0.005280]\n",
      "280 [D loss: 0.001294] [G loss: 0.004909]\n",
      "281 [D loss: 0.001888] [G loss: 0.003821]\n",
      "282 [D loss: 0.001217] [G loss: 0.003224]\n",
      "283 [D loss: 0.002815] [G loss: 0.003213]\n",
      "284 [D loss: 0.001825] [G loss: 0.002056]\n",
      "285 [D loss: 0.001458] [G loss: 0.002348]\n",
      "286 [D loss: 0.000465] [G loss: 0.002794]\n",
      "287 [D loss: 0.000427] [G loss: 0.002279]\n",
      "288 [D loss: 0.000037] [G loss: 0.002693]\n",
      "289 [D loss: 0.000194] [G loss: 0.002787]\n",
      "290 [D loss: 0.001947] [G loss: 0.002429]\n",
      "291 [D loss: 0.001401] [G loss: 0.002140]\n",
      "292 [D loss: 0.001142] [G loss: 0.001573]\n",
      "293 [D loss: 0.002626] [G loss: 0.002480]\n",
      "294 [D loss: 0.003095] [G loss: 0.002932]\n",
      "295 [D loss: -0.000013] [G loss: 0.003190]\n",
      "296 [D loss: 0.003248] [G loss: 0.002893]\n",
      "297 [D loss: 0.000545] [G loss: 0.000604]\n",
      "298 [D loss: 0.002032] [G loss: 0.002390]\n",
      "299 [D loss: 0.002568] [G loss: 0.002218]\n",
      "300 [D loss: 0.002161] [G loss: 0.002128]\n",
      "301 [D loss: 0.000922] [G loss: 0.001932]\n",
      "302 [D loss: 0.001929] [G loss: 0.002346]\n",
      "303 [D loss: 0.002705] [G loss: 0.002042]\n",
      "304 [D loss: 0.002077] [G loss: 0.001542]\n",
      "305 [D loss: 0.001571] [G loss: 0.002926]\n",
      "306 [D loss: 0.000432] [G loss: 0.003016]\n",
      "307 [D loss: 0.001163] [G loss: 0.002969]\n",
      "308 [D loss: 0.001280] [G loss: 0.004220]\n",
      "309 [D loss: 0.002066] [G loss: 0.003344]\n",
      "310 [D loss: 0.001226] [G loss: 0.005217]\n",
      "311 [D loss: 0.001185] [G loss: 0.005826]\n",
      "312 [D loss: 0.000386] [G loss: 0.006242]\n",
      "313 [D loss: 0.000031] [G loss: 0.006206]\n",
      "314 [D loss: 0.002352] [G loss: 0.006077]\n",
      "315 [D loss: 0.000572] [G loss: 0.005427]\n",
      "316 [D loss: 0.000536] [G loss: 0.005307]\n",
      "317 [D loss: -0.000518] [G loss: 0.005406]\n",
      "318 [D loss: 0.000628] [G loss: 0.005178]\n",
      "319 [D loss: 0.000307] [G loss: 0.004694]\n",
      "320 [D loss: 0.000929] [G loss: 0.005065]\n",
      "321 [D loss: 0.001092] [G loss: 0.004286]\n",
      "322 [D loss: -0.000948] [G loss: 0.002899]\n",
      "323 [D loss: 0.001844] [G loss: 0.004108]\n",
      "324 [D loss: 0.001457] [G loss: 0.003383]\n",
      "325 [D loss: -0.000331] [G loss: 0.003243]\n",
      "326 [D loss: 0.000473] [G loss: 0.002462]\n",
      "327 [D loss: -0.001215] [G loss: 0.002416]\n",
      "328 [D loss: 0.000053] [G loss: 0.002495]\n",
      "329 [D loss: 0.000561] [G loss: 0.002540]\n",
      "330 [D loss: 0.000588] [G loss: 0.001272]\n",
      "331 [D loss: 0.001610] [G loss: 0.001705]\n",
      "332 [D loss: 0.000522] [G loss: 0.003319]\n",
      "333 [D loss: 0.000196] [G loss: 0.003106]\n",
      "334 [D loss: -0.000238] [G loss: 0.003448]\n",
      "335 [D loss: 0.000381] [G loss: 0.003136]\n",
      "336 [D loss: 0.000340] [G loss: 0.001404]\n",
      "337 [D loss: 0.002183] [G loss: 0.002854]\n",
      "338 [D loss: 0.000960] [G loss: 0.002807]\n",
      "339 [D loss: 0.000798] [G loss: 0.002154]\n",
      "340 [D loss: -0.002779] [G loss: 0.002651]\n",
      "341 [D loss: -0.000120] [G loss: 0.001984]\n",
      "342 [D loss: -0.000542] [G loss: 0.001966]\n",
      "343 [D loss: -0.001294] [G loss: 0.002273]\n",
      "344 [D loss: -0.000733] [G loss: 0.001949]\n",
      "345 [D loss: -0.001101] [G loss: 0.002668]\n",
      "346 [D loss: -0.002134] [G loss: 0.001811]\n",
      "347 [D loss: -0.000774] [G loss: 0.002513]\n",
      "348 [D loss: -0.001233] [G loss: 0.002030]\n",
      "349 [D loss: -0.002023] [G loss: 0.001443]\n",
      "350 [D loss: -0.000257] [G loss: 0.001637]\n",
      "351 [D loss: -0.000377] [G loss: 0.001657]\n",
      "352 [D loss: -0.000831] [G loss: 0.001353]\n",
      "353 [D loss: -0.000166] [G loss: 0.000494]\n",
      "354 [D loss: 0.000242] [G loss: 0.001582]\n",
      "355 [D loss: -0.000264] [G loss: 0.001543]\n",
      "356 [D loss: -0.001572] [G loss: 0.001320]\n",
      "357 [D loss: -0.001659] [G loss: 0.001052]\n",
      "358 [D loss: -0.001073] [G loss: 0.001177]\n",
      "359 [D loss: 0.000912] [G loss: 0.001709]\n",
      "360 [D loss: -0.001375] [G loss: 0.000867]\n",
      "361 [D loss: -0.001316] [G loss: 0.000675]\n",
      "362 [D loss: 0.000428] [G loss: 0.001529]\n",
      "363 [D loss: -0.000070] [G loss: 0.001079]\n",
      "364 [D loss: -0.000138] [G loss: 0.001305]\n",
      "365 [D loss: -0.001791] [G loss: 0.001575]\n",
      "366 [D loss: -0.001557] [G loss: 0.000537]\n",
      "367 [D loss: -0.002817] [G loss: 0.000534]\n",
      "368 [D loss: -0.000681] [G loss: 0.001659]\n",
      "369 [D loss: -0.000143] [G loss: 0.000622]\n",
      "370 [D loss: -0.000836] [G loss: 0.001430]\n",
      "371 [D loss: 0.000414] [G loss: 0.001008]\n",
      "372 [D loss: -0.001459] [G loss: 0.001094]\n",
      "373 [D loss: 0.000632] [G loss: 0.001326]\n",
      "374 [D loss: -0.000266] [G loss: 0.001018]\n",
      "375 [D loss: -0.000542] [G loss: 0.000849]\n",
      "376 [D loss: 0.001298] [G loss: 0.000858]\n",
      "377 [D loss: -0.001900] [G loss: 0.001121]\n",
      "378 [D loss: 0.000470] [G loss: 0.001052]\n",
      "379 [D loss: -0.000246] [G loss: 0.001615]\n",
      "380 [D loss: -0.001564] [G loss: 0.000543]\n",
      "381 [D loss: -0.000628] [G loss: 0.000884]\n",
      "382 [D loss: 0.000329] [G loss: 0.001785]\n",
      "383 [D loss: -0.000253] [G loss: 0.000284]\n",
      "384 [D loss: 0.000007] [G loss: 0.000810]\n",
      "385 [D loss: -0.000765] [G loss: 0.000830]\n",
      "386 [D loss: -0.001155] [G loss: 0.001103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387 [D loss: -0.001668] [G loss: 0.000711]\n",
      "388 [D loss: -0.001579] [G loss: 0.000605]\n",
      "389 [D loss: -0.000685] [G loss: 0.001331]\n",
      "390 [D loss: -0.000448] [G loss: 0.000569]\n",
      "391 [D loss: -0.000407] [G loss: 0.000356]\n",
      "392 [D loss: -0.001073] [G loss: 0.001019]\n",
      "393 [D loss: 0.000060] [G loss: 0.000679]\n",
      "394 [D loss: -0.000173] [G loss: 0.000246]\n",
      "395 [D loss: -0.000601] [G loss: -0.000124]\n",
      "396 [D loss: -0.000346] [G loss: 0.000376]\n",
      "397 [D loss: -0.001117] [G loss: 0.000228]\n",
      "398 [D loss: -0.000388] [G loss: 0.000899]\n",
      "399 [D loss: -0.001266] [G loss: 0.001702]\n",
      "400 [D loss: -0.000720] [G loss: 0.001990]\n",
      "401 [D loss: -0.000709] [G loss: 0.000427]\n",
      "402 [D loss: 0.000136] [G loss: 0.001432]\n",
      "403 [D loss: -0.000322] [G loss: 0.000519]\n",
      "404 [D loss: -0.001943] [G loss: 0.000385]\n",
      "405 [D loss: -0.001820] [G loss: 0.000548]\n",
      "406 [D loss: -0.001222] [G loss: 0.000762]\n",
      "407 [D loss: -0.000962] [G loss: 0.000318]\n",
      "408 [D loss: -0.000701] [G loss: 0.000685]\n",
      "409 [D loss: -0.001095] [G loss: -0.000516]\n",
      "410 [D loss: -0.000336] [G loss: 0.000111]\n",
      "411 [D loss: -0.000973] [G loss: -0.000885]\n",
      "412 [D loss: 0.000336] [G loss: -0.000544]\n",
      "413 [D loss: 0.000162] [G loss: -0.000260]\n",
      "414 [D loss: -0.000416] [G loss: -0.000013]\n",
      "415 [D loss: -0.001657] [G loss: 0.000399]\n",
      "416 [D loss: -0.000793] [G loss: 0.000573]\n",
      "417 [D loss: -0.001164] [G loss: 0.000694]\n",
      "418 [D loss: -0.001919] [G loss: 0.001099]\n",
      "419 [D loss: -0.001154] [G loss: 0.000961]\n",
      "420 [D loss: -0.001351] [G loss: 0.001506]\n",
      "421 [D loss: -0.001290] [G loss: 0.001573]\n",
      "422 [D loss: -0.001354] [G loss: 0.001495]\n",
      "423 [D loss: -0.000284] [G loss: 0.001592]\n",
      "424 [D loss: -0.001011] [G loss: 0.001472]\n",
      "425 [D loss: -0.000290] [G loss: 0.001523]\n",
      "426 [D loss: -0.000691] [G loss: 0.001433]\n",
      "427 [D loss: -0.001528] [G loss: 0.001571]\n",
      "428 [D loss: -0.001526] [G loss: 0.001209]\n",
      "429 [D loss: -0.000428] [G loss: 0.001372]\n",
      "430 [D loss: -0.000249] [G loss: 0.000559]\n",
      "431 [D loss: -0.001123] [G loss: 0.001076]\n",
      "432 [D loss: -0.001910] [G loss: 0.000141]\n",
      "433 [D loss: -0.001169] [G loss: -0.000067]\n",
      "434 [D loss: -0.001779] [G loss: -0.000010]\n",
      "435 [D loss: -0.000956] [G loss: -0.000154]\n",
      "436 [D loss: -0.001018] [G loss: 0.000003]\n",
      "437 [D loss: -0.001498] [G loss: 0.000412]\n",
      "438 [D loss: -0.000763] [G loss: 0.000026]\n",
      "439 [D loss: -0.000786] [G loss: -0.000227]\n",
      "440 [D loss: -0.001291] [G loss: -0.000428]\n",
      "441 [D loss: -0.001496] [G loss: -0.000152]\n",
      "442 [D loss: -0.000316] [G loss: 0.000180]\n",
      "443 [D loss: -0.000240] [G loss: 0.000045]\n",
      "444 [D loss: -0.000429] [G loss: 0.000513]\n",
      "445 [D loss: -0.001243] [G loss: 0.000829]\n",
      "446 [D loss: -0.000921] [G loss: 0.000442]\n",
      "447 [D loss: -0.001704] [G loss: 0.000831]\n",
      "448 [D loss: -0.001031] [G loss: 0.001464]\n",
      "449 [D loss: -0.000454] [G loss: 0.001877]\n",
      "450 [D loss: -0.000773] [G loss: 0.001823]\n",
      "451 [D loss: -0.000855] [G loss: 0.002241]\n",
      "452 [D loss: -0.000214] [G loss: 0.002117]\n",
      "453 [D loss: -0.000103] [G loss: 0.001499]\n",
      "454 [D loss: -0.001602] [G loss: 0.002097]\n",
      "455 [D loss: -0.000884] [G loss: 0.002759]\n",
      "456 [D loss: -0.000816] [G loss: 0.002281]\n",
      "457 [D loss: -0.000517] [G loss: 0.002016]\n",
      "458 [D loss: -0.000843] [G loss: 0.002249]\n",
      "459 [D loss: -0.000710] [G loss: 0.002093]\n",
      "460 [D loss: -0.001423] [G loss: 0.001865]\n",
      "461 [D loss: -0.000837] [G loss: 0.001618]\n",
      "462 [D loss: -0.001282] [G loss: 0.001274]\n",
      "463 [D loss: -0.000766] [G loss: 0.002290]\n",
      "464 [D loss: -0.001129] [G loss: 0.002110]\n",
      "465 [D loss: -0.000801] [G loss: 0.002308]\n",
      "466 [D loss: -0.001352] [G loss: 0.001762]\n",
      "467 [D loss: -0.001268] [G loss: 0.001849]\n",
      "468 [D loss: -0.000937] [G loss: 0.002173]\n",
      "469 [D loss: -0.001419] [G loss: 0.001794]\n",
      "470 [D loss: -0.000650] [G loss: 0.002152]\n",
      "471 [D loss: -0.000964] [G loss: 0.001453]\n",
      "472 [D loss: -0.000711] [G loss: 0.001628]\n",
      "473 [D loss: -0.001345] [G loss: 0.001533]\n",
      "474 [D loss: -0.000743] [G loss: 0.001440]\n",
      "475 [D loss: -0.000836] [G loss: 0.001281]\n",
      "476 [D loss: -0.000728] [G loss: 0.001536]\n",
      "477 [D loss: -0.000720] [G loss: 0.001788]\n",
      "478 [D loss: -0.001154] [G loss: 0.002049]\n",
      "479 [D loss: -0.000831] [G loss: 0.001735]\n",
      "480 [D loss: -0.000760] [G loss: 0.001167]\n",
      "481 [D loss: -0.000460] [G loss: 0.001070]\n",
      "482 [D loss: -0.001104] [G loss: 0.001211]\n",
      "483 [D loss: -0.001252] [G loss: 0.001972]\n",
      "484 [D loss: -0.001924] [G loss: 0.002258]\n",
      "485 [D loss: -0.001363] [G loss: 0.002637]\n",
      "486 [D loss: -0.001147] [G loss: 0.002274]\n",
      "487 [D loss: -0.001837] [G loss: 0.002503]\n",
      "488 [D loss: -0.001457] [G loss: 0.002382]\n",
      "489 [D loss: -0.001054] [G loss: 0.002599]\n",
      "490 [D loss: -0.001357] [G loss: 0.002404]\n",
      "491 [D loss: -0.000972] [G loss: 0.002756]\n",
      "492 [D loss: -0.001471] [G loss: 0.002687]\n",
      "493 [D loss: -0.001216] [G loss: 0.002694]\n",
      "494 [D loss: -0.001175] [G loss: 0.002707]\n",
      "495 [D loss: -0.001148] [G loss: 0.003220]\n",
      "496 [D loss: -0.000505] [G loss: 0.002600]\n",
      "497 [D loss: -0.001416] [G loss: 0.002714]\n",
      "498 [D loss: -0.001053] [G loss: 0.003032]\n",
      "499 [D loss: -0.001133] [G loss: 0.002928]\n",
      "500 [D loss: -0.000985] [G loss: 0.003286]\n",
      "501 [D loss: -0.001404] [G loss: 0.002554]\n",
      "502 [D loss: -0.000658] [G loss: 0.002714]\n",
      "503 [D loss: -0.001318] [G loss: 0.002462]\n",
      "504 [D loss: -0.000960] [G loss: 0.002342]\n",
      "505 [D loss: -0.001121] [G loss: 0.002768]\n",
      "506 [D loss: -0.001046] [G loss: 0.002496]\n",
      "507 [D loss: -0.001278] [G loss: 0.002550]\n",
      "508 [D loss: -0.001019] [G loss: 0.002610]\n",
      "509 [D loss: -0.001022] [G loss: 0.002665]\n",
      "510 [D loss: -0.000946] [G loss: 0.002253]\n",
      "511 [D loss: -0.000879] [G loss: 0.002426]\n",
      "512 [D loss: -0.000940] [G loss: 0.002370]\n",
      "513 [D loss: -0.001116] [G loss: 0.002235]\n",
      "514 [D loss: -0.000884] [G loss: 0.002365]\n",
      "515 [D loss: -0.001342] [G loss: 0.001975]\n",
      "516 [D loss: -0.001245] [G loss: 0.002099]\n",
      "517 [D loss: -0.001057] [G loss: 0.002122]\n",
      "518 [D loss: -0.000433] [G loss: 0.002357]\n",
      "519 [D loss: -0.001205] [G loss: 0.002441]\n",
      "520 [D loss: -0.000786] [G loss: 0.002581]\n",
      "521 [D loss: -0.001178] [G loss: 0.002530]\n",
      "522 [D loss: -0.001025] [G loss: 0.002381]\n",
      "523 [D loss: -0.000885] [G loss: 0.002438]\n",
      "524 [D loss: -0.000985] [G loss: 0.002613]\n",
      "525 [D loss: -0.000843] [G loss: 0.002739]\n",
      "526 [D loss: -0.001106] [G loss: 0.002963]\n",
      "527 [D loss: -0.001075] [G loss: 0.003368]\n",
      "528 [D loss: -0.001019] [G loss: 0.003280]\n",
      "529 [D loss: -0.001118] [G loss: 0.003349]\n",
      "530 [D loss: -0.000957] [G loss: 0.003326]\n",
      "531 [D loss: -0.000854] [G loss: 0.003034]\n",
      "532 [D loss: -0.001084] [G loss: 0.003337]\n",
      "533 [D loss: -0.001033] [G loss: 0.002981]\n",
      "534 [D loss: -0.001253] [G loss: 0.003083]\n",
      "535 [D loss: -0.001088] [G loss: 0.003019]\n",
      "536 [D loss: -0.000841] [G loss: 0.002990]\n",
      "537 [D loss: -0.001026] [G loss: 0.002788]\n",
      "538 [D loss: -0.000870] [G loss: 0.002559]\n",
      "539 [D loss: -0.000825] [G loss: 0.002631]\n",
      "540 [D loss: -0.000709] [G loss: 0.002277]\n",
      "541 [D loss: -0.001094] [G loss: 0.002360]\n",
      "542 [D loss: -0.001188] [G loss: 0.002517]\n",
      "543 [D loss: -0.001068] [G loss: 0.002298]\n",
      "544 [D loss: -0.001014] [G loss: 0.002437]\n",
      "545 [D loss: -0.001087] [G loss: 0.002654]\n",
      "546 [D loss: -0.001141] [G loss: 0.002696]\n",
      "547 [D loss: -0.000975] [G loss: 0.002720]\n",
      "548 [D loss: -0.001049] [G loss: 0.002951]\n",
      "549 [D loss: -0.001126] [G loss: 0.002673]\n",
      "550 [D loss: -0.001065] [G loss: 0.002624]\n",
      "551 [D loss: -0.000987] [G loss: 0.002693]\n",
      "552 [D loss: -0.001320] [G loss: 0.002974]\n",
      "553 [D loss: -0.000989] [G loss: 0.003105]\n",
      "554 [D loss: -0.000995] [G loss: 0.002540]\n",
      "555 [D loss: -0.000979] [G loss: 0.002803]\n",
      "556 [D loss: -0.000975] [G loss: 0.002966]\n",
      "557 [D loss: -0.000832] [G loss: 0.002955]\n",
      "558 [D loss: -0.001102] [G loss: 0.002884]\n",
      "559 [D loss: -0.000958] [G loss: 0.002859]\n",
      "560 [D loss: -0.000933] [G loss: 0.002750]\n",
      "561 [D loss: -0.000818] [G loss: 0.002932]\n",
      "562 [D loss: -0.001077] [G loss: 0.002973]\n",
      "563 [D loss: -0.000865] [G loss: 0.002826]\n",
      "564 [D loss: -0.000998] [G loss: 0.002872]\n",
      "565 [D loss: -0.001005] [G loss: 0.002832]\n",
      "566 [D loss: -0.001053] [G loss: 0.002754]\n",
      "567 [D loss: -0.001119] [G loss: 0.002724]\n",
      "568 [D loss: -0.001176] [G loss: 0.002622]\n",
      "569 [D loss: -0.000941] [G loss: 0.002397]\n",
      "570 [D loss: -0.001120] [G loss: 0.002582]\n",
      "571 [D loss: -0.001014] [G loss: 0.002534]\n",
      "572 [D loss: -0.000744] [G loss: 0.002553]\n",
      "573 [D loss: -0.001127] [G loss: 0.002284]\n",
      "574 [D loss: -0.001029] [G loss: 0.002434]\n",
      "575 [D loss: -0.000926] [G loss: 0.002562]\n",
      "576 [D loss: -0.000899] [G loss: 0.002494]\n",
      "577 [D loss: -0.000889] [G loss: 0.002577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578 [D loss: -0.001015] [G loss: 0.002612]\n",
      "579 [D loss: -0.001012] [G loss: 0.002681]\n",
      "580 [D loss: -0.000945] [G loss: 0.002665]\n",
      "581 [D loss: -0.000994] [G loss: 0.002564]\n",
      "582 [D loss: -0.001005] [G loss: 0.002901]\n",
      "583 [D loss: -0.000970] [G loss: 0.002984]\n",
      "584 [D loss: -0.001155] [G loss: 0.003007]\n",
      "585 [D loss: -0.000993] [G loss: 0.002978]\n",
      "586 [D loss: -0.000884] [G loss: 0.003024]\n",
      "587 [D loss: -0.000887] [G loss: 0.002776]\n",
      "588 [D loss: -0.000893] [G loss: 0.003001]\n",
      "589 [D loss: -0.001178] [G loss: 0.002752]\n",
      "590 [D loss: -0.000934] [G loss: 0.002701]\n",
      "591 [D loss: -0.001087] [G loss: 0.002535]\n",
      "592 [D loss: -0.001090] [G loss: 0.002423]\n",
      "593 [D loss: -0.001112] [G loss: 0.002443]\n",
      "594 [D loss: -0.001264] [G loss: 0.002362]\n",
      "595 [D loss: -0.001051] [G loss: 0.002330]\n",
      "596 [D loss: -0.000852] [G loss: 0.002218]\n",
      "597 [D loss: -0.001034] [G loss: 0.002423]\n",
      "598 [D loss: -0.000899] [G loss: 0.002276]\n",
      "599 [D loss: -0.001114] [G loss: 0.002203]\n",
      "600 [D loss: -0.001013] [G loss: 0.002406]\n",
      "601 [D loss: -0.001040] [G loss: 0.002440]\n",
      "602 [D loss: -0.001042] [G loss: 0.002572]\n",
      "603 [D loss: -0.001017] [G loss: 0.002551]\n",
      "604 [D loss: -0.001036] [G loss: 0.002663]\n",
      "605 [D loss: -0.001125] [G loss: 0.002744]\n",
      "606 [D loss: -0.000999] [G loss: 0.002506]\n",
      "607 [D loss: -0.000958] [G loss: 0.002654]\n",
      "608 [D loss: -0.001001] [G loss: 0.002566]\n",
      "609 [D loss: -0.000995] [G loss: 0.002420]\n",
      "610 [D loss: -0.001057] [G loss: 0.002530]\n",
      "611 [D loss: -0.001116] [G loss: 0.002458]\n",
      "612 [D loss: -0.000951] [G loss: 0.002338]\n",
      "613 [D loss: -0.001027] [G loss: 0.002234]\n",
      "614 [D loss: -0.001024] [G loss: 0.002368]\n",
      "615 [D loss: -0.001094] [G loss: 0.002378]\n",
      "616 [D loss: -0.001119] [G loss: 0.002455]\n",
      "617 [D loss: -0.001070] [G loss: 0.002547]\n",
      "618 [D loss: -0.001037] [G loss: 0.002525]\n",
      "619 [D loss: -0.001140] [G loss: 0.002487]\n",
      "620 [D loss: -0.001047] [G loss: 0.002519]\n",
      "621 [D loss: -0.001022] [G loss: 0.002427]\n",
      "622 [D loss: -0.001000] [G loss: 0.002409]\n",
      "623 [D loss: -0.001059] [G loss: 0.002460]\n",
      "624 [D loss: -0.001168] [G loss: 0.002377]\n",
      "625 [D loss: -0.001152] [G loss: 0.002423]\n",
      "626 [D loss: -0.001042] [G loss: 0.002385]\n",
      "627 [D loss: -0.001117] [G loss: 0.002389]\n",
      "628 [D loss: -0.000948] [G loss: 0.002408]\n",
      "629 [D loss: -0.001129] [G loss: 0.002351]\n",
      "630 [D loss: -0.001114] [G loss: 0.002413]\n",
      "631 [D loss: -0.000999] [G loss: 0.002494]\n",
      "632 [D loss: -0.001024] [G loss: 0.002531]\n",
      "633 [D loss: -0.000957] [G loss: 0.002610]\n",
      "634 [D loss: -0.001102] [G loss: 0.002557]\n",
      "635 [D loss: -0.001020] [G loss: 0.002552]\n",
      "636 [D loss: -0.001023] [G loss: 0.002367]\n",
      "637 [D loss: -0.001123] [G loss: 0.002532]\n",
      "638 [D loss: -0.000994] [G loss: 0.002392]\n",
      "639 [D loss: -0.000927] [G loss: 0.002351]\n",
      "640 [D loss: -0.000993] [G loss: 0.002276]\n",
      "641 [D loss: -0.000950] [G loss: 0.002342]\n",
      "642 [D loss: -0.000991] [G loss: 0.002460]\n",
      "643 [D loss: -0.001099] [G loss: 0.002362]\n",
      "644 [D loss: -0.000985] [G loss: 0.002343]\n",
      "645 [D loss: -0.001071] [G loss: 0.002363]\n",
      "646 [D loss: -0.001063] [G loss: 0.002314]\n",
      "647 [D loss: -0.001034] [G loss: 0.002385]\n",
      "648 [D loss: -0.001077] [G loss: 0.002360]\n",
      "649 [D loss: -0.001051] [G loss: 0.002397]\n",
      "650 [D loss: -0.000991] [G loss: 0.002430]\n",
      "651 [D loss: -0.001147] [G loss: 0.002444]\n",
      "652 [D loss: -0.001087] [G loss: 0.002428]\n",
      "653 [D loss: -0.001055] [G loss: 0.002394]\n",
      "654 [D loss: -0.000984] [G loss: 0.002344]\n",
      "655 [D loss: -0.001059] [G loss: 0.002434]\n",
      "656 [D loss: -0.001017] [G loss: 0.002425]\n",
      "657 [D loss: -0.001037] [G loss: 0.002286]\n",
      "658 [D loss: -0.001055] [G loss: 0.002388]\n",
      "659 [D loss: -0.001117] [G loss: 0.002253]\n",
      "660 [D loss: -0.001030] [G loss: 0.002284]\n",
      "661 [D loss: -0.001037] [G loss: 0.002145]\n",
      "662 [D loss: -0.001080] [G loss: 0.002368]\n",
      "663 [D loss: -0.001049] [G loss: 0.002344]\n",
      "664 [D loss: -0.001069] [G loss: 0.002228]\n",
      "665 [D loss: -0.001040] [G loss: 0.002281]\n",
      "666 [D loss: -0.001035] [G loss: 0.002314]\n",
      "667 [D loss: -0.001012] [G loss: 0.002293]\n",
      "668 [D loss: -0.000958] [G loss: 0.002304]\n",
      "669 [D loss: -0.000983] [G loss: 0.002278]\n",
      "670 [D loss: -0.001046] [G loss: 0.002315]\n",
      "671 [D loss: -0.001053] [G loss: 0.002248]\n",
      "672 [D loss: -0.001072] [G loss: 0.002198]\n",
      "673 [D loss: -0.001085] [G loss: 0.002178]\n",
      "674 [D loss: -0.001046] [G loss: 0.002199]\n",
      "675 [D loss: -0.001031] [G loss: 0.002201]\n",
      "676 [D loss: -0.000944] [G loss: 0.002201]\n",
      "677 [D loss: -0.000932] [G loss: 0.002161]\n",
      "678 [D loss: -0.001037] [G loss: 0.002208]\n",
      "679 [D loss: -0.001021] [G loss: 0.002138]\n",
      "680 [D loss: -0.001050] [G loss: 0.002168]\n",
      "681 [D loss: -0.001009] [G loss: 0.002170]\n",
      "682 [D loss: -0.001003] [G loss: 0.002203]\n",
      "683 [D loss: -0.001091] [G loss: 0.002145]\n",
      "684 [D loss: -0.001010] [G loss: 0.002106]\n",
      "685 [D loss: -0.001003] [G loss: 0.002082]\n",
      "686 [D loss: -0.001042] [G loss: 0.002115]\n",
      "687 [D loss: -0.001007] [G loss: 0.002094]\n",
      "688 [D loss: -0.001042] [G loss: 0.002074]\n",
      "689 [D loss: -0.000993] [G loss: 0.002041]\n",
      "690 [D loss: -0.001026] [G loss: 0.001974]\n",
      "691 [D loss: -0.001029] [G loss: 0.001884]\n",
      "692 [D loss: -0.000987] [G loss: 0.001968]\n",
      "693 [D loss: -0.001007] [G loss: 0.002030]\n",
      "694 [D loss: -0.001071] [G loss: 0.002083]\n",
      "695 [D loss: -0.000934] [G loss: 0.001952]\n",
      "696 [D loss: -0.000942] [G loss: 0.002064]\n",
      "697 [D loss: -0.001022] [G loss: 0.002169]\n",
      "698 [D loss: -0.001008] [G loss: 0.002098]\n",
      "699 [D loss: -0.001057] [G loss: 0.001978]\n",
      "700 [D loss: -0.001011] [G loss: 0.002048]\n",
      "701 [D loss: -0.000982] [G loss: 0.002013]\n",
      "702 [D loss: -0.001008] [G loss: 0.001921]\n",
      "703 [D loss: -0.001060] [G loss: 0.001919]\n",
      "704 [D loss: -0.001033] [G loss: 0.001861]\n",
      "705 [D loss: -0.000990] [G loss: 0.001925]\n",
      "706 [D loss: -0.001001] [G loss: 0.001815]\n",
      "707 [D loss: -0.001022] [G loss: 0.001841]\n",
      "708 [D loss: -0.000959] [G loss: 0.001849]\n",
      "709 [D loss: -0.000995] [G loss: 0.001936]\n",
      "710 [D loss: -0.001101] [G loss: 0.001949]\n",
      "711 [D loss: -0.000965] [G loss: 0.001964]\n",
      "712 [D loss: -0.001001] [G loss: 0.001856]\n",
      "713 [D loss: -0.001022] [G loss: 0.001906]\n",
      "714 [D loss: -0.000979] [G loss: 0.001793]\n",
      "715 [D loss: -0.001015] [G loss: 0.001818]\n",
      "716 [D loss: -0.000933] [G loss: 0.001731]\n",
      "717 [D loss: -0.000962] [G loss: 0.001667]\n",
      "718 [D loss: -0.000974] [G loss: 0.001738]\n",
      "719 [D loss: -0.000980] [G loss: 0.001743]\n",
      "720 [D loss: -0.001013] [G loss: 0.001697]\n",
      "721 [D loss: -0.001013] [G loss: 0.001722]\n",
      "722 [D loss: -0.000955] [G loss: 0.001786]\n",
      "723 [D loss: -0.001011] [G loss: 0.001826]\n",
      "724 [D loss: -0.000939] [G loss: 0.001750]\n",
      "725 [D loss: -0.001035] [G loss: 0.001847]\n",
      "726 [D loss: -0.001025] [G loss: 0.001752]\n",
      "727 [D loss: -0.001038] [G loss: 0.001722]\n",
      "728 [D loss: -0.000940] [G loss: 0.001738]\n",
      "729 [D loss: -0.001028] [G loss: 0.001739]\n",
      "730 [D loss: -0.000947] [G loss: 0.001678]\n",
      "731 [D loss: -0.001005] [G loss: 0.001797]\n",
      "732 [D loss: -0.001015] [G loss: 0.001694]\n",
      "733 [D loss: -0.001080] [G loss: 0.001636]\n",
      "734 [D loss: -0.000997] [G loss: 0.001567]\n",
      "735 [D loss: -0.001035] [G loss: 0.001683]\n",
      "736 [D loss: -0.000988] [G loss: 0.001589]\n",
      "737 [D loss: -0.000987] [G loss: 0.001661]\n",
      "738 [D loss: -0.001000] [G loss: 0.001625]\n",
      "739 [D loss: -0.001054] [G loss: 0.001548]\n",
      "740 [D loss: -0.000988] [G loss: 0.001448]\n",
      "741 [D loss: -0.001021] [G loss: 0.001459]\n",
      "742 [D loss: -0.001114] [G loss: 0.001520]\n",
      "743 [D loss: -0.001020] [G loss: 0.001404]\n",
      "744 [D loss: -0.001025] [G loss: 0.001431]\n",
      "745 [D loss: -0.000975] [G loss: 0.001543]\n",
      "746 [D loss: -0.001027] [G loss: 0.001396]\n",
      "747 [D loss: -0.001035] [G loss: 0.001417]\n",
      "748 [D loss: -0.001022] [G loss: 0.001448]\n",
      "749 [D loss: -0.000960] [G loss: 0.001506]\n",
      "750 [D loss: -0.001011] [G loss: 0.001477]\n",
      "751 [D loss: -0.001031] [G loss: 0.001365]\n",
      "752 [D loss: -0.001009] [G loss: 0.001417]\n",
      "753 [D loss: -0.001018] [G loss: 0.001400]\n",
      "754 [D loss: -0.001066] [G loss: 0.001514]\n",
      "755 [D loss: -0.001032] [G loss: 0.001431]\n",
      "756 [D loss: -0.001044] [G loss: 0.001363]\n",
      "757 [D loss: -0.001073] [G loss: 0.001329]\n",
      "758 [D loss: -0.000958] [G loss: 0.001272]\n",
      "759 [D loss: -0.001104] [G loss: 0.001359]\n",
      "760 [D loss: -0.001048] [G loss: 0.001371]\n",
      "761 [D loss: -0.001065] [G loss: 0.001373]\n",
      "762 [D loss: -0.001006] [G loss: 0.001438]\n",
      "763 [D loss: -0.001020] [G loss: 0.001321]\n",
      "764 [D loss: -0.000996] [G loss: 0.001274]\n",
      "765 [D loss: -0.000997] [G loss: 0.001362]\n",
      "766 [D loss: -0.001036] [G loss: 0.001339]\n",
      "767 [D loss: -0.001049] [G loss: 0.001253]\n",
      "768 [D loss: -0.001050] [G loss: 0.001265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769 [D loss: -0.001003] [G loss: 0.001252]\n",
      "770 [D loss: -0.001026] [G loss: 0.001305]\n",
      "771 [D loss: -0.001023] [G loss: 0.001209]\n",
      "772 [D loss: -0.001015] [G loss: 0.001203]\n",
      "773 [D loss: -0.000980] [G loss: 0.001159]\n",
      "774 [D loss: -0.000989] [G loss: 0.001226]\n",
      "775 [D loss: -0.000999] [G loss: 0.001130]\n",
      "776 [D loss: -0.001002] [G loss: 0.001151]\n",
      "777 [D loss: -0.001051] [G loss: 0.001256]\n",
      "778 [D loss: -0.001086] [G loss: 0.001204]\n",
      "779 [D loss: -0.001035] [G loss: 0.001192]\n",
      "780 [D loss: -0.001045] [G loss: 0.001256]\n",
      "781 [D loss: -0.000980] [G loss: 0.001276]\n",
      "782 [D loss: -0.001049] [G loss: 0.001183]\n",
      "783 [D loss: -0.001097] [G loss: 0.001202]\n",
      "784 [D loss: -0.001047] [G loss: 0.001184]\n",
      "785 [D loss: -0.001008] [G loss: 0.001088]\n",
      "786 [D loss: -0.001047] [G loss: 0.001088]\n",
      "787 [D loss: -0.001012] [G loss: 0.000944]\n",
      "788 [D loss: -0.000944] [G loss: 0.000948]\n",
      "789 [D loss: -0.001032] [G loss: 0.000961]\n",
      "790 [D loss: -0.001031] [G loss: 0.001028]\n",
      "791 [D loss: -0.001043] [G loss: 0.000974]\n",
      "792 [D loss: -0.001044] [G loss: 0.001010]\n",
      "793 [D loss: -0.001019] [G loss: 0.001066]\n",
      "794 [D loss: -0.001021] [G loss: 0.001039]\n",
      "795 [D loss: -0.000998] [G loss: 0.001068]\n",
      "796 [D loss: -0.001053] [G loss: 0.001011]\n",
      "797 [D loss: -0.001009] [G loss: 0.000974]\n",
      "798 [D loss: -0.000962] [G loss: 0.000939]\n",
      "799 [D loss: -0.001077] [G loss: 0.001042]\n",
      "800 [D loss: -0.001074] [G loss: 0.000939]\n",
      "801 [D loss: -0.001045] [G loss: 0.001011]\n",
      "802 [D loss: -0.000990] [G loss: 0.000929]\n",
      "803 [D loss: -0.000991] [G loss: 0.000976]\n",
      "804 [D loss: -0.001023] [G loss: 0.000950]\n",
      "805 [D loss: -0.001003] [G loss: 0.000862]\n",
      "806 [D loss: -0.000996] [G loss: 0.000920]\n",
      "807 [D loss: -0.001043] [G loss: 0.000978]\n",
      "808 [D loss: -0.001039] [G loss: 0.000950]\n",
      "809 [D loss: -0.000973] [G loss: 0.000855]\n",
      "810 [D loss: -0.001048] [G loss: 0.000819]\n",
      "811 [D loss: -0.001007] [G loss: 0.000855]\n",
      "812 [D loss: -0.001075] [G loss: 0.000801]\n",
      "813 [D loss: -0.001087] [G loss: 0.000871]\n",
      "814 [D loss: -0.001021] [G loss: 0.000827]\n",
      "815 [D loss: -0.001085] [G loss: 0.000769]\n",
      "816 [D loss: -0.001088] [G loss: 0.000839]\n",
      "817 [D loss: -0.000973] [G loss: 0.000787]\n",
      "818 [D loss: -0.001014] [G loss: 0.000744]\n",
      "819 [D loss: -0.000983] [G loss: 0.000728]\n",
      "820 [D loss: -0.001014] [G loss: 0.000729]\n",
      "821 [D loss: -0.001018] [G loss: 0.000768]\n",
      "822 [D loss: -0.000980] [G loss: 0.000770]\n",
      "823 [D loss: -0.001044] [G loss: 0.000722]\n",
      "824 [D loss: -0.000939] [G loss: 0.000664]\n",
      "825 [D loss: -0.000956] [G loss: 0.000752]\n",
      "826 [D loss: -0.001030] [G loss: 0.000663]\n",
      "827 [D loss: -0.001016] [G loss: 0.000603]\n",
      "828 [D loss: -0.000999] [G loss: 0.000740]\n",
      "829 [D loss: -0.000979] [G loss: 0.000695]\n",
      "830 [D loss: -0.001027] [G loss: 0.000638]\n",
      "831 [D loss: -0.001008] [G loss: 0.000670]\n",
      "832 [D loss: -0.000998] [G loss: 0.000560]\n",
      "833 [D loss: -0.001019] [G loss: 0.000656]\n",
      "834 [D loss: -0.000979] [G loss: 0.000634]\n",
      "835 [D loss: -0.000973] [G loss: 0.000706]\n",
      "836 [D loss: -0.000981] [G loss: 0.000628]\n",
      "837 [D loss: -0.000985] [G loss: 0.000526]\n",
      "838 [D loss: -0.001032] [G loss: 0.000626]\n",
      "839 [D loss: -0.000972] [G loss: 0.000657]\n",
      "840 [D loss: -0.000982] [G loss: 0.000553]\n",
      "841 [D loss: -0.001059] [G loss: 0.000506]\n",
      "842 [D loss: -0.001028] [G loss: 0.000435]\n",
      "843 [D loss: -0.001104] [G loss: 0.000471]\n",
      "844 [D loss: -0.000952] [G loss: 0.000516]\n",
      "845 [D loss: -0.000991] [G loss: 0.000529]\n",
      "846 [D loss: -0.001016] [G loss: 0.000531]\n",
      "847 [D loss: -0.001015] [G loss: 0.000476]\n",
      "848 [D loss: -0.000989] [G loss: 0.000446]\n",
      "849 [D loss: -0.001035] [G loss: 0.000424]\n",
      "850 [D loss: -0.000964] [G loss: 0.000345]\n",
      "851 [D loss: -0.001014] [G loss: 0.000399]\n",
      "852 [D loss: -0.000942] [G loss: 0.000514]\n",
      "853 [D loss: -0.001022] [G loss: 0.000417]\n",
      "854 [D loss: -0.000995] [G loss: 0.000456]\n",
      "855 [D loss: -0.001016] [G loss: 0.000447]\n",
      "856 [D loss: -0.001044] [G loss: 0.000550]\n",
      "857 [D loss: -0.000984] [G loss: 0.000388]\n",
      "858 [D loss: -0.001020] [G loss: 0.000442]\n",
      "859 [D loss: -0.000970] [G loss: 0.000341]\n",
      "860 [D loss: -0.001014] [G loss: 0.000480]\n",
      "861 [D loss: -0.001070] [G loss: 0.000421]\n",
      "862 [D loss: -0.001030] [G loss: 0.000369]\n",
      "863 [D loss: -0.001026] [G loss: 0.000283]\n",
      "864 [D loss: -0.000963] [G loss: 0.000315]\n",
      "865 [D loss: -0.001051] [G loss: 0.000331]\n",
      "866 [D loss: -0.001032] [G loss: 0.000312]\n",
      "867 [D loss: -0.001008] [G loss: 0.000313]\n",
      "868 [D loss: -0.000972] [G loss: 0.000334]\n",
      "869 [D loss: -0.001026] [G loss: 0.000344]\n",
      "870 [D loss: -0.000974] [G loss: 0.000274]\n",
      "871 [D loss: -0.001008] [G loss: 0.000333]\n",
      "872 [D loss: -0.001037] [G loss: 0.000284]\n",
      "873 [D loss: -0.001036] [G loss: 0.000347]\n",
      "874 [D loss: -0.001034] [G loss: 0.000255]\n",
      "875 [D loss: -0.001013] [G loss: 0.000291]\n",
      "876 [D loss: -0.001043] [G loss: 0.000240]\n",
      "877 [D loss: -0.001020] [G loss: 0.000160]\n",
      "878 [D loss: -0.000990] [G loss: 0.000254]\n",
      "879 [D loss: -0.000983] [G loss: 0.000215]\n",
      "880 [D loss: -0.001021] [G loss: 0.000123]\n",
      "881 [D loss: -0.001011] [G loss: 0.000080]\n",
      "882 [D loss: -0.001015] [G loss: 0.000072]\n",
      "883 [D loss: -0.001044] [G loss: 0.000092]\n",
      "884 [D loss: -0.000990] [G loss: 0.000191]\n",
      "885 [D loss: -0.000998] [G loss: 0.000218]\n",
      "886 [D loss: -0.001052] [G loss: 0.000149]\n",
      "887 [D loss: -0.001024] [G loss: 0.000193]\n",
      "888 [D loss: -0.000955] [G loss: 0.000178]\n",
      "889 [D loss: -0.000985] [G loss: 0.000184]\n",
      "890 [D loss: -0.000988] [G loss: 0.000236]\n",
      "891 [D loss: -0.001017] [G loss: 0.000130]\n",
      "892 [D loss: -0.000960] [G loss: 0.000144]\n",
      "893 [D loss: -0.000969] [G loss: 0.000062]\n",
      "894 [D loss: -0.000998] [G loss: 0.000004]\n",
      "895 [D loss: -0.001077] [G loss: 0.000029]\n",
      "896 [D loss: -0.000960] [G loss: -0.000024]\n",
      "897 [D loss: -0.000976] [G loss: 0.000015]\n",
      "898 [D loss: -0.001055] [G loss: 0.000139]\n",
      "899 [D loss: -0.000939] [G loss: 0.000196]\n",
      "900 [D loss: -0.001008] [G loss: -0.000000]\n",
      "901 [D loss: -0.000970] [G loss: -0.000010]\n",
      "902 [D loss: -0.001017] [G loss: 0.000008]\n",
      "903 [D loss: -0.001023] [G loss: 0.000017]\n",
      "904 [D loss: -0.001116] [G loss: 0.000006]\n",
      "905 [D loss: -0.000957] [G loss: -0.000025]\n",
      "906 [D loss: -0.001101] [G loss: 0.000082]\n",
      "907 [D loss: -0.001042] [G loss: 0.000037]\n",
      "908 [D loss: -0.000951] [G loss: 0.000058]\n",
      "909 [D loss: -0.000966] [G loss: -0.000040]\n",
      "910 [D loss: -0.000957] [G loss: -0.000035]\n",
      "911 [D loss: -0.001017] [G loss: -0.000012]\n",
      "912 [D loss: -0.001026] [G loss: -0.000158]\n",
      "913 [D loss: -0.001024] [G loss: -0.000162]\n",
      "914 [D loss: -0.000952] [G loss: -0.000139]\n",
      "915 [D loss: -0.000990] [G loss: -0.000093]\n",
      "916 [D loss: -0.001005] [G loss: -0.000062]\n",
      "917 [D loss: -0.001045] [G loss: -0.000005]\n",
      "918 [D loss: -0.001003] [G loss: -0.000072]\n",
      "919 [D loss: -0.001023] [G loss: -0.000056]\n",
      "920 [D loss: -0.001033] [G loss: 0.000006]\n",
      "921 [D loss: -0.001003] [G loss: -0.000064]\n",
      "922 [D loss: -0.001032] [G loss: -0.000076]\n",
      "923 [D loss: -0.000984] [G loss: -0.000151]\n",
      "924 [D loss: -0.000974] [G loss: -0.000116]\n",
      "925 [D loss: -0.001001] [G loss: -0.000050]\n",
      "926 [D loss: -0.000939] [G loss: -0.000055]\n",
      "927 [D loss: -0.001001] [G loss: -0.000149]\n",
      "928 [D loss: -0.000965] [G loss: -0.000226]\n",
      "929 [D loss: -0.000962] [G loss: -0.000065]\n",
      "930 [D loss: -0.000966] [G loss: -0.000248]\n",
      "931 [D loss: -0.000981] [G loss: -0.000178]\n",
      "932 [D loss: -0.000944] [G loss: -0.000268]\n",
      "933 [D loss: -0.001028] [G loss: -0.000337]\n",
      "934 [D loss: -0.001006] [G loss: -0.000450]\n",
      "935 [D loss: -0.001019] [G loss: -0.000257]\n",
      "936 [D loss: -0.001001] [G loss: -0.000271]\n",
      "937 [D loss: -0.001013] [G loss: -0.000273]\n",
      "938 [D loss: -0.000984] [G loss: -0.000314]\n",
      "939 [D loss: -0.000923] [G loss: -0.000271]\n",
      "940 [D loss: -0.000984] [G loss: -0.000290]\n",
      "941 [D loss: -0.000971] [G loss: -0.000342]\n",
      "942 [D loss: -0.000978] [G loss: -0.000313]\n",
      "943 [D loss: -0.000946] [G loss: -0.000323]\n",
      "944 [D loss: -0.001041] [G loss: -0.000274]\n",
      "945 [D loss: -0.001108] [G loss: -0.000368]\n",
      "946 [D loss: -0.001060] [G loss: -0.000359]\n",
      "947 [D loss: -0.000988] [G loss: -0.000268]\n",
      "948 [D loss: -0.001061] [G loss: -0.000350]\n",
      "949 [D loss: -0.000939] [G loss: -0.000356]\n",
      "950 [D loss: -0.000974] [G loss: -0.000447]\n",
      "951 [D loss: -0.001028] [G loss: -0.000353]\n",
      "952 [D loss: -0.000981] [G loss: -0.000286]\n",
      "953 [D loss: -0.000970] [G loss: -0.000351]\n",
      "954 [D loss: -0.000978] [G loss: -0.000511]\n",
      "955 [D loss: -0.000983] [G loss: -0.000427]\n",
      "956 [D loss: -0.000971] [G loss: -0.000479]\n",
      "957 [D loss: -0.000959] [G loss: -0.000448]\n",
      "958 [D loss: -0.001084] [G loss: -0.000411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "959 [D loss: -0.000990] [G loss: -0.000309]\n",
      "960 [D loss: -0.001009] [G loss: -0.000440]\n",
      "961 [D loss: -0.001002] [G loss: -0.000401]\n",
      "962 [D loss: -0.000908] [G loss: -0.000380]\n",
      "963 [D loss: -0.001066] [G loss: -0.000316]\n",
      "964 [D loss: -0.001053] [G loss: -0.000359]\n",
      "965 [D loss: -0.000968] [G loss: -0.000465]\n",
      "966 [D loss: -0.001006] [G loss: -0.000513]\n",
      "967 [D loss: -0.001002] [G loss: -0.000550]\n",
      "968 [D loss: -0.001056] [G loss: -0.000478]\n",
      "969 [D loss: -0.001017] [G loss: -0.000607]\n",
      "970 [D loss: -0.001036] [G loss: -0.000556]\n",
      "971 [D loss: -0.001002] [G loss: -0.000574]\n",
      "972 [D loss: -0.001077] [G loss: -0.000554]\n",
      "973 [D loss: -0.000999] [G loss: -0.000523]\n",
      "974 [D loss: -0.000964] [G loss: -0.000499]\n",
      "975 [D loss: -0.000988] [G loss: -0.000422]\n",
      "976 [D loss: -0.001046] [G loss: -0.000458]\n",
      "977 [D loss: -0.001040] [G loss: -0.000431]\n",
      "978 [D loss: -0.000984] [G loss: -0.000397]\n",
      "979 [D loss: -0.001001] [G loss: -0.000546]\n",
      "980 [D loss: -0.001012] [G loss: -0.000693]\n",
      "981 [D loss: -0.000992] [G loss: -0.000567]\n",
      "982 [D loss: -0.001013] [G loss: -0.000636]\n",
      "983 [D loss: -0.000927] [G loss: -0.000676]\n",
      "984 [D loss: -0.001054] [G loss: -0.000731]\n",
      "985 [D loss: -0.000985] [G loss: -0.000559]\n",
      "986 [D loss: -0.000980] [G loss: -0.000496]\n",
      "987 [D loss: -0.001021] [G loss: -0.000696]\n",
      "988 [D loss: -0.000982] [G loss: -0.000586]\n",
      "989 [D loss: -0.000989] [G loss: -0.000564]\n",
      "990 [D loss: -0.000940] [G loss: -0.000505]\n",
      "991 [D loss: -0.001028] [G loss: -0.000527]\n",
      "992 [D loss: -0.001006] [G loss: -0.000575]\n",
      "993 [D loss: -0.001045] [G loss: -0.000630]\n",
      "994 [D loss: -0.000979] [G loss: -0.000643]\n",
      "995 [D loss: -0.001018] [G loss: -0.000719]\n",
      "996 [D loss: -0.000985] [G loss: -0.000680]\n",
      "997 [D loss: -0.000997] [G loss: -0.000696]\n",
      "998 [D loss: -0.000941] [G loss: -0.000664]\n",
      "999 [D loss: -0.001026] [G loss: -0.000597]\n",
      "1000 [D loss: -0.001010] [G loss: -0.000598]\n",
      "1001 [D loss: -0.000885] [G loss: -0.000527]\n",
      "1002 [D loss: -0.001007] [G loss: -0.000519]\n",
      "1003 [D loss: -0.000998] [G loss: -0.000518]\n",
      "1004 [D loss: -0.000979] [G loss: -0.000587]\n",
      "1005 [D loss: -0.001073] [G loss: -0.000565]\n",
      "1006 [D loss: -0.000921] [G loss: -0.000602]\n",
      "1007 [D loss: -0.000989] [G loss: -0.000629]\n",
      "1008 [D loss: -0.001002] [G loss: -0.000642]\n",
      "1009 [D loss: -0.000971] [G loss: -0.000629]\n",
      "1010 [D loss: -0.000931] [G loss: -0.000663]\n",
      "1011 [D loss: -0.000974] [G loss: -0.000533]\n",
      "1012 [D loss: -0.001014] [G loss: -0.000584]\n",
      "1013 [D loss: -0.001033] [G loss: -0.000669]\n",
      "1014 [D loss: -0.000970] [G loss: -0.000579]\n",
      "1015 [D loss: -0.001005] [G loss: -0.000624]\n",
      "1016 [D loss: -0.000986] [G loss: -0.000619]\n",
      "1017 [D loss: -0.000951] [G loss: -0.000634]\n",
      "1018 [D loss: -0.000970] [G loss: -0.000632]\n",
      "1019 [D loss: -0.001130] [G loss: -0.000680]\n",
      "1020 [D loss: -0.001017] [G loss: -0.000683]\n",
      "1021 [D loss: -0.001064] [G loss: -0.000699]\n",
      "1022 [D loss: -0.000986] [G loss: -0.000859]\n",
      "1023 [D loss: -0.000882] [G loss: -0.000828]\n",
      "1024 [D loss: -0.000983] [G loss: -0.000789]\n",
      "1025 [D loss: -0.000959] [G loss: -0.000912]\n",
      "1026 [D loss: -0.000928] [G loss: -0.000872]\n",
      "1027 [D loss: -0.001002] [G loss: -0.000643]\n",
      "1028 [D loss: -0.001005] [G loss: -0.000692]\n",
      "1029 [D loss: -0.001044] [G loss: -0.000692]\n",
      "1030 [D loss: -0.000963] [G loss: -0.000787]\n",
      "1031 [D loss: -0.001001] [G loss: -0.000709]\n",
      "1032 [D loss: -0.001058] [G loss: -0.000762]\n",
      "1033 [D loss: -0.001043] [G loss: -0.000716]\n",
      "1034 [D loss: -0.000999] [G loss: -0.000796]\n",
      "1035 [D loss: -0.001011] [G loss: -0.000777]\n",
      "1036 [D loss: -0.000993] [G loss: -0.000769]\n",
      "1037 [D loss: -0.001016] [G loss: -0.000832]\n",
      "1038 [D loss: -0.001022] [G loss: -0.000809]\n",
      "1039 [D loss: -0.000952] [G loss: -0.000820]\n",
      "1040 [D loss: -0.000974] [G loss: -0.000843]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8d924792a633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mprint_every_n_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPRINT_EVERY_N_BATCHES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mINITIAL_EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;34m,\u001b[0m\u001b[0mdiscriminator_training_loops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/Git/Personal/GDL/generative_deep_learning_code/models/GAN_IMP.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, batch_size, epochs, run_folder, print_every_n_batches, initial_epoch, lr_decay, discriminator_training_loops, clip_threshold)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;31m# If at save interval => save generated image samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every_n_batches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/Personal/GDL/generative_deep_learning_code/models/GAN_IMP.py\u001b[0m in \u001b[0;36msample_images\u001b[0;34m(self, epoch, run_folder)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgen_imgs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[1;32m   1217\u001b[0m     axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,\n\u001b[1;32m   1218\u001b[0m                        \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_kw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                        gridspec_kw=gridspec_kw)\n\u001b[0m\u001b[1;32m   1220\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(self, nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw)\u001b[0m\n\u001b[1;32m   1530\u001b[0m                 \u001b[0msubplot_kw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sharex\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_with\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msharex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m                 \u001b[0msubplot_kw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sharey\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_with\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msharey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m                 \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0;31m# turn off redundant tick labeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# _axes_class is set in the subplot_class_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axes_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;31m# add a layout box to this, for both the full axis, and the poss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# of the axis.  We need both because the axes may become smaller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterization_zorder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# a dict from events to (id, func)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# funcs used to format x and y - fall back on major formatters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mcla\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mset_clip_path\u001b[0;34m(self, clippath, transform)\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArtist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclippath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminorTicks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclippath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mset_clip_path\u001b[0;34m(self, clippath, transform)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclippath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArtist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclippath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclippath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mset_clip_path\u001b[0;34m(self, path, transform)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# This may result in the callbacks being hit twice, but guarantees they\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;31m# will be hit at least once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpchanged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mpchanged\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mpchanged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \"\"\"\n\u001b[1;32m    312\u001b[0m         \u001b[0mFire\u001b[0m \u001b[0man\u001b[0m \u001b[0mevent\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mproperty\u001b[0m \u001b[0mchanged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalling\u001b[0m \u001b[0mall\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 17 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gan.train(     \n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = 40000\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    "    ,discriminator_training_loops = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(gan.discriminator.predict(np.array([x_train[i]]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, 100)\n",
    "img = gan.generator.predict(np.array([noise]))[0]\n",
    "\n",
    "print(img.shape)\n",
    "plt.imshow(img[:,:,0])\n",
    "\n",
    "gan.discriminator.predict(np.array([img]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gan.discriminator.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "for x, y in enumerate(gan.discriminator.layers):\n",
    "    \n",
    "    print(y)\n",
    "    print(y.trainable)\n",
    "    for i in gan.discriminator.layers[x].get_weights():\n",
    "        \n",
    "        print(pointer)\n",
    "        print(i.shape)\n",
    "        pointer+=1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gan.discriminator.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.get_weights()[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.model.save_weights('./run/0006/weights/weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
