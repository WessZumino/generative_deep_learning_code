{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.GAN_ELE import GAN\n",
    "from utils.loaders import load_safari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = '0011'\n",
    "RUN_FOLDER = os.path.join(\"./run\", RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = load_safari('camel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12622f080>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0,:,:,0], cmap = 'gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = (28,28,1)\n",
    "\n",
    "CONV_FILTERS = [64,64,128,128]\n",
    "CONV_KERNEL_SIZES = [5,5,5,5]\n",
    "CONV_STRIDES = [2,2,2,1]\n",
    "CONV_PADDINGS = ['same', 'same', 'same', 'same']\n",
    "\n",
    "CONV_T_FILTERS = [128,64, 64,1]\n",
    "CONV_T_KERNEL_SIZES = [5,5,5,5]\n",
    "CONV_T_STRIDES = [1,1,1,1]\n",
    "CONV_T_PADDINGS = ['same','same','same','same']\n",
    "\n",
    "Z_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(INPUT_DIM\n",
    "                , CONV_FILTERS\n",
    "                , CONV_KERNEL_SIZES\n",
    "                , CONV_STRIDES\n",
    "                , CONV_PADDINGS\n",
    "                , CONV_T_FILTERS\n",
    "                , CONV_T_KERNEL_SIZES\n",
    "                , CONV_T_STRIDES\n",
    "                , CONV_T_PADDINGS\n",
    "                , Z_DIM\n",
    "                 )\n",
    "\n",
    "gan.save(RUN_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 1,441,666\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 720,833\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfoster/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_0 (Conv2DTr (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_1 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_2 (Conv2DTr (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_3 (Conv2DTr (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 843,905\n",
      "Trainable params: 837,249\n",
      "Non-trainable params: 6,656\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 6000\n",
    "PRINT_EVERY_N_BATCHES = 10\n",
    "INITIAL_EPOCH = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.696314] [G loss: 1.513349]\n",
      "1 [D loss: 0.796233] [G loss: 0.669560]\n",
      "2 [D loss: 0.668781] [G loss: 0.522640]\n",
      "3 [D loss: 0.739252] [G loss: 0.702309]\n",
      "4 [D loss: 0.640991] [G loss: 0.531736]\n",
      "5 [D loss: 0.587770] [G loss: 0.746495]\n",
      "6 [D loss: 0.494699] [G loss: 0.206667]\n",
      "7 [D loss: 0.977221] [G loss: 0.922132]\n",
      "8 [D loss: 0.603177] [G loss: 0.977096]\n",
      "9 [D loss: 0.495747] [G loss: 0.892973]\n",
      "10 [D loss: 0.532542] [G loss: 0.831562]\n",
      "11 [D loss: 0.401092] [G loss: 0.844649]\n",
      "12 [D loss: 0.350743] [G loss: 1.099344]\n",
      "13 [D loss: 0.343176] [G loss: 0.647866]\n",
      "14 [D loss: 1.105895] [G loss: 1.119429]\n",
      "15 [D loss: 0.532919] [G loss: 0.987632]\n",
      "16 [D loss: 0.512992] [G loss: 1.219846]\n",
      "17 [D loss: 0.442277] [G loss: 1.388048]\n",
      "18 [D loss: 0.412155] [G loss: 5.202266]\n",
      "19 [D loss: 2.683417] [G loss: 0.868209]\n",
      "20 [D loss: 0.506754] [G loss: 0.658244]\n",
      "21 [D loss: 0.653667] [G loss: 0.793266]\n",
      "22 [D loss: 0.591293] [G loss: 0.914239]\n",
      "23 [D loss: 0.557957] [G loss: 1.077663]\n",
      "24 [D loss: 0.521903] [G loss: 1.112738]\n",
      "25 [D loss: 0.478359] [G loss: 1.179044]\n",
      "26 [D loss: 0.467927] [G loss: 1.009848]\n",
      "27 [D loss: 0.447154] [G loss: 1.953632]\n",
      "28 [D loss: 0.737343] [G loss: 0.173058]\n",
      "29 [D loss: 1.124059] [G loss: 1.164057]\n",
      "30 [D loss: 0.510893] [G loss: 0.987134]\n",
      "31 [D loss: 0.469118] [G loss: 1.074061]\n",
      "32 [D loss: 0.493177] [G loss: 0.984742]\n",
      "33 [D loss: 0.484617] [G loss: 1.460141]\n",
      "34 [D loss: 0.492115] [G loss: 0.311765]\n",
      "35 [D loss: 0.827651] [G loss: 2.038247]\n",
      "36 [D loss: 0.739937] [G loss: 0.530425]\n",
      "37 [D loss: 0.615972] [G loss: 0.700443]\n",
      "38 [D loss: 0.564900] [G loss: 0.880311]\n",
      "39 [D loss: 0.520037] [G loss: 1.199701]\n",
      "40 [D loss: 0.480265] [G loss: 1.095119]\n",
      "41 [D loss: 0.461464] [G loss: 1.833939]\n",
      "42 [D loss: 0.517631] [G loss: 0.145016]\n",
      "43 [D loss: 1.309805] [G loss: 1.468282]\n",
      "44 [D loss: 0.613950] [G loss: 0.769502]\n",
      "45 [D loss: 0.563010] [G loss: 0.856155]\n",
      "46 [D loss: 0.539362] [G loss: 1.097875]\n",
      "47 [D loss: 0.510150] [G loss: 1.287967]\n",
      "48 [D loss: 0.491657] [G loss: 1.069078]\n",
      "49 [D loss: 0.465034] [G loss: 2.023993]\n",
      "50 [D loss: 0.677241] [G loss: 0.281632]\n",
      "51 [D loss: 1.031338] [G loss: 0.723825]\n",
      "52 [D loss: 0.615241] [G loss: 0.736441]\n",
      "53 [D loss: 0.608668] [G loss: 0.785066]\n",
      "54 [D loss: 0.589447] [G loss: 0.916839]\n",
      "55 [D loss: 0.552924] [G loss: 1.267228]\n",
      "56 [D loss: 0.504952] [G loss: 0.999846]\n",
      "57 [D loss: 0.500607] [G loss: 1.855501]\n",
      "58 [D loss: 0.539536] [G loss: 0.371506]\n",
      "59 [D loss: 0.701731] [G loss: 0.712296]\n",
      "60 [D loss: 0.534424] [G loss: 1.153443]\n",
      "61 [D loss: 0.453209] [G loss: 1.601703]\n",
      "62 [D loss: 0.449110] [G loss: 0.462071]\n",
      "63 [D loss: 0.660524] [G loss: 1.674538]\n",
      "64 [D loss: 0.431318] [G loss: 0.824981]\n",
      "65 [D loss: 0.491922] [G loss: 3.622437]\n",
      "66 [D loss: 1.034926] [G loss: 0.319706]\n",
      "67 [D loss: 0.799503] [G loss: 0.829180]\n",
      "68 [D loss: 0.561260] [G loss: 0.933002]\n",
      "69 [D loss: 0.503821] [G loss: 1.257743]\n",
      "70 [D loss: 0.475016] [G loss: 1.269744]\n",
      "71 [D loss: 0.436228] [G loss: 1.322200]\n",
      "72 [D loss: 0.404811] [G loss: 1.363696]\n",
      "73 [D loss: 0.361124] [G loss: 1.933524]\n",
      "74 [D loss: 0.440362] [G loss: 0.189392]\n",
      "75 [D loss: 1.064196] [G loss: 1.785702]\n",
      "76 [D loss: 0.546006] [G loss: 0.864020]\n",
      "77 [D loss: 0.535658] [G loss: 1.241579]\n",
      "78 [D loss: 0.452203] [G loss: 1.819183]\n",
      "79 [D loss: 0.453089] [G loss: 0.785822]\n",
      "80 [D loss: 0.491653] [G loss: 2.479395]\n",
      "81 [D loss: 0.518791] [G loss: 0.380621]\n",
      "82 [D loss: 0.744574] [G loss: 1.117895]\n",
      "83 [D loss: 0.474867] [G loss: 1.622161]\n",
      "84 [D loss: 0.422534] [G loss: 1.442882]\n",
      "85 [D loss: 0.383226] [G loss: 2.341228]\n",
      "86 [D loss: 0.428642] [G loss: 0.473343]\n",
      "87 [D loss: 0.793588] [G loss: 1.304357]\n",
      "88 [D loss: 0.457887] [G loss: 1.861770]\n",
      "89 [D loss: 0.388633] [G loss: 1.736816]\n",
      "90 [D loss: 0.372850] [G loss: 2.750029]\n",
      "91 [D loss: 0.331620] [G loss: 0.525146]\n",
      "92 [D loss: 0.821100] [G loss: 2.785455]\n",
      "93 [D loss: 0.447033] [G loss: 0.775637]\n",
      "94 [D loss: 0.499951] [G loss: 1.526303]\n",
      "95 [D loss: 0.465612] [G loss: 2.419040]\n",
      "96 [D loss: 0.428561] [G loss: 1.158108]\n",
      "97 [D loss: 0.509122] [G loss: 4.009765]\n",
      "98 [D loss: 1.014718] [G loss: 0.496343]\n",
      "99 [D loss: 0.629540] [G loss: 0.696433]\n",
      "100 [D loss: 0.489409] [G loss: 0.832106]\n",
      "101 [D loss: 0.459150] [G loss: 0.945539]\n",
      "102 [D loss: 0.420641] [G loss: 1.217440]\n",
      "103 [D loss: 0.427153] [G loss: 1.252010]\n",
      "104 [D loss: 0.502826] [G loss: 1.812707]\n",
      "105 [D loss: 0.467573] [G loss: 1.505605]\n",
      "106 [D loss: 0.441738] [G loss: 3.262197]\n",
      "107 [D loss: 0.463079] [G loss: 0.380709]\n",
      "108 [D loss: 0.464966] [G loss: 2.367978]\n",
      "109 [D loss: 0.337891] [G loss: 1.185812]\n",
      "110 [D loss: 0.361104] [G loss: 1.504202]\n",
      "111 [D loss: 0.625698] [G loss: 1.989044]\n",
      "112 [D loss: 0.583813] [G loss: 0.737605]\n",
      "113 [D loss: 0.670586] [G loss: 1.442972]\n",
      "114 [D loss: 0.487089] [G loss: 1.226084]\n",
      "115 [D loss: 0.423822] [G loss: 1.633640]\n",
      "116 [D loss: 0.412772] [G loss: 1.347214]\n",
      "117 [D loss: 0.434456] [G loss: 2.287773]\n",
      "118 [D loss: 0.450570] [G loss: 0.814765]\n",
      "119 [D loss: 0.693619] [G loss: 2.687948]\n",
      "120 [D loss: 0.682888] [G loss: 1.021090]\n",
      "121 [D loss: 0.516016] [G loss: 1.291795]\n",
      "122 [D loss: 0.505866] [G loss: 1.342284]\n",
      "123 [D loss: 0.465327] [G loss: 1.701674]\n",
      "124 [D loss: 0.421528] [G loss: 1.501497]\n",
      "125 [D loss: 0.497611] [G loss: 1.697211]\n",
      "126 [D loss: 0.408900] [G loss: 1.447087]\n",
      "127 [D loss: 0.465526] [G loss: 2.331081]\n",
      "128 [D loss: 0.536264] [G loss: 0.675453]\n",
      "129 [D loss: 0.698969] [G loss: 2.003533]\n",
      "130 [D loss: 0.589283] [G loss: 1.136916]\n",
      "131 [D loss: 0.475993] [G loss: 1.309923]\n",
      "132 [D loss: 0.432724] [G loss: 1.296829]\n",
      "133 [D loss: 0.473979] [G loss: 1.421766]\n",
      "134 [D loss: 0.407666] [G loss: 1.266269]\n",
      "135 [D loss: 0.475098] [G loss: 2.231848]\n",
      "136 [D loss: 0.523371] [G loss: 0.380025]\n",
      "137 [D loss: 1.091825] [G loss: 1.421583]\n",
      "138 [D loss: 0.527463] [G loss: 1.210314]\n",
      "139 [D loss: 0.454011] [G loss: 1.353704]\n",
      "140 [D loss: 0.485620] [G loss: 1.362563]\n",
      "141 [D loss: 0.452883] [G loss: 1.606740]\n",
      "142 [D loss: 0.356986] [G loss: 1.558988]\n",
      "143 [D loss: 0.363338] [G loss: 2.862928]\n",
      "144 [D loss: 0.622096] [G loss: 0.513603]\n",
      "145 [D loss: 0.843601] [G loss: 1.098429]\n",
      "146 [D loss: 0.497799] [G loss: 1.306676]\n",
      "147 [D loss: 0.435145] [G loss: 1.695860]\n",
      "148 [D loss: 0.427370] [G loss: 1.230933]\n",
      "149 [D loss: 0.414677] [G loss: 2.078055]\n",
      "150 [D loss: 0.433008] [G loss: 1.018195]\n",
      "151 [D loss: 0.460929] [G loss: 2.407768]\n",
      "152 [D loss: 0.481400] [G loss: 0.885872]\n",
      "153 [D loss: 0.504527] [G loss: 1.965485]\n",
      "154 [D loss: 0.463441] [G loss: 1.010936]\n",
      "155 [D loss: 0.453116] [G loss: 2.276786]\n",
      "156 [D loss: 0.528435] [G loss: 0.859756]\n",
      "157 [D loss: 0.532239] [G loss: 1.671498]\n",
      "158 [D loss: 0.437541] [G loss: 1.340168]\n",
      "159 [D loss: 0.434291] [G loss: 1.957573]\n",
      "160 [D loss: 0.360813] [G loss: 1.366328]\n",
      "161 [D loss: 0.429459] [G loss: 2.599740]\n",
      "162 [D loss: 0.550432] [G loss: 0.551233]\n",
      "163 [D loss: 0.736409] [G loss: 1.527708]\n",
      "164 [D loss: 0.475347] [G loss: 1.389693]\n",
      "165 [D loss: 0.447964] [G loss: 1.567827]\n",
      "166 [D loss: 0.415174] [G loss: 1.499190]\n",
      "167 [D loss: 0.435787] [G loss: 1.378208]\n",
      "168 [D loss: 0.422887] [G loss: 2.066168]\n",
      "169 [D loss: 0.404111] [G loss: 1.084468]\n",
      "170 [D loss: 0.448846] [G loss: 2.786354]\n",
      "171 [D loss: 0.590699] [G loss: 0.573855]\n",
      "172 [D loss: 0.587224] [G loss: 1.647220]\n",
      "173 [D loss: 0.423059] [G loss: 1.496466]\n",
      "174 [D loss: 0.469960] [G loss: 1.561263]\n",
      "175 [D loss: 0.434087] [G loss: 1.318442]\n",
      "176 [D loss: 0.389408] [G loss: 2.142718]\n",
      "177 [D loss: 0.426469] [G loss: 0.817284]\n",
      "178 [D loss: 0.618953] [G loss: 2.450926]\n",
      "179 [D loss: 0.577210] [G loss: 0.864451]\n",
      "180 [D loss: 0.551164] [G loss: 1.413490]\n",
      "181 [D loss: 0.447722] [G loss: 1.580889]\n",
      "182 [D loss: 0.386702] [G loss: 1.586573]\n",
      "183 [D loss: 0.378171] [G loss: 1.582525]\n",
      "184 [D loss: 0.420945] [G loss: 1.817936]\n",
      "185 [D loss: 0.423439] [G loss: 1.347371]\n",
      "186 [D loss: 0.427253] [G loss: 2.948027]\n",
      "187 [D loss: 0.668693] [G loss: 0.483573]\n",
      "188 [D loss: 0.801313] [G loss: 1.507531]\n",
      "189 [D loss: 0.484606] [G loss: 1.274689]\n",
      "190 [D loss: 0.470029] [G loss: 1.521032]\n",
      "191 [D loss: 0.461891] [G loss: 1.388976]\n",
      "192 [D loss: 0.423068] [G loss: 1.588764]\n",
      "193 [D loss: 0.380718] [G loss: 1.426106]\n",
      "194 [D loss: 0.408903] [G loss: 1.835533]\n",
      "195 [D loss: 0.377343] [G loss: 1.149508]\n",
      "196 [D loss: 0.397137] [G loss: 2.668763]\n",
      "197 [D loss: 0.577308] [G loss: 0.600259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 [D loss: 0.820517] [G loss: 1.783270]\n",
      "199 [D loss: 0.545271] [G loss: 1.163318]\n",
      "200 [D loss: 0.450629] [G loss: 1.261792]\n",
      "201 [D loss: 0.444586] [G loss: 1.378420]\n",
      "202 [D loss: 0.447525] [G loss: 1.798713]\n",
      "203 [D loss: 0.426573] [G loss: 1.443964]\n",
      "204 [D loss: 0.477271] [G loss: 1.818079]\n",
      "205 [D loss: 0.468283] [G loss: 1.008644]\n",
      "206 [D loss: 0.459495] [G loss: 2.167831]\n",
      "207 [D loss: 0.471352] [G loss: 0.813079]\n",
      "208 [D loss: 0.602740] [G loss: 2.168091]\n",
      "209 [D loss: 0.540480] [G loss: 0.924084]\n",
      "210 [D loss: 0.515544] [G loss: 1.708236]\n",
      "211 [D loss: 0.449996] [G loss: 1.279569]\n",
      "212 [D loss: 0.432540] [G loss: 1.756356]\n",
      "213 [D loss: 0.446613] [G loss: 1.238689]\n",
      "214 [D loss: 0.403760] [G loss: 1.872491]\n",
      "215 [D loss: 0.443696] [G loss: 1.393434]\n",
      "216 [D loss: 0.449501] [G loss: 2.228524]\n",
      "217 [D loss: 0.423603] [G loss: 0.949734]\n",
      "218 [D loss: 0.566973] [G loss: 2.509978]\n",
      "219 [D loss: 0.593490] [G loss: 0.917234]\n",
      "220 [D loss: 0.549214] [G loss: 1.766102]\n",
      "221 [D loss: 0.515132] [G loss: 1.183868]\n",
      "222 [D loss: 0.411720] [G loss: 1.654104]\n",
      "223 [D loss: 0.453271] [G loss: 1.441634]\n",
      "224 [D loss: 0.471084] [G loss: 1.781588]\n",
      "225 [D loss: 0.419076] [G loss: 1.245259]\n",
      "226 [D loss: 0.475884] [G loss: 2.140945]\n",
      "227 [D loss: 0.522452] [G loss: 0.972434]\n",
      "228 [D loss: 0.506921] [G loss: 1.983697]\n",
      "229 [D loss: 0.508785] [G loss: 1.238467]\n",
      "230 [D loss: 0.438137] [G loss: 1.733494]\n",
      "231 [D loss: 0.455911] [G loss: 1.249130]\n",
      "232 [D loss: 0.474202] [G loss: 1.700253]\n",
      "233 [D loss: 0.424806] [G loss: 1.532764]\n",
      "234 [D loss: 0.421047] [G loss: 1.766527]\n",
      "235 [D loss: 0.366837] [G loss: 1.469078]\n",
      "236 [D loss: 0.381060] [G loss: 2.241319]\n",
      "237 [D loss: 0.515596] [G loss: 0.782731]\n",
      "238 [D loss: 0.701153] [G loss: 2.406568]\n",
      "239 [D loss: 0.592154] [G loss: 1.117693]\n",
      "240 [D loss: 0.465631] [G loss: 1.525489]\n",
      "241 [D loss: 0.454502] [G loss: 1.219462]\n",
      "242 [D loss: 0.469047] [G loss: 1.660419]\n",
      "243 [D loss: 0.475470] [G loss: 1.493237]\n",
      "244 [D loss: 0.442722] [G loss: 1.725485]\n",
      "245 [D loss: 0.427469] [G loss: 1.255178]\n",
      "246 [D loss: 0.445674] [G loss: 2.229677]\n",
      "247 [D loss: 0.480014] [G loss: 0.937834]\n",
      "248 [D loss: 0.555623] [G loss: 2.032752]\n",
      "249 [D loss: 0.444174] [G loss: 1.268200]\n",
      "250 [D loss: 0.423770] [G loss: 1.588381]\n",
      "251 [D loss: 0.459556] [G loss: 1.548516]\n",
      "252 [D loss: 0.443742] [G loss: 2.084009]\n",
      "253 [D loss: 0.437759] [G loss: 1.033691]\n",
      "254 [D loss: 0.450489] [G loss: 2.416519]\n",
      "255 [D loss: 0.429410] [G loss: 1.036713]\n",
      "256 [D loss: 0.506878] [G loss: 2.164227]\n",
      "257 [D loss: 0.518091] [G loss: 0.915858]\n",
      "258 [D loss: 0.513848] [G loss: 1.978928]\n",
      "259 [D loss: 0.484516] [G loss: 1.403567]\n",
      "260 [D loss: 0.407913] [G loss: 1.837440]\n",
      "261 [D loss: 0.410086] [G loss: 1.559975]\n",
      "262 [D loss: 0.421793] [G loss: 1.847069]\n",
      "263 [D loss: 0.389797] [G loss: 1.356689]\n",
      "264 [D loss: 0.417922] [G loss: 2.241650]\n",
      "265 [D loss: 0.442719] [G loss: 1.172544]\n",
      "266 [D loss: 0.426075] [G loss: 2.503076]\n",
      "267 [D loss: 0.538727] [G loss: 0.917309]\n",
      "268 [D loss: 0.540624] [G loss: 2.192672]\n",
      "269 [D loss: 0.463203] [G loss: 1.222185]\n",
      "270 [D loss: 0.446455] [G loss: 1.946919]\n",
      "271 [D loss: 0.454428] [G loss: 1.381489]\n",
      "272 [D loss: 0.447108] [G loss: 1.942996]\n",
      "273 [D loss: 0.431652] [G loss: 1.335929]\n",
      "274 [D loss: 0.402412] [G loss: 2.155230]\n",
      "275 [D loss: 0.445068] [G loss: 1.283924]\n",
      "276 [D loss: 0.471254] [G loss: 2.234186]\n",
      "277 [D loss: 0.483029] [G loss: 1.279975]\n",
      "278 [D loss: 0.489350] [G loss: 2.051536]\n",
      "279 [D loss: 0.463321] [G loss: 1.392908]\n",
      "280 [D loss: 0.439144] [G loss: 2.296152]\n",
      "281 [D loss: 0.527103] [G loss: 1.229150]\n",
      "282 [D loss: 0.420082] [G loss: 1.995784]\n",
      "283 [D loss: 0.462022] [G loss: 1.289770]\n",
      "284 [D loss: 0.442064] [G loss: 2.130746]\n",
      "285 [D loss: 0.469444] [G loss: 1.253551]\n",
      "286 [D loss: 0.473242] [G loss: 2.245328]\n",
      "287 [D loss: 0.498684] [G loss: 1.281207]\n",
      "288 [D loss: 0.407335] [G loss: 1.976712]\n",
      "289 [D loss: 0.473114] [G loss: 1.454934]\n",
      "290 [D loss: 0.442131] [G loss: 1.883830]\n",
      "291 [D loss: 0.416394] [G loss: 1.334830]\n",
      "292 [D loss: 0.478543] [G loss: 2.147625]\n",
      "293 [D loss: 0.529156] [G loss: 1.149258]\n",
      "294 [D loss: 0.459156] [G loss: 1.986692]\n",
      "295 [D loss: 0.520502] [G loss: 1.193990]\n",
      "296 [D loss: 0.412837] [G loss: 1.760614]\n",
      "297 [D loss: 0.417746] [G loss: 1.543658]\n",
      "298 [D loss: 0.386860] [G loss: 1.990893]\n",
      "299 [D loss: 0.391707] [G loss: 1.714728]\n",
      "300 [D loss: 0.395571] [G loss: 2.256857]\n",
      "301 [D loss: 0.442714] [G loss: 1.145725]\n",
      "302 [D loss: 0.469045] [G loss: 2.340631]\n",
      "303 [D loss: 0.485574] [G loss: 1.003840]\n",
      "304 [D loss: 0.511783] [G loss: 2.220837]\n",
      "305 [D loss: 0.449295] [G loss: 1.319560]\n",
      "306 [D loss: 0.442259] [G loss: 1.736213]\n",
      "307 [D loss: 0.366173] [G loss: 1.581319]\n",
      "308 [D loss: 0.424520] [G loss: 2.014450]\n",
      "309 [D loss: 0.427777] [G loss: 1.435415]\n",
      "310 [D loss: 0.403363] [G loss: 2.091374]\n",
      "311 [D loss: 0.376703] [G loss: 1.475021]\n",
      "312 [D loss: 0.378450] [G loss: 2.283509]\n",
      "313 [D loss: 0.434638] [G loss: 1.362935]\n",
      "314 [D loss: 0.445018] [G loss: 2.644886]\n",
      "315 [D loss: 0.567853] [G loss: 1.054086]\n",
      "316 [D loss: 0.557994] [G loss: 2.141706]\n",
      "317 [D loss: 0.464245] [G loss: 1.357829]\n",
      "318 [D loss: 0.433197] [G loss: 2.077085]\n",
      "319 [D loss: 0.463297] [G loss: 1.449276]\n",
      "320 [D loss: 0.389193] [G loss: 1.967444]\n",
      "321 [D loss: 0.381918] [G loss: 1.746820]\n",
      "322 [D loss: 0.399802] [G loss: 1.953040]\n",
      "323 [D loss: 0.378872] [G loss: 1.784686]\n",
      "324 [D loss: 0.383472] [G loss: 2.160608]\n",
      "325 [D loss: 0.416517] [G loss: 1.049512]\n",
      "326 [D loss: 0.502552] [G loss: 2.789029]\n",
      "327 [D loss: 0.592084] [G loss: 1.242570]\n",
      "328 [D loss: 0.477884] [G loss: 1.925421]\n",
      "329 [D loss: 0.426893] [G loss: 1.588615]\n",
      "330 [D loss: 0.460810] [G loss: 1.877190]\n",
      "331 [D loss: 0.398322] [G loss: 1.655690]\n",
      "332 [D loss: 0.398563] [G loss: 1.777939]\n",
      "333 [D loss: 0.402899] [G loss: 2.105183]\n",
      "334 [D loss: 0.419248] [G loss: 1.493663]\n",
      "335 [D loss: 0.432557] [G loss: 2.588165]\n",
      "336 [D loss: 0.485115] [G loss: 1.115414]\n",
      "337 [D loss: 0.499132] [G loss: 2.326011]\n",
      "338 [D loss: 0.484773] [G loss: 1.257791]\n",
      "339 [D loss: 0.442290] [G loss: 1.909990]\n",
      "340 [D loss: 0.399360] [G loss: 1.463521]\n",
      "341 [D loss: 0.340573] [G loss: 1.715908]\n",
      "342 [D loss: 0.378899] [G loss: 1.825758]\n",
      "343 [D loss: 0.366698] [G loss: 1.977584]\n",
      "344 [D loss: 0.413107] [G loss: 1.343471]\n",
      "345 [D loss: 0.421973] [G loss: 2.726533]\n",
      "346 [D loss: 0.627935] [G loss: 0.913273]\n",
      "347 [D loss: 0.561561] [G loss: 2.229733]\n",
      "348 [D loss: 0.457420] [G loss: 1.375025]\n",
      "349 [D loss: 0.395404] [G loss: 2.022521]\n",
      "350 [D loss: 0.425649] [G loss: 1.625011]\n",
      "351 [D loss: 0.368818] [G loss: 1.785639]\n",
      "352 [D loss: 0.432918] [G loss: 1.448895]\n",
      "353 [D loss: 0.329730] [G loss: 1.913371]\n",
      "354 [D loss: 0.429659] [G loss: 1.551381]\n",
      "355 [D loss: 0.444236] [G loss: 2.380571]\n",
      "356 [D loss: 0.519113] [G loss: 1.037524]\n",
      "357 [D loss: 0.610415] [G loss: 2.202993]\n",
      "358 [D loss: 0.521481] [G loss: 1.271796]\n",
      "359 [D loss: 0.441725] [G loss: 1.624758]\n",
      "360 [D loss: 0.389774] [G loss: 1.477175]\n",
      "361 [D loss: 0.397286] [G loss: 2.014762]\n",
      "362 [D loss: 0.411909] [G loss: 1.462579]\n",
      "363 [D loss: 0.424117] [G loss: 1.989075]\n",
      "364 [D loss: 0.417024] [G loss: 1.435173]\n",
      "365 [D loss: 0.474019] [G loss: 2.519661]\n",
      "366 [D loss: 0.562114] [G loss: 1.070363]\n",
      "367 [D loss: 0.553176] [G loss: 2.073047]\n",
      "368 [D loss: 0.409844] [G loss: 1.228200]\n",
      "369 [D loss: 0.466092] [G loss: 1.759150]\n",
      "370 [D loss: 0.439763] [G loss: 1.339036]\n",
      "371 [D loss: 0.406231] [G loss: 1.948722]\n",
      "372 [D loss: 0.399720] [G loss: 1.560254]\n",
      "373 [D loss: 0.392594] [G loss: 1.943991]\n",
      "374 [D loss: 0.429635] [G loss: 1.359802]\n",
      "375 [D loss: 0.447920] [G loss: 2.396091]\n",
      "376 [D loss: 0.484583] [G loss: 1.090371]\n",
      "377 [D loss: 0.558817] [G loss: 2.317076]\n",
      "378 [D loss: 0.450058] [G loss: 1.254179]\n",
      "379 [D loss: 0.445158] [G loss: 1.960783]\n",
      "380 [D loss: 0.409487] [G loss: 1.206335]\n",
      "381 [D loss: 0.401488] [G loss: 1.772489]\n",
      "382 [D loss: 0.397262] [G loss: 1.491559]\n",
      "383 [D loss: 0.419211] [G loss: 2.277542]\n",
      "384 [D loss: 0.427451] [G loss: 1.396091]\n",
      "385 [D loss: 0.429052] [G loss: 2.370597]\n",
      "386 [D loss: 0.489048] [G loss: 1.135505]\n",
      "387 [D loss: 0.503119] [G loss: 2.259411]\n",
      "388 [D loss: 0.522334] [G loss: 1.086733]\n",
      "389 [D loss: 0.471123] [G loss: 1.939310]\n",
      "390 [D loss: 0.470428] [G loss: 1.242877]\n",
      "391 [D loss: 0.394592] [G loss: 1.962024]\n",
      "392 [D loss: 0.443491] [G loss: 1.386495]\n",
      "393 [D loss: 0.399588] [G loss: 1.775707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 [D loss: 0.424632] [G loss: 1.523492]\n",
      "395 [D loss: 0.365450] [G loss: 1.921022]\n",
      "396 [D loss: 0.388012] [G loss: 1.385487]\n",
      "397 [D loss: 0.426137] [G loss: 2.905711]\n",
      "398 [D loss: 0.559021] [G loss: 1.012742]\n",
      "399 [D loss: 0.563862] [G loss: 2.199816]\n",
      "400 [D loss: 0.500930] [G loss: 1.391890]\n",
      "401 [D loss: 0.442816] [G loss: 1.811485]\n",
      "402 [D loss: 0.470199] [G loss: 1.263982]\n",
      "403 [D loss: 0.439131] [G loss: 1.563726]\n",
      "404 [D loss: 0.427140] [G loss: 1.480453]\n",
      "405 [D loss: 0.429598] [G loss: 1.768553]\n",
      "406 [D loss: 0.348121] [G loss: 1.754399]\n",
      "407 [D loss: 0.446338] [G loss: 1.713839]\n",
      "408 [D loss: 0.385159] [G loss: 1.765837]\n",
      "409 [D loss: 0.405071] [G loss: 1.685676]\n",
      "410 [D loss: 0.478101] [G loss: 2.090763]\n",
      "411 [D loss: 0.405052] [G loss: 1.255678]\n",
      "412 [D loss: 0.590719] [G loss: 2.773008]\n",
      "413 [D loss: 0.641694] [G loss: 1.113344]\n",
      "414 [D loss: 0.491963] [G loss: 1.835737]\n",
      "415 [D loss: 0.464284] [G loss: 1.308913]\n",
      "416 [D loss: 0.422584] [G loss: 1.736901]\n",
      "417 [D loss: 0.418343] [G loss: 1.534407]\n",
      "418 [D loss: 0.346340] [G loss: 1.805081]\n",
      "419 [D loss: 0.393945] [G loss: 1.647351]\n",
      "420 [D loss: 0.415076] [G loss: 2.079277]\n",
      "421 [D loss: 0.468837] [G loss: 1.229903]\n",
      "422 [D loss: 0.474645] [G loss: 2.477197]\n",
      "423 [D loss: 0.510820] [G loss: 1.076846]\n",
      "424 [D loss: 0.514398] [G loss: 2.011834]\n",
      "425 [D loss: 0.447737] [G loss: 1.188865]\n",
      "426 [D loss: 0.432300] [G loss: 1.875835]\n",
      "427 [D loss: 0.411925] [G loss: 1.422250]\n",
      "428 [D loss: 0.421251] [G loss: 1.867798]\n",
      "429 [D loss: 0.428921] [G loss: 1.655789]\n",
      "430 [D loss: 0.409635] [G loss: 2.020954]\n",
      "431 [D loss: 0.405937] [G loss: 1.197325]\n",
      "432 [D loss: 0.436031] [G loss: 2.223053]\n",
      "433 [D loss: 0.457343] [G loss: 1.185610]\n",
      "434 [D loss: 0.518794] [G loss: 2.477911]\n",
      "435 [D loss: 0.589523] [G loss: 1.201687]\n",
      "436 [D loss: 0.465274] [G loss: 1.716020]\n",
      "437 [D loss: 0.392841] [G loss: 1.375313]\n",
      "438 [D loss: 0.390777] [G loss: 1.548341]\n",
      "439 [D loss: 0.452561] [G loss: 1.639326]\n",
      "440 [D loss: 0.394687] [G loss: 1.644911]\n",
      "441 [D loss: 0.410918] [G loss: 2.184079]\n",
      "442 [D loss: 0.507682] [G loss: 1.491048]\n",
      "443 [D loss: 0.435658] [G loss: 1.621195]\n",
      "444 [D loss: 0.441288] [G loss: 1.871918]\n",
      "445 [D loss: 0.389792] [G loss: 1.945805]\n",
      "446 [D loss: 0.315847] [G loss: 1.776177]\n",
      "447 [D loss: 0.418358] [G loss: 1.621685]\n",
      "448 [D loss: 0.446521] [G loss: 2.883013]\n",
      "449 [D loss: 0.572089] [G loss: 0.854721]\n",
      "450 [D loss: 0.653038] [G loss: 2.215793]\n",
      "451 [D loss: 0.534433] [G loss: 1.162162]\n",
      "452 [D loss: 0.402819] [G loss: 1.642547]\n",
      "453 [D loss: 0.400363] [G loss: 1.506284]\n",
      "454 [D loss: 0.373924] [G loss: 1.730611]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-df79667760d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0mprint_every_n_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPRINT_EVERY_N_BATCHES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mINITIAL_EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;34m,\u001b[0m\u001b[0mdiscriminator_training_loops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/Git/Personal/GDL/generative_deep_learning_code/models/GAN_ELE.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, batch_size, epochs, run_folder, print_every_n_batches, initial_epoch, lr_decay, discriminator_training_loops, clip_threshold)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;31m# Plot the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gdl/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan.train(     \n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = 6000\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    "    , initial_epoch = INITIAL_EPOCH\n",
    "    ,discriminator_training_loops = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89452]\n",
      "[0.9135624]\n",
      "[0.8444373]\n",
      "[0.617947]\n",
      "[0.6158441]\n",
      "[0.21588135]\n",
      "[0.7526824]\n",
      "[0.4391292]\n",
      "[0.884499]\n",
      "[0.9719837]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(gan.discriminator.predict(np.array([x_train[i]]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.20445967], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = np.random.uniform(-1, 1, 100)\n",
    "img = gan.generator.predict(np.array([noise]))[0]\n",
    "\n",
    "print(img.shape)\n",
    "plt.imshow(img[:,:,0])\n",
    "\n",
    "gan.discriminator.predict(np.array([img]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gan.discriminator.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "for x, y in enumerate(gan.discriminator.layers):\n",
    "    \n",
    "    print(y)\n",
    "    print(y.trainable)\n",
    "    for i in gan.discriminator.layers[x].get_weights():\n",
    "        \n",
    "        print(pointer)\n",
    "        print(i.shape)\n",
    "        pointer+=1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gan.discriminator.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.discriminator.get_weights()[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.model.save_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
