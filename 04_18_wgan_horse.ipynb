{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.WGAN import WGAN\n",
    "from utils.loaders import load_safari, load_cifar10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = '0018'\n",
    "RUN_FOLDER = os.path.join(\"./run\", RUN_ID)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 7\n",
    "(x_train, y_train) = load_cifar10(label)\n",
    "# (x_train, y_train) = load_safari('elephant')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12f7f85c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(x_train))\n",
    "print(np.max(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works\n",
    "\n",
    "gan = WGAN(input_dim = (IMAGE_SIZE,IMAGE_SIZE,3)\n",
    "        , discriminator_conv_filters = [64,64,128,128]\n",
    "        , discriminator_conv_kernel_size = [5,5,5,5]\n",
    "        , discriminator_conv_strides = [2,2,2,1]\n",
    "        , discriminator_conv_padding = 'same'\n",
    "        , discriminator_batch_norm_momentum = None\n",
    "        , discriminator_activation = 'leaky_relu'\n",
    "        , discriminator_dropout_rate = None\n",
    "        , discriminator_learning_rate = 0.00005\n",
    "        , generator_initial_dense_layer_size = (8, 8, 64)\n",
    "        , generator_use_upsampling = [True,True, False,False]\n",
    "        , generator_conv_t_filters = [128,64, 64,3]\n",
    "        , generator_conv_t_kernel_size = [5,5,5,5]\n",
    "        , generator_conv_t_strides = [1,1,1,1]\n",
    "        , generator_conv_t_padding = 'same'\n",
    "        , generator_batch_norm_momentum = 0.8\n",
    "        , generator_activation = 'leaky_relu'\n",
    "        , generator_dropout_rate = None\n",
    "        , generator_learning_rate = 0.00005\n",
    "        , optimiser = 'rmsprop'\n",
    "        , z_dim = 100\n",
    "        )\n",
    "\n",
    "\n",
    "gan.save(RUN_FOLDER)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"class_name\": \"Model\",\n",
      "    \"config\": {\n",
      "        \"name\": \"model_3\",\n",
      "        \"layers\": [\n",
      "            {\n",
      "                \"name\": \"model_input\",\n",
      "                \"class_name\": \"InputLayer\",\n",
      "                \"config\": {\n",
      "                    \"batch_input_shape\": [\n",
      "                        null,\n",
      "                        100\n",
      "                    ],\n",
      "                    \"dtype\": \"float32\",\n",
      "                    \"sparse\": false,\n",
      "                    \"name\": \"model_input\"\n",
      "                },\n",
      "                \"inbound_nodes\": []\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"model_2\",\n",
      "                \"class_name\": \"Model\",\n",
      "                \"config\": {\n",
      "                    \"name\": \"model_2\",\n",
      "                    \"layers\": [\n",
      "                        {\n",
      "                            \"name\": \"generator_input\",\n",
      "                            \"class_name\": \"InputLayer\",\n",
      "                            \"config\": {\n",
      "                                \"batch_input_shape\": [\n",
      "                                    null,\n",
      "                                    100\n",
      "                                ],\n",
      "                                \"dtype\": \"float32\",\n",
      "                                \"sparse\": false,\n",
      "                                \"name\": \"generator_input\"\n",
      "                            },\n",
      "                            \"inbound_nodes\": []\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"dense_2\",\n",
      "                            \"class_name\": \"Dense\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"dense_2\",\n",
      "                                \"trainable\": true,\n",
      "                                \"units\": 4096,\n",
      "                                \"activation\": \"linear\",\n",
      "                                \"use_bias\": true,\n",
      "                                \"kernel_initializer\": {\n",
      "                                    \"class_name\": \"RandomNormal\",\n",
      "                                    \"config\": {\n",
      "                                        \"mean\": 0.0,\n",
      "                                        \"stddev\": 0.02,\n",
      "                                        \"seed\": null\n",
      "                                    }\n",
      "                                },\n",
      "                                \"bias_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"kernel_regularizer\": null,\n",
      "                                \"bias_regularizer\": null,\n",
      "                                \"activity_regularizer\": null,\n",
      "                                \"kernel_constraint\": null,\n",
      "                                \"bias_constraint\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"generator_input\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"batch_normalization_1\",\n",
      "                            \"class_name\": \"BatchNormalization\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"batch_normalization_1\",\n",
      "                                \"trainable\": true,\n",
      "                                \"axis\": -1,\n",
      "                                \"momentum\": 0.8,\n",
      "                                \"epsilon\": 0.001,\n",
      "                                \"center\": true,\n",
      "                                \"scale\": true,\n",
      "                                \"beta_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"gamma_initializer\": {\n",
      "                                    \"class_name\": \"Ones\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"moving_mean_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"moving_variance_initializer\": {\n",
      "                                    \"class_name\": \"Ones\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"beta_regularizer\": null,\n",
      "                                \"gamma_regularizer\": null,\n",
      "                                \"beta_constraint\": null,\n",
      "                                \"gamma_constraint\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"dense_2\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"leaky_re_lu_5\",\n",
      "                            \"class_name\": \"LeakyReLU\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"leaky_re_lu_5\",\n",
      "                                \"trainable\": true,\n",
      "                                \"alpha\": 0.20000000298023224\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"batch_normalization_1\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"reshape_1\",\n",
      "                            \"class_name\": \"Reshape\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"reshape_1\",\n",
      "                                \"trainable\": true,\n",
      "                                \"target_shape\": [\n",
      "                                    8,\n",
      "                                    8,\n",
      "                                    64\n",
      "                                ]\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"leaky_re_lu_5\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"up_sampling2d_1\",\n",
      "                            \"class_name\": \"UpSampling2D\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"up_sampling2d_1\",\n",
      "                                \"trainable\": true,\n",
      "                                \"size\": [\n",
      "                                    2,\n",
      "                                    2\n",
      "                                ],\n",
      "                                \"data_format\": \"channels_last\",\n",
      "                                \"interpolation\": \"nearest\"\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"reshape_1\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"generator_conv_t_0\",\n",
      "                            \"class_name\": \"Conv2DTranspose\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"generator_conv_t_0\",\n",
      "                                \"trainable\": true,\n",
      "                                \"filters\": 128,\n",
      "                                \"kernel_size\": [\n",
      "                                    5,\n",
      "                                    5\n",
      "                                ],\n",
      "                                \"strides\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"padding\": \"same\",\n",
      "                                \"data_format\": \"channels_last\",\n",
      "                                \"dilation_rate\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"activation\": \"linear\",\n",
      "                                \"use_bias\": true,\n",
      "                                \"kernel_initializer\": {\n",
      "                                    \"class_name\": \"RandomNormal\",\n",
      "                                    \"config\": {\n",
      "                                        \"mean\": 0.0,\n",
      "                                        \"stddev\": 0.02,\n",
      "                                        \"seed\": null\n",
      "                                    }\n",
      "                                },\n",
      "                                \"bias_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"kernel_regularizer\": null,\n",
      "                                \"bias_regularizer\": null,\n",
      "                                \"activity_regularizer\": null,\n",
      "                                \"kernel_constraint\": null,\n",
      "                                \"bias_constraint\": null,\n",
      "                                \"output_padding\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"up_sampling2d_1\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"batch_normalization_2\",\n",
      "                            \"class_name\": \"BatchNormalization\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"batch_normalization_2\",\n",
      "                                \"trainable\": true,\n",
      "                                \"axis\": -1,\n",
      "                                \"momentum\": 0.8,\n",
      "                                \"epsilon\": 0.001,\n",
      "                                \"center\": true,\n",
      "                                \"scale\": true,\n",
      "                                \"beta_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"gamma_initializer\": {\n",
      "                                    \"class_name\": \"Ones\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"moving_mean_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"moving_variance_initializer\": {\n",
      "                                    \"class_name\": \"Ones\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"beta_regularizer\": null,\n",
      "                                \"gamma_regularizer\": null,\n",
      "                                \"beta_constraint\": null,\n",
      "                                \"gamma_constraint\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"generator_conv_t_0\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"leaky_re_lu_6\",\n",
      "                            \"class_name\": \"LeakyReLU\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"leaky_re_lu_6\",\n",
      "                                \"trainable\": true,\n",
      "                                \"alpha\": 0.20000000298023224\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"batch_normalization_2\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"up_sampling2d_2\",\n",
      "                            \"class_name\": \"UpSampling2D\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"up_sampling2d_2\",\n",
      "                                \"trainable\": true,\n",
      "                                \"size\": [\n",
      "                                    2,\n",
      "                                    2\n",
      "                                ],\n",
      "                                \"data_format\": \"channels_last\",\n",
      "                                \"interpolation\": \"nearest\"\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"leaky_re_lu_6\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"generator_conv_t_1\",\n",
      "                            \"class_name\": \"Conv2DTranspose\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"generator_conv_t_1\",\n",
      "                                \"trainable\": true,\n",
      "                                \"filters\": 64,\n",
      "                                \"kernel_size\": [\n",
      "                                    5,\n",
      "                                    5\n",
      "                                ],\n",
      "                                \"strides\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"padding\": \"same\",\n",
      "                                \"data_format\": \"channels_last\",\n",
      "                                \"dilation_rate\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"activation\": \"linear\",\n",
      "                                \"use_bias\": true,\n",
      "                                \"kernel_initializer\": {\n",
      "                                    \"class_name\": \"RandomNormal\",\n",
      "                                    \"config\": {\n",
      "                                        \"mean\": 0.0,\n",
      "                                        \"stddev\": 0.02,\n",
      "                                        \"seed\": null\n",
      "                                    }\n",
      "                                },\n",
      "                                \"bias_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"kernel_regularizer\": null,\n",
      "                                \"bias_regularizer\": null,\n",
      "                                \"activity_regularizer\": null,\n",
      "                                \"kernel_constraint\": null,\n",
      "                                \"bias_constraint\": null,\n",
      "                                \"output_padding\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"up_sampling2d_2\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"batch_normalization_3\",\n",
      "                            \"class_name\": \"BatchNormalization\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"batch_normalization_3\",\n",
      "                                \"trainable\": true,\n",
      "                                \"axis\": -1,\n",
      "                                \"momentum\": 0.8,\n",
      "                                \"epsilon\": 0.001,\n",
      "                                \"center\": true,\n",
      "                                \"scale\": true,\n",
      "                                \"beta_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"gamma_initializer\": {\n",
      "                                    \"class_name\": \"Ones\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"moving_mean_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"moving_variance_initializer\": {\n",
      "                                    \"class_name\": \"Ones\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"beta_regularizer\": null,\n",
      "                                \"gamma_regularizer\": null,\n",
      "                                \"beta_constraint\": null,\n",
      "                                \"gamma_constraint\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"generator_conv_t_1\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"leaky_re_lu_7\",\n",
      "                            \"class_name\": \"LeakyReLU\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"leaky_re_lu_7\",\n",
      "                                \"trainable\": true,\n",
      "                                \"alpha\": 0.20000000298023224\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"batch_normalization_3\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"generator_conv_t_2\",\n",
      "                            \"class_name\": \"Conv2DTranspose\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"generator_conv_t_2\",\n",
      "                                \"trainable\": true,\n",
      "                                \"filters\": 64,\n",
      "                                \"kernel_size\": [\n",
      "                                    5,\n",
      "                                    5\n",
      "                                ],\n",
      "                                \"strides\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"padding\": \"same\",\n",
      "                                \"data_format\": \"channels_last\",\n",
      "                                \"dilation_rate\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"activation\": \"linear\",\n",
      "                                \"use_bias\": true,\n",
      "                                \"kernel_initializer\": {\n",
      "                                    \"class_name\": \"RandomNormal\",\n",
      "                                    \"config\": {\n",
      "                                        \"mean\": 0.0,\n",
      "                                        \"stddev\": 0.02,\n",
      "                                        \"seed\": null\n",
      "                                    }\n",
      "                                },\n",
      "                                \"bias_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"kernel_regularizer\": null,\n",
      "                                \"bias_regularizer\": null,\n",
      "                                \"activity_regularizer\": null,\n",
      "                                \"kernel_constraint\": null,\n",
      "                                \"bias_constraint\": null,\n",
      "                                \"output_padding\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"leaky_re_lu_7\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"batch_normalization_4\",\n",
      "                            \"class_name\": \"BatchNormalization\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"batch_normalization_4\",\n",
      "                                \"trainable\": true,\n",
      "                                \"axis\": -1,\n",
      "                                \"momentum\": 0.8,\n",
      "                                \"epsilon\": 0.001,\n",
      "                                \"center\": true,\n",
      "                                \"scale\": true,\n",
      "                                \"beta_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"gamma_initializer\": {\n",
      "                                    \"class_name\": \"Ones\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"moving_mean_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"moving_variance_initializer\": {\n",
      "                                    \"class_name\": \"Ones\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"beta_regularizer\": null,\n",
      "                                \"gamma_regularizer\": null,\n",
      "                                \"beta_constraint\": null,\n",
      "                                \"gamma_constraint\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"generator_conv_t_2\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"leaky_re_lu_8\",\n",
      "                            \"class_name\": \"LeakyReLU\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"leaky_re_lu_8\",\n",
      "                                \"trainable\": true,\n",
      "                                \"alpha\": 0.20000000298023224\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"batch_normalization_4\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"generator_conv_t_3\",\n",
      "                            \"class_name\": \"Conv2DTranspose\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"generator_conv_t_3\",\n",
      "                                \"trainable\": true,\n",
      "                                \"filters\": 3,\n",
      "                                \"kernel_size\": [\n",
      "                                    5,\n",
      "                                    5\n",
      "                                ],\n",
      "                                \"strides\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"padding\": \"same\",\n",
      "                                \"data_format\": \"channels_last\",\n",
      "                                \"dilation_rate\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"activation\": \"linear\",\n",
      "                                \"use_bias\": true,\n",
      "                                \"kernel_initializer\": {\n",
      "                                    \"class_name\": \"RandomNormal\",\n",
      "                                    \"config\": {\n",
      "                                        \"mean\": 0.0,\n",
      "                                        \"stddev\": 0.02,\n",
      "                                        \"seed\": null\n",
      "                                    }\n",
      "                                },\n",
      "                                \"bias_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"kernel_regularizer\": null,\n",
      "                                \"bias_regularizer\": null,\n",
      "                                \"activity_regularizer\": null,\n",
      "                                \"kernel_constraint\": null,\n",
      "                                \"bias_constraint\": null,\n",
      "                                \"output_padding\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"leaky_re_lu_8\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"activation_1\",\n",
      "                            \"class_name\": \"Activation\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"activation_1\",\n",
      "                                \"trainable\": true,\n",
      "                                \"activation\": \"tanh\"\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"generator_conv_t_3\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        }\n",
      "                    ],\n",
      "                    \"input_layers\": [\n",
      "                        [\n",
      "                            \"generator_input\",\n",
      "                            0,\n",
      "                            0\n",
      "                        ]\n",
      "                    ],\n",
      "                    \"output_layers\": [\n",
      "                        [\n",
      "                            \"activation_1\",\n",
      "                            0,\n",
      "                            0\n",
      "                        ]\n",
      "                    ]\n",
      "                },\n",
      "                \"inbound_nodes\": [\n",
      "                    [\n",
      "                        [\n",
      "                            \"model_input\",\n",
      "                            0,\n",
      "                            0,\n",
      "                            {}\n",
      "                        ]\n",
      "                    ]\n",
      "                ]\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"model_1\",\n",
      "                \"class_name\": \"Model\",\n",
      "                \"config\": {\n",
      "                    \"name\": \"model_1\",\n",
      "                    \"layers\": [\n",
      "                        {\n",
      "                            \"name\": \"discriminator_input\",\n",
      "                            \"class_name\": \"InputLayer\",\n",
      "                            \"config\": {\n",
      "                                \"batch_input_shape\": [\n",
      "                                    null,\n",
      "                                    32,\n",
      "                                    32,\n",
      "                                    3\n",
      "                                ],\n",
      "                                \"dtype\": \"float32\",\n",
      "                                \"sparse\": false,\n",
      "                                \"name\": \"discriminator_input\"\n",
      "                            },\n",
      "                            \"inbound_nodes\": []\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"discriminator_conv_0\",\n",
      "                            \"class_name\": \"Conv2D\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"discriminator_conv_0\",\n",
      "                                \"trainable\": false,\n",
      "                                \"filters\": 64,\n",
      "                                \"kernel_size\": [\n",
      "                                    5,\n",
      "                                    5\n",
      "                                ],\n",
      "                                \"strides\": [\n",
      "                                    2,\n",
      "                                    2\n",
      "                                ],\n",
      "                                \"padding\": \"same\",\n",
      "                                \"data_format\": \"channels_last\",\n",
      "                                \"dilation_rate\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"activation\": \"linear\",\n",
      "                                \"use_bias\": true,\n",
      "                                \"kernel_initializer\": {\n",
      "                                    \"class_name\": \"RandomNormal\",\n",
      "                                    \"config\": {\n",
      "                                        \"mean\": 0.0,\n",
      "                                        \"stddev\": 0.02,\n",
      "                                        \"seed\": null\n",
      "                                    }\n",
      "                                },\n",
      "                                \"bias_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"kernel_regularizer\": null,\n",
      "                                \"bias_regularizer\": null,\n",
      "                                \"activity_regularizer\": null,\n",
      "                                \"kernel_constraint\": null,\n",
      "                                \"bias_constraint\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"discriminator_input\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"leaky_re_lu_1\",\n",
      "                            \"class_name\": \"LeakyReLU\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"leaky_re_lu_1\",\n",
      "                                \"trainable\": false,\n",
      "                                \"alpha\": 0.20000000298023224\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"discriminator_conv_0\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"discriminator_conv_1\",\n",
      "                            \"class_name\": \"Conv2D\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"discriminator_conv_1\",\n",
      "                                \"trainable\": false,\n",
      "                                \"filters\": 64,\n",
      "                                \"kernel_size\": [\n",
      "                                    5,\n",
      "                                    5\n",
      "                                ],\n",
      "                                \"strides\": [\n",
      "                                    2,\n",
      "                                    2\n",
      "                                ],\n",
      "                                \"padding\": \"same\",\n",
      "                                \"data_format\": \"channels_last\",\n",
      "                                \"dilation_rate\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"activation\": \"linear\",\n",
      "                                \"use_bias\": true,\n",
      "                                \"kernel_initializer\": {\n",
      "                                    \"class_name\": \"RandomNormal\",\n",
      "                                    \"config\": {\n",
      "                                        \"mean\": 0.0,\n",
      "                                        \"stddev\": 0.02,\n",
      "                                        \"seed\": null\n",
      "                                    }\n",
      "                                },\n",
      "                                \"bias_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"kernel_regularizer\": null,\n",
      "                                \"bias_regularizer\": null,\n",
      "                                \"activity_regularizer\": null,\n",
      "                                \"kernel_constraint\": null,\n",
      "                                \"bias_constraint\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"leaky_re_lu_1\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"leaky_re_lu_2\",\n",
      "                            \"class_name\": \"LeakyReLU\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"leaky_re_lu_2\",\n",
      "                                \"trainable\": false,\n",
      "                                \"alpha\": 0.20000000298023224\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"discriminator_conv_1\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"discriminator_conv_2\",\n",
      "                            \"class_name\": \"Conv2D\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"discriminator_conv_2\",\n",
      "                                \"trainable\": false,\n",
      "                                \"filters\": 128,\n",
      "                                \"kernel_size\": [\n",
      "                                    5,\n",
      "                                    5\n",
      "                                ],\n",
      "                                \"strides\": [\n",
      "                                    2,\n",
      "                                    2\n",
      "                                ],\n",
      "                                \"padding\": \"same\",\n",
      "                                \"data_format\": \"channels_last\",\n",
      "                                \"dilation_rate\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"activation\": \"linear\",\n",
      "                                \"use_bias\": true,\n",
      "                                \"kernel_initializer\": {\n",
      "                                    \"class_name\": \"RandomNormal\",\n",
      "                                    \"config\": {\n",
      "                                        \"mean\": 0.0,\n",
      "                                        \"stddev\": 0.02,\n",
      "                                        \"seed\": null\n",
      "                                    }\n",
      "                                },\n",
      "                                \"bias_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"kernel_regularizer\": null,\n",
      "                                \"bias_regularizer\": null,\n",
      "                                \"activity_regularizer\": null,\n",
      "                                \"kernel_constraint\": null,\n",
      "                                \"bias_constraint\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"leaky_re_lu_2\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"leaky_re_lu_3\",\n",
      "                            \"class_name\": \"LeakyReLU\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"leaky_re_lu_3\",\n",
      "                                \"trainable\": false,\n",
      "                                \"alpha\": 0.20000000298023224\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"discriminator_conv_2\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"discriminator_conv_3\",\n",
      "                            \"class_name\": \"Conv2D\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"discriminator_conv_3\",\n",
      "                                \"trainable\": false,\n",
      "                                \"filters\": 128,\n",
      "                                \"kernel_size\": [\n",
      "                                    5,\n",
      "                                    5\n",
      "                                ],\n",
      "                                \"strides\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"padding\": \"same\",\n",
      "                                \"data_format\": \"channels_last\",\n",
      "                                \"dilation_rate\": [\n",
      "                                    1,\n",
      "                                    1\n",
      "                                ],\n",
      "                                \"activation\": \"linear\",\n",
      "                                \"use_bias\": true,\n",
      "                                \"kernel_initializer\": {\n",
      "                                    \"class_name\": \"RandomNormal\",\n",
      "                                    \"config\": {\n",
      "                                        \"mean\": 0.0,\n",
      "                                        \"stddev\": 0.02,\n",
      "                                        \"seed\": null\n",
      "                                    }\n",
      "                                },\n",
      "                                \"bias_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"kernel_regularizer\": null,\n",
      "                                \"bias_regularizer\": null,\n",
      "                                \"activity_regularizer\": null,\n",
      "                                \"kernel_constraint\": null,\n",
      "                                \"bias_constraint\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"leaky_re_lu_3\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"leaky_re_lu_4\",\n",
      "                            \"class_name\": \"LeakyReLU\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"leaky_re_lu_4\",\n",
      "                                \"trainable\": false,\n",
      "                                \"alpha\": 0.20000000298023224\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"discriminator_conv_3\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"flatten_1\",\n",
      "                            \"class_name\": \"Flatten\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"flatten_1\",\n",
      "                                \"trainable\": false,\n",
      "                                \"data_format\": \"channels_last\"\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"leaky_re_lu_4\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        },\n",
      "                        {\n",
      "                            \"name\": \"dense_1\",\n",
      "                            \"class_name\": \"Dense\",\n",
      "                            \"config\": {\n",
      "                                \"name\": \"dense_1\",\n",
      "                                \"trainable\": false,\n",
      "                                \"units\": 1,\n",
      "                                \"activation\": \"linear\",\n",
      "                                \"use_bias\": true,\n",
      "                                \"kernel_initializer\": {\n",
      "                                    \"class_name\": \"RandomNormal\",\n",
      "                                    \"config\": {\n",
      "                                        \"mean\": 0.0,\n",
      "                                        \"stddev\": 0.02,\n",
      "                                        \"seed\": null\n",
      "                                    }\n",
      "                                },\n",
      "                                \"bias_initializer\": {\n",
      "                                    \"class_name\": \"Zeros\",\n",
      "                                    \"config\": {}\n",
      "                                },\n",
      "                                \"kernel_regularizer\": null,\n",
      "                                \"bias_regularizer\": null,\n",
      "                                \"activity_regularizer\": null,\n",
      "                                \"kernel_constraint\": null,\n",
      "                                \"bias_constraint\": null\n",
      "                            },\n",
      "                            \"inbound_nodes\": [\n",
      "                                [\n",
      "                                    [\n",
      "                                        \"flatten_1\",\n",
      "                                        0,\n",
      "                                        0,\n",
      "                                        {}\n",
      "                                    ]\n",
      "                                ]\n",
      "                            ]\n",
      "                        }\n",
      "                    ],\n",
      "                    \"input_layers\": [\n",
      "                        [\n",
      "                            \"discriminator_input\",\n",
      "                            0,\n",
      "                            0\n",
      "                        ]\n",
      "                    ],\n",
      "                    \"output_layers\": [\n",
      "                        [\n",
      "                            \"dense_1\",\n",
      "                            0,\n",
      "                            0\n",
      "                        ]\n",
      "                    ]\n",
      "                },\n",
      "                \"inbound_nodes\": [\n",
      "                    [\n",
      "                        [\n",
      "                            \"model_2\",\n",
      "                            1,\n",
      "                            0,\n",
      "                            {}\n",
      "                        ]\n",
      "                    ]\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"input_layers\": [\n",
      "            [\n",
      "                \"model_input\",\n",
      "                0,\n",
      "                0\n",
      "            ]\n",
      "        ],\n",
      "        \"output_layers\": [\n",
      "            [\n",
      "                \"model_1\",\n",
      "                1,\n",
      "                0\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    \"keras_version\": \"2.2.4\",\n",
      "    \"backend\": \"tensorflow\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(gan.model.to_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_0 (Conv2D (None, 16, 16, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_1 (Conv2D (None, 8, 8, 64)          102464    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_2 (Conv2D (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "discriminator_conv_3 (Conv2D (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 1,448,066\n",
      "Trainable params: 724,033\n",
      "Non-trainable params: 724,033\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidfoster/.virtualenvs/gdl/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              413696    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_0 (Conv2DTr (None, 16, 16, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_1 (Conv2DTr (None, 32, 32, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_2 (Conv2DTr (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_t_3 (Conv2DTr (None, 32, 32, 3)         4803      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 948,163\n",
      "Trainable params: 939,459\n",
      "Non-trainable params: 8,704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -0.0] [G acc: 1.000]\n",
      "1 (5, 1) [D loss: (-0.0, 0.0, 0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -0.0] [G acc: 1.000]\n",
      "2 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -0.0] [G acc: 1.000]\n",
      "3 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -0.0] [G acc: 1.000]\n",
      "4 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -0.0] [G acc: 0.906]\n",
      "5 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -0.0] [G acc: 1.000]\n",
      "6 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.992, 0.000, 0.496)] [G loss: -0.0] [G acc: 0.945]\n",
      "7 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -0.0] [G acc: 0.977]\n",
      "8 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -0.0] [G acc: 0.961]\n",
      "9 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.961, 0.055, 0.508)] [G loss: 0.0] [G acc: 0.461]\n",
      "10 (5, 1) [D loss: (-0.0, 0.0, 0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -0.0] [G acc: 0.984]\n",
      "11 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.930, 0.242, 0.586)] [G loss: 0.0] [G acc: 0.102]\n",
      "12 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.766, 0.367, 0.566)] [G loss: 0.0] [G acc: 0.102]\n",
      "13 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.594, 0.430, 0.512)] [G loss: 0.0] [G acc: 0.102]\n",
      "14 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.680, 0.625)] [G loss: 0.0] [G acc: 0.117]\n",
      "15 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.570, 0.469, 0.520)] [G loss: 0.0] [G acc: 0.156]\n",
      "16 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.617, 0.570)] [G loss: 0.0] [G acc: 0.141]\n",
      "17 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.617, 0.539)] [G loss: 0.0] [G acc: 0.062]\n",
      "18 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.477, 0.789, 0.633)] [G loss: 0.0] [G acc: 0.164]\n",
      "19 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.516, 0.535)] [G loss: 0.0] [G acc: 0.133]\n",
      "20 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.906, 0.723)] [G loss: 0.0] [G acc: 0.328]\n",
      "21 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.484, 0.543)] [G loss: 0.0] [G acc: 0.430]\n",
      "22 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.641, 0.562, 0.602)] [G loss: -0.0] [G acc: 0.594]\n",
      "23 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.672, 0.414, 0.543)] [G loss: -0.0] [G acc: 0.602]\n",
      "24 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.656, 0.500, 0.578)] [G loss: -0.0] [G acc: 0.789]\n",
      "25 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.695, 0.250, 0.473)] [G loss: -0.0] [G acc: 0.867]\n",
      "26 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.859, 0.203, 0.531)] [G loss: -0.0] [G acc: 0.852]\n",
      "27 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.859, 0.312, 0.586)] [G loss: -0.0] [G acc: 0.906]\n",
      "28 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.859, 0.180, 0.520)] [G loss: -0.0] [G acc: 0.859]\n",
      "29 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.953, 0.164, 0.559)] [G loss: -0.0] [G acc: 0.539]\n",
      "30 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.883, 0.250, 0.566)] [G loss: 0.0] [G acc: 0.367]\n",
      "31 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.859, 0.367, 0.613)] [G loss: 0.0] [G acc: 0.266]\n",
      "32 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.688, 0.445, 0.566)] [G loss: 0.0] [G acc: 0.344]\n",
      "33 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.680, 0.492, 0.586)] [G loss: 0.0] [G acc: 0.273]\n",
      "34 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.672, 0.539, 0.605)] [G loss: 0.0] [G acc: 0.250]\n",
      "35 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.586, 0.562)] [G loss: 0.0] [G acc: 0.180]\n",
      "36 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.719, 0.605)] [G loss: 0.0] [G acc: 0.195]\n",
      "37 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.469, 0.766, 0.617)] [G loss: 0.0] [G acc: 0.281]\n",
      "38 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.688, 0.559)] [G loss: 0.0] [G acc: 0.305]\n",
      "39 (5, 1) [D loss: (0.0, -0.0, 0.0)] [D acc: (0.336, 0.664, 0.500)] [G loss: 0.0] [G acc: 0.258]\n",
      "40 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.273, 0.773, 0.523)] [G loss: 0.0] [G acc: 0.188]\n",
      "41 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.289, 0.844, 0.566)] [G loss: 0.0] [G acc: 0.242]\n",
      "42 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.312, 0.734, 0.523)] [G loss: 0.0] [G acc: 0.266]\n",
      "43 (5, 1) [D loss: (0.0, -0.0, 0.0)] [D acc: (0.305, 0.672, 0.488)] [G loss: 0.0] [G acc: 0.281]\n",
      "44 (5, 1) [D loss: (0.0, -0.0, 0.0)] [D acc: (0.258, 0.711, 0.484)] [G loss: 0.0] [G acc: 0.281]\n",
      "45 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.227, 0.820, 0.523)] [G loss: 0.0] [G acc: 0.266]\n",
      "46 (5, 1) [D loss: (0.0, -0.0, 0.0)] [D acc: (0.289, 0.789, 0.539)] [G loss: 0.0] [G acc: 0.188]\n",
      "47 (5, 1) [D loss: (0.0, -0.0, 0.0)] [D acc: (0.117, 0.859, 0.488)] [G loss: 0.0] [G acc: 0.055]\n",
      "48 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.172, 0.922, 0.547)] [G loss: 0.0] [G acc: 0.102]\n",
      "49 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.227, 0.922, 0.574)] [G loss: 0.0] [G acc: 0.148]\n",
      "50 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.258, 0.875, 0.566)] [G loss: 0.0] [G acc: 0.156]\n",
      "51 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.375, 0.828, 0.602)] [G loss: 0.0] [G acc: 0.039]\n",
      "52 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.359, 0.742, 0.551)] [G loss: 0.0] [G acc: 0.070]\n",
      "53 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.766, 0.598)] [G loss: 0.0] [G acc: 0.164]\n",
      "54 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.680, 0.555)] [G loss: 0.0] [G acc: 0.156]\n",
      "55 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.758, 0.637)] [G loss: 0.0] [G acc: 0.148]\n",
      "56 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.641, 0.574)] [G loss: 0.0] [G acc: 0.125]\n",
      "57 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.695, 0.598)] [G loss: 0.0] [G acc: 0.109]\n",
      "58 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.594, 0.504)] [G loss: 0.0] [G acc: 0.117]\n",
      "59 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.805, 0.617)] [G loss: 0.0] [G acc: 0.203]\n",
      "60 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.508, 0.680, 0.594)] [G loss: 0.0] [G acc: 0.195]\n",
      "61 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.375, 0.781, 0.578)] [G loss: 0.0] [G acc: 0.156]\n",
      "62 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.305, 0.742, 0.523)] [G loss: 0.0] [G acc: 0.094]\n",
      "63 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.234, 0.805, 0.520)] [G loss: 0.0] [G acc: 0.109]\n",
      "64 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.359, 0.820, 0.590)] [G loss: 0.0] [G acc: 0.234]\n",
      "65 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.336, 0.805, 0.570)] [G loss: 0.0] [G acc: 0.273]\n",
      "66 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.312, 0.766, 0.539)] [G loss: 0.0] [G acc: 0.156]\n",
      "67 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.320, 0.859, 0.590)] [G loss: 0.0] [G acc: 0.039]\n",
      "68 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.242, 0.930, 0.586)] [G loss: 0.0] [G acc: 0.000]\n",
      "69 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.898, 0.660)] [G loss: 0.0] [G acc: 0.086]\n",
      "70 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.242, 0.992, 0.617)] [G loss: 0.0] [G acc: 0.000]\n",
      "71 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.234, 0.961, 0.598)] [G loss: 0.0] [G acc: 0.008]\n",
      "72 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.188, 0.977, 0.582)] [G loss: 0.0] [G acc: 0.000]\n",
      "73 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.156, 0.969, 0.562)] [G loss: 0.0] [G acc: 0.000]\n",
      "74 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.172, 0.969, 0.570)] [G loss: 0.0] [G acc: 0.008]\n",
      "75 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.289, 0.930, 0.609)] [G loss: 0.0] [G acc: 0.047]\n",
      "76 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.055, 1.000, 0.527)] [G loss: 0.0] [G acc: 0.000]\n",
      "77 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.039, 1.000, 0.520)] [G loss: 0.0] [G acc: 0.000]\n",
      "78 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.164, 1.000, 0.582)] [G loss: 0.1] [G acc: 0.008]\n",
      "79 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.109, 1.000, 0.555)] [G loss: 0.1] [G acc: 0.000]\n",
      "80 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.141, 0.992, 0.566)] [G loss: 0.1] [G acc: 0.031]\n",
      "81 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.078, 1.000, 0.539)] [G loss: 0.1] [G acc: 0.000]\n",
      "82 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.109, 1.000, 0.555)] [G loss: 0.1] [G acc: 0.000]\n",
      "83 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.156, 1.000, 0.578)] [G loss: 0.1] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.117, 0.969, 0.543)] [G loss: 0.1] [G acc: 0.055]\n",
      "85 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.102, 1.000, 0.551)] [G loss: 0.1] [G acc: 0.000]\n",
      "86 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.148, 0.984, 0.566)] [G loss: 0.1] [G acc: 0.156]\n",
      "87 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.148, 0.977, 0.562)] [G loss: 0.1] [G acc: 0.000]\n",
      "88 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.117, 0.992, 0.555)] [G loss: 0.1] [G acc: 0.000]\n",
      "89 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.188, 0.977, 0.582)] [G loss: 0.1] [G acc: 0.125]\n",
      "90 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.188, 0.969, 0.578)] [G loss: 0.1] [G acc: 0.008]\n",
      "91 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.172, 0.945, 0.559)] [G loss: 0.0] [G acc: 0.125]\n",
      "92 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.258, 0.992, 0.625)] [G loss: 0.1] [G acc: 0.000]\n",
      "93 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.297, 0.984, 0.641)] [G loss: 0.0] [G acc: 0.109]\n",
      "94 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.266, 0.992, 0.629)] [G loss: 0.1] [G acc: 0.000]\n",
      "95 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.289, 0.992, 0.641)] [G loss: 0.1] [G acc: 0.000]\n",
      "96 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.281, 1.000, 0.641)] [G loss: 0.1] [G acc: 0.000]\n",
      "97 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.312, 1.000, 0.656)] [G loss: 0.1] [G acc: 0.000]\n",
      "98 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.352, 0.953, 0.652)] [G loss: 0.1] [G acc: 0.000]\n",
      "99 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.992, 0.691)] [G loss: 0.0] [G acc: 0.062]\n",
      "100 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.953, 0.672)] [G loss: 0.1] [G acc: 0.008]\n",
      "101 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.297, 0.992, 0.645)] [G loss: 0.1] [G acc: 0.000]\n",
      "102 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.391, 0.977, 0.684)] [G loss: 0.1] [G acc: 0.000]\n",
      "103 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.969, 0.699)] [G loss: 0.1] [G acc: 0.000]\n",
      "104 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 1.000, 0.676)] [G loss: 0.1] [G acc: 0.000]\n",
      "105 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 1.000, 0.660)] [G loss: 0.1] [G acc: 0.000]\n",
      "106 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 1.000, 0.684)] [G loss: 0.1] [G acc: 0.000]\n",
      "107 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 1.000, 0.668)] [G loss: 0.1] [G acc: 0.000]\n",
      "108 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.508, 0.953, 0.730)] [G loss: 0.1] [G acc: 0.016]\n",
      "109 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.914, 0.668)] [G loss: 0.1] [G acc: 0.000]\n",
      "110 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.461, 1.000, 0.730)] [G loss: 0.1] [G acc: 0.000]\n",
      "111 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.477, 0.977, 0.727)] [G loss: 0.1] [G acc: 0.000]\n",
      "112 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.992, 0.695)] [G loss: 0.1] [G acc: 0.000]\n",
      "113 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.586, 0.891, 0.738)] [G loss: 0.1] [G acc: 0.047]\n",
      "114 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.477, 0.789, 0.633)] [G loss: 0.1] [G acc: 0.000]\n",
      "115 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.906, 0.738)] [G loss: 0.1] [G acc: 0.000]\n",
      "116 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.828, 0.703)] [G loss: 0.1] [G acc: 0.000]\n",
      "117 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.625, 0.781, 0.703)] [G loss: 0.1] [G acc: 0.039]\n",
      "118 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.781, 0.680)] [G loss: 0.0] [G acc: 0.055]\n",
      "119 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.820, 0.820, 0.820)] [G loss: 0.1] [G acc: 0.023]\n",
      "120 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.797, 0.797, 0.797)] [G loss: 0.1] [G acc: 0.133]\n",
      "121 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.672, 0.844, 0.758)] [G loss: 0.1] [G acc: 0.000]\n",
      "122 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.711, 0.703, 0.707)] [G loss: 0.1] [G acc: 0.039]\n",
      "123 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.852, 0.602, 0.727)] [G loss: 0.1] [G acc: 0.008]\n",
      "124 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.789, 0.898, 0.844)] [G loss: 0.1] [G acc: 0.016]\n",
      "125 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.633, 0.914, 0.773)] [G loss: 0.1] [G acc: 0.094]\n",
      "126 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.875, 0.730)] [G loss: 0.1] [G acc: 0.141]\n",
      "127 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.625, 0.906, 0.766)] [G loss: 0.1] [G acc: 0.109]\n",
      "128 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.641, 0.945, 0.793)] [G loss: 0.1] [G acc: 0.039]\n",
      "129 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.523, 0.906, 0.715)] [G loss: 0.1] [G acc: 0.000]\n",
      "130 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.539, 0.914, 0.727)] [G loss: 0.1] [G acc: 0.008]\n",
      "131 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.555, 0.930, 0.742)] [G loss: 0.1] [G acc: 0.000]\n",
      "132 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.453, 0.961, 0.707)] [G loss: 0.1] [G acc: 0.102]\n",
      "133 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.617, 0.828, 0.723)] [G loss: 0.1] [G acc: 0.266]\n",
      "134 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.555, 0.852, 0.703)] [G loss: 0.1] [G acc: 0.055]\n",
      "135 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.539, 0.867, 0.703)] [G loss: 0.0] [G acc: 0.383]\n",
      "136 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.766, 0.656)] [G loss: 0.0] [G acc: 0.289]\n",
      "137 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.719, 0.633, 0.676)] [G loss: 0.1] [G acc: 0.148]\n",
      "138 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.625, 0.766, 0.695)] [G loss: 0.1] [G acc: 0.016]\n",
      "139 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.625, 0.812, 0.719)] [G loss: 0.1] [G acc: 0.008]\n",
      "140 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.625, 0.656, 0.641)] [G loss: 0.1] [G acc: 0.016]\n",
      "141 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.672, 0.629)] [G loss: 0.1] [G acc: 0.016]\n",
      "142 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.750, 0.516, 0.633)] [G loss: 0.1] [G acc: 0.086]\n",
      "143 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.781, 0.484, 0.633)] [G loss: 0.1] [G acc: 0.078]\n",
      "144 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.820, 0.492, 0.656)] [G loss: 0.1] [G acc: 0.164]\n",
      "145 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.719, 0.609, 0.664)] [G loss: 0.1] [G acc: 0.047]\n",
      "146 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.688, 0.633, 0.660)] [G loss: 0.1] [G acc: 0.039]\n",
      "147 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.758, 0.727, 0.742)] [G loss: 0.1] [G acc: 0.109]\n",
      "148 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.719, 0.797, 0.758)] [G loss: 0.1] [G acc: 0.133]\n",
      "149 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.688, 0.820, 0.754)] [G loss: 0.1] [G acc: 0.117]\n",
      "150 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.570, 0.898, 0.734)] [G loss: 0.1] [G acc: 0.008]\n",
      "151 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.695, 0.797, 0.746)] [G loss: 0.1] [G acc: 0.008]\n",
      "152 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.766, 0.586, 0.676)] [G loss: 0.1] [G acc: 0.047]\n",
      "153 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.805, 0.695, 0.750)] [G loss: 0.1] [G acc: 0.133]\n",
      "154 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.773, 0.594, 0.684)] [G loss: 0.1] [G acc: 0.312]\n",
      "155 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.773, 0.516, 0.645)] [G loss: 0.1] [G acc: 0.344]\n",
      "156 (5, 1) [D loss: (-0.1, 0.0, -0.1)] [D acc: (0.797, 0.445, 0.621)] [G loss: 0.0] [G acc: 0.383]\n",
      "157 (5, 1) [D loss: (-0.2, 0.0, -0.1)] [D acc: (0.859, 0.430, 0.645)] [G loss: 0.0] [G acc: 0.445]\n",
      "158 (5, 1) [D loss: (-0.2, 0.0, -0.1)] [D acc: (0.898, 0.414, 0.656)] [G loss: -0.0] [G acc: 0.609]\n",
      "159 (5, 1) [D loss: (-0.2, 0.1, -0.1)] [D acc: (0.859, 0.359, 0.609)] [G loss: -0.0] [G acc: 0.516]\n",
      "160 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.859, 0.328, 0.594)] [G loss: -0.0] [G acc: 0.523]\n",
      "161 (5, 1) [D loss: (-0.1, 0.1, -0.0)] [D acc: (0.883, 0.250, 0.566)] [G loss: 0.0] [G acc: 0.422]\n",
      "162 (5, 1) [D loss: (-0.1, 0.0, -0.1)] [D acc: (0.734, 0.523, 0.629)] [G loss: 0.1] [G acc: 0.195]\n",
      "163 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.586, 0.711, 0.648)] [G loss: 0.1] [G acc: 0.289]\n",
      "164 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.531, 0.688, 0.609)] [G loss: 0.1] [G acc: 0.297]\n",
      "165 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.555, 0.734, 0.645)] [G loss: 0.1] [G acc: 0.289]\n",
      "166 (5, 1) [D loss: (-0.2, -0.0, -0.1)] [D acc: (0.625, 0.695, 0.660)] [G loss: 0.1] [G acc: 0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.469, 0.695, 0.582)] [G loss: 0.3] [G acc: 0.047]\n",
      "168 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.531, 0.570, 0.551)] [G loss: 0.3] [G acc: 0.258]\n",
      "169 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.594, 0.555, 0.574)] [G loss: 0.2] [G acc: 0.375]\n",
      "170 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.719, 0.547, 0.633)] [G loss: 0.1] [G acc: 0.492]\n",
      "171 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.711, 0.406, 0.559)] [G loss: -0.0] [G acc: 0.602]\n",
      "172 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.734, 0.422, 0.578)] [G loss: -0.1] [G acc: 0.625]\n",
      "173 (5, 1) [D loss: (-0.2, 0.0, -0.1)] [D acc: (0.859, 0.375, 0.617)] [G loss: 0.0] [G acc: 0.398]\n",
      "174 (5, 1) [D loss: (-0.1, 0.0, -0.1)] [D acc: (0.836, 0.453, 0.645)] [G loss: 0.1] [G acc: 0.109]\n",
      "175 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.703, 0.648, 0.676)] [G loss: 0.2] [G acc: 0.188]\n",
      "176 (5, 1) [D loss: (-0.2, 0.0, -0.1)] [D acc: (0.688, 0.586, 0.637)] [G loss: 0.1] [G acc: 0.312]\n",
      "177 (5, 1) [D loss: (-0.1, 0.0, -0.1)] [D acc: (0.672, 0.586, 0.629)] [G loss: 0.1] [G acc: 0.289]\n",
      "178 (5, 1) [D loss: (-0.1, 0.1, -0.0)] [D acc: (0.586, 0.547, 0.566)] [G loss: 0.0] [G acc: 0.375]\n",
      "179 (5, 1) [D loss: (-0.1, 0.1, -0.0)] [D acc: (0.711, 0.422, 0.566)] [G loss: 0.0] [G acc: 0.391]\n",
      "180 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.617, 0.492, 0.555)] [G loss: 0.1] [G acc: 0.203]\n",
      "181 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.586, 0.578)] [G loss: 0.2] [G acc: 0.117]\n",
      "182 (5, 1) [D loss: (0.0, -0.2, -0.1)] [D acc: (0.422, 0.734, 0.578)] [G loss: 0.4] [G acc: 0.070]\n",
      "183 (5, 1) [D loss: (0.0, -0.3, -0.1)] [D acc: (0.469, 0.906, 0.688)] [G loss: 0.5] [G acc: 0.047]\n",
      "184 (5, 1) [D loss: (0.0, -0.4, -0.2)] [D acc: (0.477, 0.891, 0.684)] [G loss: 0.5] [G acc: 0.094]\n",
      "185 (5, 1) [D loss: (0.1, -0.2, -0.1)] [D acc: (0.469, 0.695, 0.582)] [G loss: 0.3] [G acc: 0.211]\n",
      "186 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.445, 0.703, 0.574)] [G loss: 0.2] [G acc: 0.352]\n",
      "187 (5, 1) [D loss: (0.1, -0.1, 0.0)] [D acc: (0.383, 0.555, 0.469)] [G loss: 0.0] [G acc: 0.516]\n",
      "188 (5, 1) [D loss: (0.1, -0.0, 0.0)] [D acc: (0.461, 0.570, 0.516)] [G loss: -0.0] [G acc: 0.523]\n",
      "189 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.680, 0.414, 0.547)] [G loss: 0.0] [G acc: 0.477]\n",
      "190 (5, 1) [D loss: (-0.2, -0.0, -0.1)] [D acc: (0.898, 0.539, 0.719)] [G loss: 0.2] [G acc: 0.156]\n",
      "191 (5, 1) [D loss: (-0.1, -0.2, -0.2)] [D acc: (0.672, 0.891, 0.781)] [G loss: 0.4] [G acc: 0.000]\n",
      "192 (5, 1) [D loss: (-0.2, -0.3, -0.2)] [D acc: (0.773, 0.922, 0.848)] [G loss: 0.5] [G acc: 0.000]\n",
      "193 (5, 1) [D loss: (-0.2, -0.3, -0.3)] [D acc: (0.742, 0.883, 0.812)] [G loss: 0.6] [G acc: 0.023]\n",
      "194 (5, 1) [D loss: (-0.2, -0.1, -0.2)] [D acc: (0.656, 0.656, 0.656)] [G loss: 0.4] [G acc: 0.141]\n",
      "195 (5, 1) [D loss: (-0.2, 0.1, -0.0)] [D acc: (0.703, 0.469, 0.586)] [G loss: 0.1] [G acc: 0.352]\n",
      "196 (5, 1) [D loss: (-0.2, 0.2, -0.0)] [D acc: (0.820, 0.328, 0.574)] [G loss: -0.0] [G acc: 0.508]\n",
      "197 (5, 1) [D loss: (-0.3, 0.3, 0.0)] [D acc: (0.820, 0.227, 0.523)] [G loss: -0.1] [G acc: 0.570]\n",
      "198 (5, 1) [D loss: (-0.3, 0.4, 0.0)] [D acc: (0.906, 0.133, 0.520)] [G loss: -0.3] [G acc: 0.820]\n",
      "199 (5, 1) [D loss: (-0.3, 0.3, 0.0)] [D acc: (0.836, 0.234, 0.535)] [G loss: -0.3] [G acc: 0.805]\n",
      "200 (5, 1) [D loss: (-0.2, 0.2, 0.0)] [D acc: (0.719, 0.289, 0.504)] [G loss: -0.2] [G acc: 0.727]\n",
      "201 (5, 1) [D loss: (-0.2, -0.1, -0.1)] [D acc: (0.836, 0.625, 0.730)] [G loss: 0.1] [G acc: 0.430]\n",
      "202 (5, 1) [D loss: (-0.1, -0.2, -0.2)] [D acc: (0.656, 0.734, 0.695)] [G loss: 0.4] [G acc: 0.172]\n",
      "203 (5, 1) [D loss: (-0.1, -0.4, -0.3)] [D acc: (0.625, 0.805, 0.715)] [G loss: 0.7] [G acc: 0.023]\n",
      "204 (5, 1) [D loss: (-0.0, -0.4, -0.2)] [D acc: (0.508, 0.852, 0.680)] [G loss: 0.7] [G acc: 0.062]\n",
      "205 (5, 1) [D loss: (0.1, -0.3, -0.1)] [D acc: (0.398, 0.727, 0.562)] [G loss: 0.4] [G acc: 0.203]\n",
      "206 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.516, 0.648, 0.582)] [G loss: 0.2] [G acc: 0.367]\n",
      "207 (5, 1) [D loss: (0.2, 0.1, 0.1)] [D acc: (0.344, 0.531, 0.438)] [G loss: -0.0] [G acc: 0.477]\n",
      "208 (5, 1) [D loss: (-0.0, 0.1, 0.0)] [D acc: (0.500, 0.562, 0.531)] [G loss: -0.1] [G acc: 0.516]\n",
      "209 (5, 1) [D loss: (-0.0, 0.1, 0.1)] [D acc: (0.539, 0.500, 0.520)] [G loss: -0.0] [G acc: 0.469]\n",
      "210 (5, 1) [D loss: (0.0, 0.2, 0.1)] [D acc: (0.445, 0.391, 0.418)] [G loss: 0.1] [G acc: 0.359]\n",
      "211 (5, 1) [D loss: (-0.0, -0.3, -0.2)] [D acc: (0.602, 0.922, 0.762)] [G loss: 0.5] [G acc: 0.039]\n",
      "212 (5, 1) [D loss: (-0.1, -0.5, -0.3)] [D acc: (0.680, 0.992, 0.836)] [G loss: 0.8] [G acc: 0.000]\n",
      "213 (5, 1) [D loss: (-0.2, -0.7, -0.4)] [D acc: (0.648, 1.000, 0.824)] [G loss: 0.9] [G acc: 0.000]\n",
      "214 (5, 1) [D loss: (-0.1, -0.6, -0.4)] [D acc: (0.602, 0.984, 0.793)] [G loss: 0.9] [G acc: 0.008]\n",
      "215 (5, 1) [D loss: (-0.1, -0.4, -0.3)] [D acc: (0.578, 0.766, 0.672)] [G loss: 0.7] [G acc: 0.070]\n",
      "216 (5, 1) [D loss: (-0.1, 0.1, -0.0)] [D acc: (0.609, 0.391, 0.500)] [G loss: 0.2] [G acc: 0.445]\n",
      "217 (5, 1) [D loss: (-0.1, 0.2, 0.1)] [D acc: (0.625, 0.312, 0.469)] [G loss: -0.1] [G acc: 0.664]\n",
      "218 (5, 1) [D loss: (-0.1, 0.4, 0.1)] [D acc: (0.641, 0.195, 0.418)] [G loss: -0.3] [G acc: 0.773]\n",
      "219 (5, 1) [D loss: (-0.3, 0.5, 0.1)] [D acc: (0.828, 0.086, 0.457)] [G loss: -0.4] [G acc: 0.852]\n",
      "220 (5, 1) [D loss: (-0.3, 0.4, 0.0)] [D acc: (0.859, 0.062, 0.461)] [G loss: -0.3] [G acc: 0.734]\n",
      "221 (5, 1) [D loss: (-0.4, 0.2, -0.1)] [D acc: (0.961, 0.281, 0.621)] [G loss: 0.1] [G acc: 0.336]\n",
      "222 (5, 1) [D loss: (-0.2, -0.5, -0.4)] [D acc: (0.625, 1.000, 0.812)] [G loss: 0.8] [G acc: 0.000]\n",
      "223 (5, 1) [D loss: (-0.1, -1.0, -0.5)] [D acc: (0.555, 1.000, 0.777)] [G loss: 1.2] [G acc: 0.000]\n",
      "224 (5, 1) [D loss: (-0.2, -1.0, -0.6)] [D acc: (0.602, 0.969, 0.785)] [G loss: 1.3] [G acc: 0.000]\n",
      "225 (5, 1) [D loss: (-0.3, -0.8, -0.5)] [D acc: (0.641, 0.883, 0.762)] [G loss: 0.9] [G acc: 0.180]\n",
      "226 (5, 1) [D loss: (0.1, -0.5, -0.2)] [D acc: (0.500, 0.742, 0.621)] [G loss: 0.4] [G acc: 0.320]\n",
      "227 (5, 1) [D loss: (0.1, 0.2, 0.1)] [D acc: (0.477, 0.492, 0.484)] [G loss: -0.1] [G acc: 0.516]\n",
      "228 (5, 1) [D loss: (0.1, 0.4, 0.3)] [D acc: (0.484, 0.367, 0.426)] [G loss: -0.4] [G acc: 0.609]\n",
      "229 (5, 1) [D loss: (0.0, 0.6, 0.3)] [D acc: (0.477, 0.266, 0.371)] [G loss: -0.5] [G acc: 0.688]\n",
      "230 (5, 1) [D loss: (-0.0, 0.6, 0.3)] [D acc: (0.547, 0.180, 0.363)] [G loss: -0.5] [G acc: 0.719]\n",
      "231 (5, 1) [D loss: (-0.3, 0.8, 0.2)] [D acc: (0.648, 0.117, 0.383)] [G loss: -0.4] [G acc: 0.719]\n",
      "232 (5, 1) [D loss: (-0.4, 0.4, -0.0)] [D acc: (0.828, 0.320, 0.574)] [G loss: -0.0] [G acc: 0.484]\n",
      "233 (5, 1) [D loss: (-0.4, -0.2, -0.3)] [D acc: (0.766, 0.672, 0.719)] [G loss: 0.6] [G acc: 0.164]\n",
      "234 (5, 1) [D loss: (-0.2, -0.9, -0.5)] [D acc: (0.703, 0.914, 0.809)] [G loss: 1.2] [G acc: 0.016]\n",
      "235 (5, 1) [D loss: (-0.2, -1.2, -0.7)] [D acc: (0.664, 0.953, 0.809)] [G loss: 1.6] [G acc: 0.016]\n",
      "236 (5, 1) [D loss: (0.0, -1.2, -0.6)] [D acc: (0.477, 0.914, 0.695)] [G loss: 1.8] [G acc: 0.008]\n",
      "237 (5, 1) [D loss: (0.2, -1.2, -0.5)] [D acc: (0.453, 0.938, 0.695)] [G loss: 1.7] [G acc: 0.000]\n",
      "238 (5, 1) [D loss: (0.7, -1.2, -0.2)] [D acc: (0.250, 1.000, 0.625)] [G loss: 1.4] [G acc: 0.000]\n",
      "239 (5, 1) [D loss: (0.9, -1.3, -0.2)] [D acc: (0.164, 0.992, 0.578)] [G loss: 1.0] [G acc: 0.055]\n",
      "240 (5, 1) [D loss: (1.4, -1.1, 0.1)] [D acc: (0.102, 0.992, 0.547)] [G loss: 0.6] [G acc: 0.312]\n",
      "241 (5, 1) [D loss: (1.7, -1.0, 0.3)] [D acc: (0.039, 0.961, 0.500)] [G loss: 0.7] [G acc: 0.234]\n",
      "242 (5, 1) [D loss: (1.7, -1.1, 0.3)] [D acc: (0.008, 0.977, 0.492)] [G loss: 1.0] [G acc: 0.055]\n",
      "243 (5, 1) [D loss: (1.7, -1.4, 0.2)] [D acc: (0.000, 1.000, 0.500)] [G loss: 1.4] [G acc: 0.008]\n",
      "244 (5, 1) [D loss: (1.6, -1.6, 0.0)] [D acc: (0.000, 1.000, 0.500)] [G loss: 1.7] [G acc: 0.000]\n",
      "245 (5, 1) [D loss: (1.4, -1.9, -0.2)] [D acc: (0.008, 1.000, 0.504)] [G loss: 2.2] [G acc: 0.000]\n",
      "246 (5, 1) [D loss: (1.3, -2.3, -0.5)] [D acc: (0.062, 1.000, 0.531)] [G loss: 2.7] [G acc: 0.000]\n",
      "247 (5, 1) [D loss: (1.3, -2.8, -0.8)] [D acc: (0.117, 1.000, 0.559)] [G loss: 3.3] [G acc: 0.000]\n",
      "248 (5, 1) [D loss: (1.0, -3.1, -1.1)] [D acc: (0.305, 1.000, 0.652)] [G loss: 3.5] [G acc: 0.000]\n",
      "249 (5, 1) [D loss: (0.5, -2.9, -1.2)] [D acc: (0.367, 1.000, 0.684)] [G loss: 3.2] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 (5, 1) [D loss: (0.2, -2.4, -1.1)] [D acc: (0.492, 0.992, 0.742)] [G loss: 2.5] [G acc: 0.008]\n",
      "251 (5, 1) [D loss: (-0.1, -1.1, -0.6)] [D acc: (0.578, 0.672, 0.625)] [G loss: 0.8] [G acc: 0.398]\n",
      "252 (5, 1) [D loss: (-0.1, 0.4, 0.2)] [D acc: (0.547, 0.375, 0.461)] [G loss: -0.3] [G acc: 0.570]\n",
      "253 (5, 1) [D loss: (-0.6, 1.6, 0.5)] [D acc: (0.727, 0.055, 0.391)] [G loss: -1.3] [G acc: 0.859]\n",
      "254 (5, 1) [D loss: (-1.2, 1.8, 0.3)] [D acc: (0.914, 0.000, 0.457)] [G loss: -1.7] [G acc: 1.000]\n",
      "255 (5, 1) [D loss: (-1.3, 1.9, 0.3)] [D acc: (0.953, 0.000, 0.477)] [G loss: -1.8] [G acc: 1.000]\n",
      "256 (5, 1) [D loss: (-1.6, 1.9, 0.2)] [D acc: (0.984, 0.000, 0.492)] [G loss: -1.7] [G acc: 1.000]\n",
      "257 (5, 1) [D loss: (-1.6, 1.8, 0.1)] [D acc: (1.000, 0.000, 0.500)] [G loss: -1.6] [G acc: 1.000]\n",
      "258 (5, 1) [D loss: (-1.7, 1.7, 0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -1.4] [G acc: 1.000]\n",
      "259 (5, 1) [D loss: (-1.6, 1.6, -0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -1.4] [G acc: 1.000]\n",
      "260 (5, 1) [D loss: (-1.7, 1.6, -0.1)] [D acc: (1.000, 0.000, 0.500)] [G loss: -1.3] [G acc: 1.000]\n",
      "261 (5, 1) [D loss: (-1.7, 1.7, 0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -1.2] [G acc: 0.992]\n",
      "262 (5, 1) [D loss: (-1.6, 1.5, -0.0)] [D acc: (1.000, 0.000, 0.500)] [G loss: -1.0] [G acc: 0.977]\n",
      "263 (5, 1) [D loss: (-1.4, 1.4, -0.0)] [D acc: (1.000, 0.047, 0.523)] [G loss: -0.7] [G acc: 0.688]\n",
      "264 (5, 1) [D loss: (-1.1, 0.9, -0.1)] [D acc: (0.945, 0.180, 0.562)] [G loss: -0.3] [G acc: 0.516]\n",
      "265 (5, 1) [D loss: (-0.7, 0.8, 0.0)] [D acc: (0.805, 0.359, 0.582)] [G loss: 0.1] [G acc: 0.445]\n",
      "266 (5, 1) [D loss: (-0.4, 0.5, 0.0)] [D acc: (0.711, 0.367, 0.539)] [G loss: 0.4] [G acc: 0.445]\n",
      "267 (5, 1) [D loss: (-0.3, 0.3, 0.0)] [D acc: (0.617, 0.406, 0.512)] [G loss: 0.5] [G acc: 0.445]\n",
      "268 (5, 1) [D loss: (-0.1, 0.2, 0.0)] [D acc: (0.625, 0.375, 0.500)] [G loss: 0.6] [G acc: 0.492]\n",
      "269 (5, 1) [D loss: (-0.1, 0.2, 0.0)] [D acc: (0.656, 0.367, 0.512)] [G loss: 0.6] [G acc: 0.531]\n",
      "270 (5, 1) [D loss: (-0.1, 0.1, 0.0)] [D acc: (0.641, 0.344, 0.492)] [G loss: 0.4] [G acc: 0.523]\n",
      "271 (5, 1) [D loss: (0.0, 0.3, 0.1)] [D acc: (0.617, 0.266, 0.441)] [G loss: 0.1] [G acc: 0.531]\n",
      "272 (5, 1) [D loss: (-0.3, 0.3, -0.0)] [D acc: (0.727, 0.367, 0.547)] [G loss: -0.3] [G acc: 0.562]\n",
      "273 (5, 1) [D loss: (-0.3, 0.3, 0.0)] [D acc: (0.570, 0.555, 0.562)] [G loss: -0.3] [G acc: 0.445]\n",
      "274 (5, 1) [D loss: (-0.3, 0.4, 0.1)] [D acc: (0.594, 0.539, 0.566)] [G loss: -0.1] [G acc: 0.445]\n",
      "275 (5, 1) [D loss: (-0.2, -0.1, -0.1)] [D acc: (0.656, 0.570, 0.613)] [G loss: 0.4] [G acc: 0.133]\n",
      "276 (5, 1) [D loss: (0.1, -0.4, -0.2)] [D acc: (0.453, 0.820, 0.637)] [G loss: 0.8] [G acc: 0.047]\n",
      "277 (5, 1) [D loss: (0.2, -0.6, -0.2)] [D acc: (0.398, 0.805, 0.602)] [G loss: 1.2] [G acc: 0.086]\n",
      "278 (5, 1) [D loss: (0.4, -0.5, -0.1)] [D acc: (0.359, 0.547, 0.453)] [G loss: 1.0] [G acc: 0.250]\n",
      "279 (5, 1) [D loss: (0.4, -0.4, 0.0)] [D acc: (0.359, 0.516, 0.438)] [G loss: 0.6] [G acc: 0.344]\n",
      "280 (5, 1) [D loss: (0.3, -0.1, 0.1)] [D acc: (0.438, 0.344, 0.391)] [G loss: 0.2] [G acc: 0.508]\n",
      "281 (5, 1) [D loss: (0.0, 0.3, 0.2)] [D acc: (0.539, 0.281, 0.410)] [G loss: -0.1] [G acc: 0.617]\n",
      "282 (5, 1) [D loss: (-0.2, 0.3, 0.0)] [D acc: (0.680, 0.281, 0.480)] [G loss: -0.2] [G acc: 0.633]\n",
      "283 (5, 1) [D loss: (-0.3, 0.5, 0.1)] [D acc: (0.672, 0.219, 0.445)] [G loss: -0.2] [G acc: 0.625]\n",
      "284 (5, 1) [D loss: (-0.4, 0.4, 0.0)] [D acc: (0.828, 0.148, 0.488)] [G loss: -0.3] [G acc: 0.727]\n",
      "285 (5, 1) [D loss: (-0.4, 0.3, -0.0)] [D acc: (0.930, 0.094, 0.512)] [G loss: -0.2] [G acc: 0.734]\n",
      "286 (5, 1) [D loss: (-0.4, 0.3, -0.0)] [D acc: (0.859, 0.156, 0.508)] [G loss: -0.2] [G acc: 0.609]\n",
      "287 (5, 1) [D loss: (-0.4, 0.2, -0.1)] [D acc: (0.781, 0.406, 0.594)] [G loss: 0.1] [G acc: 0.367]\n",
      "288 (5, 1) [D loss: (-0.2, 0.0, -0.1)] [D acc: (0.711, 0.484, 0.598)] [G loss: 0.3] [G acc: 0.289]\n",
      "289 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.531, 0.609, 0.570)] [G loss: 0.4] [G acc: 0.289]\n",
      "290 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.594, 0.520)] [G loss: 0.4] [G acc: 0.227]\n",
      "291 (5, 1) [D loss: (0.1, -0.2, -0.1)] [D acc: (0.422, 0.719, 0.570)] [G loss: 0.2] [G acc: 0.422]\n",
      "292 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.281, 0.656, 0.469)] [G loss: 0.0] [G acc: 0.453]\n",
      "293 (5, 1) [D loss: (0.1, -0.1, 0.0)] [D acc: (0.367, 0.516, 0.441)] [G loss: 0.0] [G acc: 0.523]\n",
      "294 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.406, 0.586, 0.496)] [G loss: 0.4] [G acc: 0.312]\n",
      "295 (5, 1) [D loss: (0.3, -0.7, -0.2)] [D acc: (0.227, 0.977, 0.602)] [G loss: 1.2] [G acc: 0.000]\n",
      "296 (5, 1) [D loss: (0.5, -1.4, -0.4)] [D acc: (0.117, 1.000, 0.559)] [G loss: 2.0] [G acc: 0.000]\n",
      "297 (5, 1) [D loss: (0.7, -1.7, -0.5)] [D acc: (0.133, 1.000, 0.566)] [G loss: 2.3] [G acc: 0.000]\n",
      "298 (5, 1) [D loss: (0.7, -1.8, -0.5)] [D acc: (0.164, 0.969, 0.566)] [G loss: 2.1] [G acc: 0.000]\n",
      "299 (5, 1) [D loss: (0.9, -1.2, -0.1)] [D acc: (0.156, 0.812, 0.484)] [G loss: 1.5] [G acc: 0.047]\n",
      "300 (5, 1) [D loss: (0.9, -1.0, -0.0)] [D acc: (0.250, 0.672, 0.461)] [G loss: 1.0] [G acc: 0.305]\n",
      "301 (5, 1) [D loss: (0.5, -0.4, 0.0)] [D acc: (0.359, 0.578, 0.469)] [G loss: 0.5] [G acc: 0.375]\n",
      "302 (5, 1) [D loss: (0.2, -0.1, 0.1)] [D acc: (0.484, 0.453, 0.469)] [G loss: 0.2] [G acc: 0.328]\n",
      "303 (5, 1) [D loss: (-0.2, -0.1, -0.2)] [D acc: (0.688, 0.711, 0.699)] [G loss: 0.4] [G acc: 0.117]\n",
      "304 (5, 1) [D loss: (-0.1, -0.5, -0.3)] [D acc: (0.570, 0.883, 0.727)] [G loss: 0.6] [G acc: 0.156]\n",
      "305 (5, 1) [D loss: (-0.1, -0.6, -0.4)] [D acc: (0.547, 0.828, 0.688)] [G loss: 0.7] [G acc: 0.156]\n",
      "306 (5, 1) [D loss: (-0.2, -0.4, -0.3)] [D acc: (0.523, 0.672, 0.598)] [G loss: 0.7] [G acc: 0.250]\n",
      "307 (5, 1) [D loss: (-0.3, -0.4, -0.4)] [D acc: (0.586, 0.680, 0.633)] [G loss: 0.5] [G acc: 0.336]\n",
      "308 (5, 1) [D loss: (0.0, -0.4, -0.2)] [D acc: (0.500, 0.594, 0.547)] [G loss: 0.2] [G acc: 0.508]\n",
      "309 (5, 1) [D loss: (-0.2, -0.2, -0.2)] [D acc: (0.625, 0.523, 0.574)] [G loss: -0.0] [G acc: 0.547]\n",
      "310 (5, 1) [D loss: (0.1, 0.4, 0.2)] [D acc: (0.477, 0.344, 0.410)] [G loss: -0.3] [G acc: 0.664]\n",
      "311 (5, 1) [D loss: (-0.1, 0.6, 0.3)] [D acc: (0.547, 0.266, 0.406)] [G loss: -0.6] [G acc: 0.703]\n",
      "312 (5, 1) [D loss: (-0.2, 0.5, 0.2)] [D acc: (0.633, 0.227, 0.430)] [G loss: -0.7] [G acc: 0.797]\n",
      "313 (5, 1) [D loss: (-0.3, 0.7, 0.2)] [D acc: (0.828, 0.117, 0.473)] [G loss: -0.5] [G acc: 0.812]\n",
      "314 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.656, 0.672, 0.664)] [G loss: 0.9] [G acc: 0.078]\n",
      "315 (5, 1) [D loss: (0.3, -1.3, -0.5)] [D acc: (0.359, 0.930, 0.645)] [G loss: 2.3] [G acc: 0.008]\n",
      "316 (5, 1) [D loss: (0.4, -1.5, -0.6)] [D acc: (0.422, 0.945, 0.684)] [G loss: 2.7] [G acc: 0.000]\n",
      "317 (5, 1) [D loss: (0.6, -1.4, -0.4)] [D acc: (0.297, 0.898, 0.598)] [G loss: 2.3] [G acc: 0.102]\n",
      "318 (5, 1) [D loss: (0.6, -0.7, -0.0)] [D acc: (0.391, 0.664, 0.527)] [G loss: 1.6] [G acc: 0.141]\n",
      "319 (5, 1) [D loss: (0.4, -0.1, 0.1)] [D acc: (0.461, 0.445, 0.453)] [G loss: 0.7] [G acc: 0.305]\n",
      "320 (5, 1) [D loss: (0.1, 0.3, 0.2)] [D acc: (0.516, 0.234, 0.375)] [G loss: 0.2] [G acc: 0.391]\n",
      "321 (5, 1) [D loss: (-0.4, 0.3, -0.0)] [D acc: (0.930, 0.016, 0.473)] [G loss: -0.1] [G acc: 0.719]\n",
      "322 (5, 1) [D loss: (-0.5, 0.4, -0.1)] [D acc: (1.000, 0.000, 0.500)] [G loss: -0.2] [G acc: 0.914]\n",
      "323 (5, 1) [D loss: (-0.6, 0.3, -0.2)] [D acc: (1.000, 0.016, 0.508)] [G loss: -0.1] [G acc: 0.703]\n",
      "324 (5, 1) [D loss: (-0.6, 0.1, -0.2)] [D acc: (0.891, 0.391, 0.641)] [G loss: 0.3] [G acc: 0.219]\n",
      "325 (5, 1) [D loss: (-0.3, -0.1, -0.2)] [D acc: (0.648, 0.570, 0.609)] [G loss: 0.2] [G acc: 0.391]\n",
      "326 (5, 1) [D loss: (-0.3, 0.1, -0.1)] [D acc: (0.641, 0.555, 0.598)] [G loss: 0.0] [G acc: 0.492]\n",
      "327 (5, 1) [D loss: (-0.4, 0.2, -0.1)] [D acc: (0.664, 0.453, 0.559)] [G loss: -0.1] [G acc: 0.516]\n",
      "328 (5, 1) [D loss: (-0.2, 0.2, 0.0)] [D acc: (0.609, 0.406, 0.508)] [G loss: -0.1] [G acc: 0.523]\n",
      "329 (5, 1) [D loss: (-0.2, 0.4, 0.1)] [D acc: (0.727, 0.258, 0.492)] [G loss: 0.1] [G acc: 0.375]\n",
      "330 (5, 1) [D loss: (0.0, -0.2, -0.1)] [D acc: (0.555, 0.562, 0.559)] [G loss: 0.7] [G acc: 0.148]\n",
      "331 (5, 1) [D loss: (0.3, -0.8, -0.2)] [D acc: (0.438, 0.594, 0.516)] [G loss: 1.3] [G acc: 0.266]\n",
      "332 (5, 1) [D loss: (0.4, -1.2, -0.4)] [D acc: (0.391, 0.812, 0.602)] [G loss: 1.8] [G acc: 0.086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333 (5, 1) [D loss: (0.5, -1.2, -0.4)] [D acc: (0.359, 0.742, 0.551)] [G loss: 1.8] [G acc: 0.141]\n",
      "334 (5, 1) [D loss: (0.6, -1.1, -0.2)] [D acc: (0.383, 0.703, 0.543)] [G loss: 1.5] [G acc: 0.266]\n",
      "335 (5, 1) [D loss: (0.3, -0.7, -0.2)] [D acc: (0.367, 0.719, 0.543)] [G loss: 0.9] [G acc: 0.133]\n",
      "336 (5, 1) [D loss: (0.3, -0.4, -0.1)] [D acc: (0.469, 0.742, 0.605)] [G loss: 0.5] [G acc: 0.203]\n",
      "337 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.461, 0.625, 0.543)] [G loss: 0.1] [G acc: 0.461]\n",
      "338 (5, 1) [D loss: (-0.1, -0.2, -0.1)] [D acc: (0.625, 0.641, 0.633)] [G loss: 0.1] [G acc: 0.422]\n",
      "339 (5, 1) [D loss: (-0.2, 0.1, -0.1)] [D acc: (0.617, 0.492, 0.555)] [G loss: 0.1] [G acc: 0.398]\n",
      "340 (5, 1) [D loss: (-0.3, -0.0, -0.2)] [D acc: (0.836, 0.531, 0.684)] [G loss: 0.3] [G acc: 0.195]\n",
      "341 (5, 1) [D loss: (-0.4, -0.3, -0.4)] [D acc: (0.828, 0.648, 0.738)] [G loss: 0.7] [G acc: 0.109]\n",
      "342 (5, 1) [D loss: (-0.4, -0.4, -0.4)] [D acc: (0.820, 0.789, 0.805)] [G loss: 1.0] [G acc: 0.000]\n",
      "343 (5, 1) [D loss: (-0.5, -0.5, -0.5)] [D acc: (0.711, 0.891, 0.801)] [G loss: 1.2] [G acc: 0.000]\n",
      "344 (5, 1) [D loss: (-0.3, -0.6, -0.5)] [D acc: (0.578, 0.906, 0.742)] [G loss: 1.3] [G acc: 0.000]\n",
      "345 (5, 1) [D loss: (-0.3, -0.4, -0.4)] [D acc: (0.547, 0.781, 0.664)] [G loss: 1.0] [G acc: 0.055]\n",
      "346 (5, 1) [D loss: (-0.3, -0.3, -0.3)] [D acc: (0.586, 0.703, 0.645)] [G loss: 0.8] [G acc: 0.180]\n",
      "347 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.469, 0.648, 0.559)] [G loss: 0.4] [G acc: 0.289]\n",
      "348 (5, 1) [D loss: (0.0, 0.1, 0.0)] [D acc: (0.484, 0.484, 0.484)] [G loss: 0.3] [G acc: 0.344]\n",
      "349 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.406, 0.656, 0.531)] [G loss: 0.6] [G acc: 0.102]\n",
      "350 (5, 1) [D loss: (0.1, -0.3, -0.1)] [D acc: (0.398, 0.797, 0.598)] [G loss: 1.1] [G acc: 0.008]\n",
      "351 (5, 1) [D loss: (-0.0, -0.6, -0.3)] [D acc: (0.609, 0.711, 0.660)] [G loss: 1.3] [G acc: 0.102]\n",
      "352 (5, 1) [D loss: (-0.0, -0.6, -0.3)] [D acc: (0.641, 0.508, 0.574)] [G loss: 1.1] [G acc: 0.383]\n",
      "353 (5, 1) [D loss: (0.0, -0.5, -0.2)] [D acc: (0.594, 0.586, 0.590)] [G loss: 0.7] [G acc: 0.367]\n",
      "354 (5, 1) [D loss: (-0.2, -0.3, -0.2)] [D acc: (0.680, 0.570, 0.625)] [G loss: 0.2] [G acc: 0.531]\n",
      "355 (5, 1) [D loss: (-0.3, 0.1, -0.1)] [D acc: (0.664, 0.508, 0.586)] [G loss: 0.1] [G acc: 0.469]\n",
      "356 (5, 1) [D loss: (-0.4, 0.2, -0.1)] [D acc: (0.789, 0.375, 0.582)] [G loss: -0.1] [G acc: 0.562]\n",
      "357 (5, 1) [D loss: (-0.5, 0.3, -0.1)] [D acc: (0.781, 0.320, 0.551)] [G loss: -0.2] [G acc: 0.688]\n",
      "358 (5, 1) [D loss: (-0.5, 0.3, -0.1)] [D acc: (0.742, 0.398, 0.570)] [G loss: -0.2] [G acc: 0.641]\n",
      "359 (5, 1) [D loss: (-0.4, 0.2, -0.1)] [D acc: (0.781, 0.438, 0.609)] [G loss: -0.1] [G acc: 0.602]\n",
      "360 (5, 1) [D loss: (-0.3, -0.0, -0.2)] [D acc: (0.727, 0.547, 0.637)] [G loss: 0.4] [G acc: 0.234]\n",
      "361 (5, 1) [D loss: (-0.2, -0.1, -0.1)] [D acc: (0.586, 0.633, 0.609)] [G loss: 0.5] [G acc: 0.203]\n",
      "362 (5, 1) [D loss: (-0.2, -0.2, -0.2)] [D acc: (0.594, 0.617, 0.605)] [G loss: 0.7] [G acc: 0.141]\n",
      "363 (5, 1) [D loss: (0.0, 0.0, 0.0)] [D acc: (0.477, 0.594, 0.535)] [G loss: 0.8] [G acc: 0.070]\n",
      "364 (5, 1) [D loss: (-0.0, -0.3, -0.1)] [D acc: (0.461, 0.844, 0.652)] [G loss: 1.0] [G acc: 0.000]\n",
      "365 (5, 1) [D loss: (0.1, -0.4, -0.1)] [D acc: (0.398, 0.867, 0.633)] [G loss: 1.1] [G acc: 0.000]\n",
      "366 (5, 1) [D loss: (0.1, -0.4, -0.2)] [D acc: (0.453, 0.766, 0.609)] [G loss: 1.2] [G acc: 0.055]\n",
      "367 (5, 1) [D loss: (0.3, -0.5, -0.1)] [D acc: (0.359, 0.727, 0.543)] [G loss: 1.1] [G acc: 0.102]\n",
      "368 (5, 1) [D loss: (0.4, -0.4, -0.0)] [D acc: (0.336, 0.664, 0.500)] [G loss: 1.0] [G acc: 0.156]\n",
      "369 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.227, 0.797, 0.512)] [G loss: 1.1] [G acc: 0.008]\n",
      "370 (5, 1) [D loss: (0.8, -0.9, -0.0)] [D acc: (0.109, 0.992, 0.551)] [G loss: 1.4] [G acc: 0.000]\n",
      "371 (5, 1) [D loss: (0.9, -1.1, -0.1)] [D acc: (0.031, 1.000, 0.516)] [G loss: 1.5] [G acc: 0.000]\n",
      "372 (5, 1) [D loss: (1.0, -1.3, -0.1)] [D acc: (0.008, 1.000, 0.504)] [G loss: 1.8] [G acc: 0.000]\n",
      "373 (5, 1) [D loss: (1.0, -1.3, -0.1)] [D acc: (0.062, 0.992, 0.527)] [G loss: 1.8] [G acc: 0.000]\n",
      "374 (5, 1) [D loss: (1.0, -1.1, -0.1)] [D acc: (0.023, 1.000, 0.512)] [G loss: 1.6] [G acc: 0.000]\n",
      "375 (5, 1) [D loss: (0.8, -1.1, -0.2)] [D acc: (0.047, 1.000, 0.523)] [G loss: 1.5] [G acc: 0.000]\n",
      "376 (5, 1) [D loss: (0.9, -1.2, -0.1)] [D acc: (0.031, 1.000, 0.516)] [G loss: 1.4] [G acc: 0.000]\n",
      "377 (5, 1) [D loss: (0.9, -1.1, -0.1)] [D acc: (0.047, 1.000, 0.523)] [G loss: 1.5] [G acc: 0.000]\n",
      "378 (5, 1) [D loss: (1.0, -1.2, -0.1)] [D acc: (0.094, 1.000, 0.547)] [G loss: 1.6] [G acc: 0.000]\n",
      "379 (5, 1) [D loss: (1.0, -1.1, -0.1)] [D acc: (0.047, 0.992, 0.520)] [G loss: 1.7] [G acc: 0.000]\n",
      "380 (5, 1) [D loss: (1.0, -1.1, -0.1)] [D acc: (0.023, 1.000, 0.512)] [G loss: 1.6] [G acc: 0.000]\n",
      "381 (5, 1) [D loss: (0.9, -1.0, -0.0)] [D acc: (0.023, 1.000, 0.512)] [G loss: 1.5] [G acc: 0.000]\n",
      "382 (5, 1) [D loss: (1.0, -1.0, 0.0)] [D acc: (0.008, 1.000, 0.504)] [G loss: 1.4] [G acc: 0.000]\n",
      "383 (5, 1) [D loss: (0.8, -1.0, -0.1)] [D acc: (0.070, 1.000, 0.535)] [G loss: 1.3] [G acc: 0.000]\n",
      "384 (5, 1) [D loss: (0.9, -1.1, -0.1)] [D acc: (0.086, 0.961, 0.523)] [G loss: 1.5] [G acc: 0.016]\n",
      "385 (5, 1) [D loss: (0.8, -0.9, -0.0)] [D acc: (0.141, 0.891, 0.516)] [G loss: 1.3] [G acc: 0.055]\n",
      "386 (5, 1) [D loss: (0.8, -1.0, -0.1)] [D acc: (0.023, 0.930, 0.477)] [G loss: 1.4] [G acc: 0.000]\n",
      "387 (5, 1) [D loss: (0.9, -0.9, 0.0)] [D acc: (0.047, 1.000, 0.523)] [G loss: 1.3] [G acc: 0.000]\n",
      "388 (5, 1) [D loss: (1.0, -1.0, 0.0)] [D acc: (0.016, 1.000, 0.508)] [G loss: 1.3] [G acc: 0.000]\n",
      "389 (5, 1) [D loss: (0.9, -0.9, 0.0)] [D acc: (0.062, 0.992, 0.527)] [G loss: 1.3] [G acc: 0.000]\n",
      "390 (5, 1) [D loss: (0.8, -0.7, 0.0)] [D acc: (0.078, 0.953, 0.516)] [G loss: 1.1] [G acc: 0.000]\n",
      "391 (5, 1) [D loss: (0.9, -0.8, 0.0)] [D acc: (0.008, 1.000, 0.504)] [G loss: 1.1] [G acc: 0.000]\n",
      "392 (5, 1) [D loss: (0.7, -0.6, 0.0)] [D acc: (0.047, 1.000, 0.523)] [G loss: 1.0] [G acc: 0.000]\n",
      "393 (5, 1) [D loss: (0.6, -0.6, 0.0)] [D acc: (0.039, 0.992, 0.516)] [G loss: 0.9] [G acc: 0.000]\n",
      "394 (5, 1) [D loss: (0.5, -0.5, 0.0)] [D acc: (0.117, 0.930, 0.523)] [G loss: 0.8] [G acc: 0.000]\n",
      "395 (5, 1) [D loss: (0.5, -0.5, 0.0)] [D acc: (0.078, 0.992, 0.535)] [G loss: 0.7] [G acc: 0.000]\n",
      "396 (5, 1) [D loss: (0.4, -0.4, -0.0)] [D acc: (0.156, 0.992, 0.574)] [G loss: 0.7] [G acc: 0.000]\n",
      "397 (5, 1) [D loss: (0.3, -0.3, -0.0)] [D acc: (0.211, 0.898, 0.555)] [G loss: 0.6] [G acc: 0.008]\n",
      "398 (5, 1) [D loss: (0.1, -0.2, -0.1)] [D acc: (0.336, 0.641, 0.488)] [G loss: 0.5] [G acc: 0.094]\n",
      "399 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.422, 0.641, 0.531)] [G loss: 0.5] [G acc: 0.039]\n",
      "400 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.531, 0.523)] [G loss: 0.5] [G acc: 0.102]\n",
      "401 (5, 1) [D loss: (0.1, 0.1, 0.1)] [D acc: (0.328, 0.438, 0.383)] [G loss: 0.4] [G acc: 0.078]\n",
      "402 (5, 1) [D loss: (0.2, -0.1, 0.0)] [D acc: (0.305, 0.562, 0.434)] [G loss: 0.5] [G acc: 0.102]\n",
      "403 (5, 1) [D loss: (0.2, -0.2, 0.0)] [D acc: (0.242, 0.852, 0.547)] [G loss: 0.5] [G acc: 0.008]\n",
      "404 (5, 1) [D loss: (0.3, -0.3, -0.0)] [D acc: (0.203, 0.898, 0.551)] [G loss: 0.6] [G acc: 0.000]\n",
      "405 (5, 1) [D loss: (0.4, -0.5, -0.1)] [D acc: (0.164, 0.969, 0.566)] [G loss: 0.8] [G acc: 0.000]\n",
      "406 (5, 1) [D loss: (0.4, -0.6, -0.1)] [D acc: (0.109, 0.984, 0.547)] [G loss: 0.8] [G acc: 0.000]\n",
      "407 (5, 1) [D loss: (0.5, -0.6, -0.1)] [D acc: (0.086, 0.992, 0.539)] [G loss: 0.9] [G acc: 0.000]\n",
      "408 (5, 1) [D loss: (0.4, -0.6, -0.1)] [D acc: (0.117, 0.953, 0.535)] [G loss: 1.0] [G acc: 0.000]\n",
      "409 (5, 1) [D loss: (0.6, -0.8, -0.1)] [D acc: (0.109, 1.000, 0.555)] [G loss: 1.0] [G acc: 0.000]\n",
      "410 (5, 1) [D loss: (0.6, -0.8, -0.1)] [D acc: (0.078, 1.000, 0.539)] [G loss: 1.0] [G acc: 0.000]\n",
      "411 (5, 1) [D loss: (0.6, -0.8, -0.1)] [D acc: (0.133, 1.000, 0.566)] [G loss: 1.0] [G acc: 0.000]\n",
      "412 (5, 1) [D loss: (0.6, -0.8, -0.1)] [D acc: (0.102, 1.000, 0.551)] [G loss: 1.0] [G acc: 0.000]\n",
      "413 (5, 1) [D loss: (0.6, -0.7, -0.1)] [D acc: (0.070, 0.992, 0.531)] [G loss: 0.9] [G acc: 0.000]\n",
      "414 (5, 1) [D loss: (0.5, -0.7, -0.1)] [D acc: (0.094, 0.969, 0.531)] [G loss: 0.9] [G acc: 0.000]\n",
      "415 (5, 1) [D loss: (0.5, -0.7, -0.1)] [D acc: (0.016, 0.992, 0.504)] [G loss: 1.0] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416 (5, 1) [D loss: (0.4, -0.7, -0.1)] [D acc: (0.086, 0.938, 0.512)] [G loss: 0.9] [G acc: 0.016]\n",
      "417 (5, 1) [D loss: (0.3, -0.5, -0.1)] [D acc: (0.195, 0.883, 0.539)] [G loss: 0.8] [G acc: 0.000]\n",
      "418 (5, 1) [D loss: (0.4, -0.6, -0.1)] [D acc: (0.070, 0.992, 0.531)] [G loss: 0.9] [G acc: 0.000]\n",
      "419 (5, 1) [D loss: (0.5, -0.7, -0.1)] [D acc: (0.141, 0.977, 0.559)] [G loss: 1.0] [G acc: 0.008]\n",
      "420 (5, 1) [D loss: (0.6, -0.6, -0.0)] [D acc: (0.117, 0.906, 0.512)] [G loss: 1.0] [G acc: 0.000]\n",
      "421 (5, 1) [D loss: (0.6, -0.8, -0.1)] [D acc: (0.086, 0.969, 0.527)] [G loss: 1.1] [G acc: 0.000]\n",
      "422 (5, 1) [D loss: (0.7, -0.9, -0.1)] [D acc: (0.008, 1.000, 0.504)] [G loss: 1.1] [G acc: 0.000]\n",
      "423 (5, 1) [D loss: (0.7, -0.9, -0.1)] [D acc: (0.000, 1.000, 0.500)] [G loss: 1.1] [G acc: 0.000]\n",
      "424 (5, 1) [D loss: (0.8, -0.9, -0.1)] [D acc: (0.062, 1.000, 0.531)] [G loss: 1.2] [G acc: 0.000]\n",
      "425 (5, 1) [D loss: (0.8, -1.0, -0.1)] [D acc: (0.023, 1.000, 0.512)] [G loss: 1.3] [G acc: 0.000]\n",
      "426 (5, 1) [D loss: (0.8, -1.0, -0.1)] [D acc: (0.055, 0.992, 0.523)] [G loss: 1.3] [G acc: 0.000]\n",
      "427 (5, 1) [D loss: (0.9, -1.1, -0.1)] [D acc: (0.047, 1.000, 0.523)] [G loss: 1.4] [G acc: 0.000]\n",
      "428 (5, 1) [D loss: (0.9, -1.2, -0.1)] [D acc: (0.102, 0.977, 0.539)] [G loss: 1.4] [G acc: 0.008]\n",
      "429 (5, 1) [D loss: (0.9, -1.2, -0.2)] [D acc: (0.016, 1.000, 0.508)] [G loss: 1.4] [G acc: 0.000]\n",
      "430 (5, 1) [D loss: (0.7, -1.1, -0.2)] [D acc: (0.078, 0.992, 0.535)] [G loss: 1.4] [G acc: 0.000]\n",
      "431 (5, 1) [D loss: (0.8, -1.1, -0.2)] [D acc: (0.086, 0.992, 0.539)] [G loss: 1.3] [G acc: 0.008]\n",
      "432 (5, 1) [D loss: (0.9, -1.0, -0.1)] [D acc: (0.039, 0.992, 0.516)] [G loss: 1.4] [G acc: 0.000]\n",
      "433 (5, 1) [D loss: (1.0, -1.2, -0.1)] [D acc: (0.000, 1.000, 0.500)] [G loss: 1.5] [G acc: 0.000]\n",
      "434 (5, 1) [D loss: (1.1, -1.1, -0.0)] [D acc: (0.016, 1.000, 0.508)] [G loss: 1.4] [G acc: 0.000]\n",
      "435 (5, 1) [D loss: (0.9, -1.0, -0.1)] [D acc: (0.062, 0.992, 0.527)] [G loss: 1.3] [G acc: 0.000]\n",
      "436 (5, 1) [D loss: (0.9, -1.0, -0.1)] [D acc: (0.008, 1.000, 0.504)] [G loss: 1.3] [G acc: 0.000]\n",
      "437 (5, 1) [D loss: (0.8, -0.9, -0.0)] [D acc: (0.055, 1.000, 0.527)] [G loss: 1.2] [G acc: 0.000]\n",
      "438 (5, 1) [D loss: (0.8, -0.9, -0.0)] [D acc: (0.055, 0.969, 0.512)] [G loss: 1.1] [G acc: 0.031]\n",
      "439 (5, 1) [D loss: (0.8, -0.8, -0.0)] [D acc: (0.000, 1.000, 0.500)] [G loss: 1.1] [G acc: 0.000]\n",
      "440 (5, 1) [D loss: (0.8, -0.9, -0.0)] [D acc: (0.008, 1.000, 0.504)] [G loss: 1.1] [G acc: 0.000]\n",
      "441 (5, 1) [D loss: (0.8, -0.8, 0.0)] [D acc: (0.070, 0.938, 0.504)] [G loss: 1.0] [G acc: 0.016]\n",
      "442 (5, 1) [D loss: (0.7, -0.7, 0.0)] [D acc: (0.125, 0.844, 0.484)] [G loss: 0.9] [G acc: 0.047]\n",
      "443 (5, 1) [D loss: (0.8, -0.7, 0.0)] [D acc: (0.031, 1.000, 0.516)] [G loss: 1.0] [G acc: 0.000]\n",
      "444 (5, 1) [D loss: (0.8, -0.8, 0.0)] [D acc: (0.000, 0.977, 0.488)] [G loss: 1.1] [G acc: 0.000]\n",
      "445 (5, 1) [D loss: (0.9, -0.7, 0.1)] [D acc: (0.016, 0.992, 0.504)] [G loss: 1.0] [G acc: 0.000]\n",
      "446 (5, 1) [D loss: (0.8, -0.8, 0.0)] [D acc: (0.031, 0.930, 0.480)] [G loss: 0.9] [G acc: 0.031]\n",
      "447 (5, 1) [D loss: (0.9, -0.8, 0.1)] [D acc: (0.016, 0.977, 0.496)] [G loss: 1.0] [G acc: 0.000]\n",
      "448 (5, 1) [D loss: (0.9, -0.8, 0.0)] [D acc: (0.000, 1.000, 0.500)] [G loss: 1.0] [G acc: 0.000]\n",
      "449 (5, 1) [D loss: (0.8, -0.8, 0.0)] [D acc: (0.008, 1.000, 0.504)] [G loss: 1.0] [G acc: 0.000]\n",
      "450 (5, 1) [D loss: (0.8, -0.7, 0.1)] [D acc: (0.000, 1.000, 0.500)] [G loss: 0.9] [G acc: 0.000]\n",
      "451 (5, 1) [D loss: (0.8, -0.7, 0.0)] [D acc: (0.000, 1.000, 0.500)] [G loss: 0.9] [G acc: 0.000]\n",
      "452 (5, 1) [D loss: (0.8, -0.7, 0.1)] [D acc: (0.000, 1.000, 0.500)] [G loss: 0.9] [G acc: 0.000]\n",
      "453 (5, 1) [D loss: (0.7, -0.6, 0.0)] [D acc: (0.016, 1.000, 0.508)] [G loss: 0.8] [G acc: 0.000]\n",
      "454 (5, 1) [D loss: (0.7, -0.6, 0.1)] [D acc: (0.023, 0.977, 0.500)] [G loss: 0.8] [G acc: 0.000]\n",
      "455 (5, 1) [D loss: (0.6, -0.6, -0.0)] [D acc: (0.000, 1.000, 0.500)] [G loss: 0.8] [G acc: 0.000]\n",
      "456 (5, 1) [D loss: (0.6, -0.6, 0.0)] [D acc: (0.008, 1.000, 0.504)] [G loss: 0.7] [G acc: 0.000]\n",
      "457 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.031, 0.977, 0.504)] [G loss: 0.7] [G acc: 0.008]\n",
      "458 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.031, 0.961, 0.496)] [G loss: 0.6] [G acc: 0.000]\n",
      "459 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.023, 1.000, 0.512)] [G loss: 0.7] [G acc: 0.000]\n",
      "460 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.047, 0.984, 0.516)] [G loss: 0.7] [G acc: 0.000]\n",
      "461 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.055, 0.984, 0.520)] [G loss: 0.7] [G acc: 0.000]\n",
      "462 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.109, 0.992, 0.551)] [G loss: 0.6] [G acc: 0.031]\n",
      "463 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.102, 0.984, 0.543)] [G loss: 0.6] [G acc: 0.000]\n",
      "464 (5, 1) [D loss: (0.4, -0.4, -0.0)] [D acc: (0.031, 0.977, 0.504)] [G loss: 0.6] [G acc: 0.008]\n",
      "465 (5, 1) [D loss: (0.3, -0.4, -0.1)] [D acc: (0.078, 0.945, 0.512)] [G loss: 0.6] [G acc: 0.000]\n",
      "466 (5, 1) [D loss: (0.3, -0.4, -0.1)] [D acc: (0.117, 0.938, 0.527)] [G loss: 0.6] [G acc: 0.008]\n",
      "467 (5, 1) [D loss: (0.3, -0.5, -0.1)] [D acc: (0.164, 0.992, 0.578)] [G loss: 0.7] [G acc: 0.000]\n",
      "468 (5, 1) [D loss: (0.4, -0.6, -0.1)] [D acc: (0.102, 1.000, 0.551)] [G loss: 0.8] [G acc: 0.000]\n",
      "469 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.086, 0.992, 0.539)] [G loss: 0.7] [G acc: 0.000]\n",
      "470 (5, 1) [D loss: (0.3, -0.5, -0.1)] [D acc: (0.172, 0.984, 0.578)] [G loss: 0.7] [G acc: 0.008]\n",
      "471 (5, 1) [D loss: (0.3, -0.4, -0.1)] [D acc: (0.219, 0.914, 0.566)] [G loss: 0.6] [G acc: 0.016]\n",
      "472 (5, 1) [D loss: (0.3, -0.5, -0.1)] [D acc: (0.195, 0.953, 0.574)] [G loss: 0.6] [G acc: 0.008]\n",
      "473 (5, 1) [D loss: (0.3, -0.4, -0.1)] [D acc: (0.102, 0.945, 0.523)] [G loss: 0.6] [G acc: 0.000]\n",
      "474 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.102, 1.000, 0.551)] [G loss: 0.7] [G acc: 0.000]\n",
      "475 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.078, 0.984, 0.531)] [G loss: 0.7] [G acc: 0.000]\n",
      "476 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.164, 0.883, 0.523)] [G loss: 0.5] [G acc: 0.070]\n",
      "477 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.016, 1.000, 0.508)] [G loss: 0.7] [G acc: 0.000]\n",
      "478 (5, 1) [D loss: (0.5, -0.6, -0.1)] [D acc: (0.070, 1.000, 0.535)] [G loss: 0.7] [G acc: 0.000]\n",
      "479 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.016, 1.000, 0.508)] [G loss: 0.7] [G acc: 0.000]\n",
      "480 (5, 1) [D loss: (0.5, -0.6, -0.0)] [D acc: (0.016, 1.000, 0.508)] [G loss: 0.7] [G acc: 0.000]\n",
      "481 (5, 1) [D loss: (0.5, -0.6, -0.0)] [D acc: (0.055, 0.992, 0.523)] [G loss: 0.7] [G acc: 0.008]\n",
      "482 (5, 1) [D loss: (0.5, -0.6, -0.0)] [D acc: (0.039, 0.992, 0.516)] [G loss: 0.6] [G acc: 0.016]\n",
      "483 (5, 1) [D loss: (0.5, -0.6, -0.0)] [D acc: (0.039, 0.984, 0.512)] [G loss: 0.7] [G acc: 0.000]\n",
      "484 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.016, 0.992, 0.504)] [G loss: 0.7] [G acc: 0.000]\n",
      "485 (5, 1) [D loss: (0.5, -0.6, -0.0)] [D acc: (0.070, 0.984, 0.527)] [G loss: 0.8] [G acc: 0.000]\n",
      "486 (5, 1) [D loss: (0.6, -0.6, 0.0)] [D acc: (0.008, 0.992, 0.500)] [G loss: 0.7] [G acc: 0.000]\n",
      "487 (5, 1) [D loss: (0.6, -0.6, 0.0)] [D acc: (0.008, 1.000, 0.504)] [G loss: 0.8] [G acc: 0.008]\n",
      "488 (5, 1) [D loss: (0.6, -0.6, -0.0)] [D acc: (0.031, 0.977, 0.504)] [G loss: 0.8] [G acc: 0.008]\n",
      "489 (5, 1) [D loss: (0.7, -0.7, 0.0)] [D acc: (0.000, 0.992, 0.496)] [G loss: 0.8] [G acc: 0.000]\n",
      "490 (5, 1) [D loss: (0.7, -0.7, 0.0)] [D acc: (0.008, 1.000, 0.504)] [G loss: 0.8] [G acc: 0.000]\n",
      "491 (5, 1) [D loss: (0.7, -0.7, 0.0)] [D acc: (0.000, 1.000, 0.500)] [G loss: 0.9] [G acc: 0.000]\n",
      "492 (5, 1) [D loss: (0.7, -0.7, -0.0)] [D acc: (0.008, 1.000, 0.504)] [G loss: 0.9] [G acc: 0.000]\n",
      "493 (5, 1) [D loss: (0.6, -0.7, -0.0)] [D acc: (0.023, 1.000, 0.512)] [G loss: 0.8] [G acc: 0.000]\n",
      "494 (5, 1) [D loss: (0.7, -0.7, -0.0)] [D acc: (0.008, 0.992, 0.500)] [G loss: 1.0] [G acc: 0.000]\n",
      "495 (5, 1) [D loss: (0.7, -0.8, -0.1)] [D acc: (0.000, 1.000, 0.500)] [G loss: 0.8] [G acc: 0.000]\n",
      "496 (5, 1) [D loss: (0.7, -0.6, 0.0)] [D acc: (0.000, 1.000, 0.500)] [G loss: 0.8] [G acc: 0.000]\n",
      "497 (5, 1) [D loss: (0.6, -0.6, -0.0)] [D acc: (0.016, 1.000, 0.508)] [G loss: 0.8] [G acc: 0.000]\n",
      "498 (5, 1) [D loss: (0.5, -0.6, -0.0)] [D acc: (0.055, 0.992, 0.523)] [G loss: 0.8] [G acc: 0.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 (5, 1) [D loss: (0.5, -0.6, -0.0)] [D acc: (0.016, 0.984, 0.500)] [G loss: 0.8] [G acc: 0.000]\n",
      "500 (5, 1) [D loss: (0.6, -0.7, -0.1)] [D acc: (0.000, 1.000, 0.500)] [G loss: 0.9] [G acc: 0.000]\n",
      "501 (5, 1) [D loss: (0.6, -0.7, -0.0)] [D acc: (0.023, 1.000, 0.512)] [G loss: 0.9] [G acc: 0.000]\n",
      "502 (5, 1) [D loss: (0.5, -0.6, -0.0)] [D acc: (0.023, 1.000, 0.512)] [G loss: 0.7] [G acc: 0.000]\n",
      "503 (5, 1) [D loss: (0.4, -0.5, -0.1)] [D acc: (0.141, 0.945, 0.543)] [G loss: 0.8] [G acc: 0.008]\n",
      "504 (5, 1) [D loss: (0.4, -0.5, -0.1)] [D acc: (0.086, 0.969, 0.527)] [G loss: 0.6] [G acc: 0.016]\n",
      "505 (5, 1) [D loss: (0.3, -0.4, -0.0)] [D acc: (0.109, 0.922, 0.516)] [G loss: 0.6] [G acc: 0.008]\n",
      "506 (5, 1) [D loss: (0.4, -0.4, -0.0)] [D acc: (0.148, 0.914, 0.531)] [G loss: 0.7] [G acc: 0.031]\n",
      "507 (5, 1) [D loss: (0.4, -0.6, -0.1)] [D acc: (0.062, 1.000, 0.531)] [G loss: 0.8] [G acc: 0.000]\n",
      "508 (5, 1) [D loss: (0.4, -0.6, -0.1)] [D acc: (0.031, 0.992, 0.512)] [G loss: 0.8] [G acc: 0.000]\n",
      "509 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.016, 0.992, 0.504)] [G loss: 0.7] [G acc: 0.000]\n",
      "510 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.023, 0.992, 0.508)] [G loss: 0.7] [G acc: 0.000]\n",
      "511 (5, 1) [D loss: (0.4, -0.5, -0.1)] [D acc: (0.086, 1.000, 0.543)] [G loss: 0.7] [G acc: 0.000]\n",
      "512 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.148, 0.898, 0.523)] [G loss: 0.7] [G acc: 0.023]\n",
      "513 (5, 1) [D loss: (0.5, -0.6, -0.1)] [D acc: (0.047, 0.938, 0.492)] [G loss: 0.8] [G acc: 0.031]\n",
      "514 (5, 1) [D loss: (0.5, -0.7, -0.1)] [D acc: (0.023, 1.000, 0.512)] [G loss: 0.8] [G acc: 0.000]\n",
      "515 (5, 1) [D loss: (0.5, -0.6, -0.1)] [D acc: (0.062, 0.977, 0.520)] [G loss: 0.8] [G acc: 0.000]\n",
      "516 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.039, 0.961, 0.500)] [G loss: 0.8] [G acc: 0.000]\n",
      "517 (5, 1) [D loss: (0.4, -0.6, -0.1)] [D acc: (0.117, 0.938, 0.527)] [G loss: 0.8] [G acc: 0.047]\n",
      "518 (5, 1) [D loss: (0.4, -0.4, -0.0)] [D acc: (0.180, 0.836, 0.508)] [G loss: 0.7] [G acc: 0.055]\n",
      "519 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.102, 0.945, 0.523)] [G loss: 0.7] [G acc: 0.008]\n",
      "520 (5, 1) [D loss: (0.5, -0.6, -0.0)] [D acc: (0.031, 0.992, 0.512)] [G loss: 0.8] [G acc: 0.000]\n",
      "521 (5, 1) [D loss: (0.6, -0.7, -0.1)] [D acc: (0.000, 1.000, 0.500)] [G loss: 0.9] [G acc: 0.000]\n",
      "522 (5, 1) [D loss: (0.6, -0.7, -0.0)] [D acc: (0.000, 1.000, 0.500)] [G loss: 0.8] [G acc: 0.000]\n",
      "523 (5, 1) [D loss: (0.6, -0.6, -0.0)] [D acc: (0.023, 1.000, 0.512)] [G loss: 0.8] [G acc: 0.000]\n",
      "524 (5, 1) [D loss: (0.6, -0.6, -0.0)] [D acc: (0.016, 1.000, 0.508)] [G loss: 0.8] [G acc: 0.000]\n",
      "525 (5, 1) [D loss: (0.5, -0.6, -0.0)] [D acc: (0.102, 0.930, 0.516)] [G loss: 0.7] [G acc: 0.039]\n",
      "526 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.047, 0.969, 0.508)] [G loss: 0.6] [G acc: 0.008]\n",
      "527 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.062, 0.938, 0.500)] [G loss: 0.7] [G acc: 0.008]\n",
      "528 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.031, 0.969, 0.500)] [G loss: 0.7] [G acc: 0.000]\n",
      "529 (5, 1) [D loss: (0.5, -0.6, -0.1)] [D acc: (0.180, 0.914, 0.547)] [G loss: 0.7] [G acc: 0.094]\n",
      "530 (5, 1) [D loss: (0.5, -0.6, -0.1)] [D acc: (0.047, 0.945, 0.496)] [G loss: 0.7] [G acc: 0.047]\n",
      "531 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.078, 0.953, 0.516)] [G loss: 0.7] [G acc: 0.008]\n",
      "532 (5, 1) [D loss: (0.4, -0.4, 0.0)] [D acc: (0.117, 0.883, 0.500)] [G loss: 0.6] [G acc: 0.039]\n",
      "533 (5, 1) [D loss: (0.4, -0.4, -0.0)] [D acc: (0.180, 0.891, 0.535)] [G loss: 0.5] [G acc: 0.148]\n",
      "534 (5, 1) [D loss: (0.4, -0.4, -0.0)] [D acc: (0.062, 0.961, 0.512)] [G loss: 0.7] [G acc: 0.000]\n",
      "535 (5, 1) [D loss: (0.4, -0.4, -0.0)] [D acc: (0.156, 0.938, 0.547)] [G loss: 0.6] [G acc: 0.008]\n",
      "536 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.070, 0.953, 0.512)] [G loss: 0.7] [G acc: 0.000]\n",
      "537 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.031, 1.000, 0.516)] [G loss: 0.7] [G acc: 0.000]\n",
      "538 (5, 1) [D loss: (0.5, -0.5, 0.0)] [D acc: (0.023, 0.977, 0.500)] [G loss: 0.6] [G acc: 0.008]\n",
      "539 (5, 1) [D loss: (0.5, -0.4, 0.0)] [D acc: (0.016, 0.992, 0.504)] [G loss: 0.6] [G acc: 0.000]\n",
      "540 (5, 1) [D loss: (0.4, -0.4, -0.0)] [D acc: (0.031, 0.977, 0.504)] [G loss: 0.6] [G acc: 0.000]\n",
      "541 (5, 1) [D loss: (0.3, -0.4, -0.1)] [D acc: (0.086, 0.992, 0.539)] [G loss: 0.6] [G acc: 0.039]\n",
      "542 (5, 1) [D loss: (0.2, -0.4, -0.1)] [D acc: (0.141, 0.945, 0.543)] [G loss: 0.6] [G acc: 0.000]\n",
      "543 (5, 1) [D loss: (0.2, -0.4, -0.1)] [D acc: (0.180, 0.992, 0.586)] [G loss: 0.6] [G acc: 0.000]\n",
      "544 (5, 1) [D loss: (0.2, -0.4, -0.1)] [D acc: (0.164, 0.977, 0.570)] [G loss: 0.6] [G acc: 0.008]\n",
      "545 (5, 1) [D loss: (0.3, -0.5, -0.1)] [D acc: (0.133, 0.984, 0.559)] [G loss: 0.6] [G acc: 0.000]\n",
      "546 (5, 1) [D loss: (0.2, -0.4, -0.1)] [D acc: (0.133, 0.953, 0.543)] [G loss: 0.6] [G acc: 0.008]\n",
      "547 (5, 1) [D loss: (0.3, -0.4, -0.0)] [D acc: (0.078, 0.914, 0.496)] [G loss: 0.6] [G acc: 0.008]\n",
      "548 (5, 1) [D loss: (0.3, -0.3, 0.0)] [D acc: (0.125, 0.867, 0.496)] [G loss: 0.5] [G acc: 0.023]\n",
      "549 (5, 1) [D loss: (0.3, -0.3, -0.0)] [D acc: (0.078, 0.914, 0.496)] [G loss: 0.5] [G acc: 0.023]\n",
      "550 (5, 1) [D loss: (0.4, -0.4, -0.0)] [D acc: (0.078, 0.945, 0.512)] [G loss: 0.6] [G acc: 0.000]\n",
      "551 (5, 1) [D loss: (0.4, -0.5, -0.1)] [D acc: (0.031, 0.977, 0.504)] [G loss: 0.7] [G acc: 0.000]\n",
      "552 (5, 1) [D loss: (0.5, -0.5, -0.0)] [D acc: (0.062, 0.961, 0.512)] [G loss: 0.7] [G acc: 0.008]\n",
      "553 (5, 1) [D loss: (0.4, -0.5, -0.0)] [D acc: (0.008, 0.922, 0.465)] [G loss: 0.7] [G acc: 0.000]\n",
      "554 (5, 1) [D loss: (0.4, -0.4, -0.0)] [D acc: (0.109, 0.961, 0.535)] [G loss: 0.6] [G acc: 0.000]\n",
      "555 (5, 1) [D loss: (0.3, -0.4, -0.0)] [D acc: (0.062, 0.938, 0.500)] [G loss: 0.6] [G acc: 0.008]\n",
      "556 (5, 1) [D loss: (0.3, -0.5, -0.1)] [D acc: (0.148, 0.875, 0.512)] [G loss: 0.6] [G acc: 0.062]\n",
      "557 (5, 1) [D loss: (0.3, -0.3, -0.0)] [D acc: (0.125, 0.914, 0.520)] [G loss: 0.5] [G acc: 0.031]\n",
      "558 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.297, 0.812, 0.555)] [G loss: 0.5] [G acc: 0.109]\n",
      "559 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.172, 0.836, 0.504)] [G loss: 0.5] [G acc: 0.008]\n",
      "560 (5, 1) [D loss: (0.3, -0.4, -0.1)] [D acc: (0.188, 0.906, 0.547)] [G loss: 0.6] [G acc: 0.070]\n",
      "561 (5, 1) [D loss: (0.3, -0.3, -0.0)] [D acc: (0.109, 0.938, 0.523)] [G loss: 0.6] [G acc: 0.008]\n",
      "562 (5, 1) [D loss: (0.3, -0.3, -0.0)] [D acc: (0.133, 0.859, 0.496)] [G loss: 0.5] [G acc: 0.016]\n",
      "563 (5, 1) [D loss: (0.3, -0.3, -0.0)] [D acc: (0.117, 0.930, 0.523)] [G loss: 0.5] [G acc: 0.016]\n",
      "564 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.133, 0.953, 0.543)] [G loss: 0.4] [G acc: 0.062]\n",
      "565 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.219, 0.859, 0.539)] [G loss: 0.4] [G acc: 0.070]\n",
      "566 (5, 1) [D loss: (0.2, -0.1, 0.1)] [D acc: (0.234, 0.656, 0.445)] [G loss: 0.4] [G acc: 0.062]\n",
      "567 (5, 1) [D loss: (0.2, -0.2, 0.0)] [D acc: (0.078, 0.844, 0.461)] [G loss: 0.4] [G acc: 0.008]\n",
      "568 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.164, 0.930, 0.547)] [G loss: 0.5] [G acc: 0.000]\n",
      "569 (5, 1) [D loss: (0.2, -0.3, -0.1)] [D acc: (0.195, 0.945, 0.570)] [G loss: 0.4] [G acc: 0.039]\n",
      "570 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.164, 0.953, 0.559)] [G loss: 0.5] [G acc: 0.016]\n",
      "571 (5, 1) [D loss: (0.2, -0.3, -0.1)] [D acc: (0.242, 0.891, 0.566)] [G loss: 0.5] [G acc: 0.016]\n",
      "572 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.195, 0.906, 0.551)] [G loss: 0.4] [G acc: 0.000]\n",
      "573 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.164, 0.953, 0.559)] [G loss: 0.4] [G acc: 0.000]\n",
      "574 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.227, 0.938, 0.582)] [G loss: 0.4] [G acc: 0.008]\n",
      "575 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.180, 0.797, 0.488)] [G loss: 0.4] [G acc: 0.000]\n",
      "576 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.070, 1.000, 0.535)] [G loss: 0.4] [G acc: 0.000]\n",
      "577 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.086, 0.969, 0.527)] [G loss: 0.4] [G acc: 0.000]\n",
      "578 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.227, 0.742, 0.484)] [G loss: 0.4] [G acc: 0.008]\n",
      "579 (5, 1) [D loss: (0.2, -0.3, -0.1)] [D acc: (0.234, 0.961, 0.598)] [G loss: 0.4] [G acc: 0.000]\n",
      "580 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.273, 0.805, 0.539)] [G loss: 0.4] [G acc: 0.016]\n",
      "581 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.133, 0.844, 0.488)] [G loss: 0.4] [G acc: 0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.164, 0.898, 0.531)] [G loss: 0.4] [G acc: 0.000]\n",
      "583 (5, 1) [D loss: (0.1, -0.3, -0.1)] [D acc: (0.219, 0.953, 0.586)] [G loss: 0.4] [G acc: 0.000]\n",
      "584 (5, 1) [D loss: (0.1, -0.3, -0.1)] [D acc: (0.312, 0.891, 0.602)] [G loss: 0.4] [G acc: 0.070]\n",
      "585 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.203, 0.820, 0.512)] [G loss: 0.4] [G acc: 0.016]\n",
      "586 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.227, 0.914, 0.570)] [G loss: 0.5] [G acc: 0.023]\n",
      "587 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.172, 0.859, 0.516)] [G loss: 0.4] [G acc: 0.109]\n",
      "588 (5, 1) [D loss: (0.3, -0.3, -0.0)] [D acc: (0.180, 0.797, 0.488)] [G loss: 0.4] [G acc: 0.148]\n",
      "589 (5, 1) [D loss: (0.3, -0.2, 0.0)] [D acc: (0.148, 0.758, 0.453)] [G loss: 0.4] [G acc: 0.148]\n",
      "590 (5, 1) [D loss: (0.3, -0.3, 0.0)] [D acc: (0.148, 0.789, 0.469)] [G loss: 0.5] [G acc: 0.070]\n",
      "591 (5, 1) [D loss: (0.3, -0.3, 0.0)] [D acc: (0.062, 0.883, 0.473)] [G loss: 0.5] [G acc: 0.047]\n",
      "592 (5, 1) [D loss: (0.3, -0.4, -0.0)] [D acc: (0.266, 0.828, 0.547)] [G loss: 0.5] [G acc: 0.148]\n",
      "593 (5, 1) [D loss: (0.3, -0.4, -0.0)] [D acc: (0.047, 0.992, 0.520)] [G loss: 0.6] [G acc: 0.000]\n",
      "594 (5, 1) [D loss: (0.3, -0.4, -0.0)] [D acc: (0.039, 0.984, 0.512)] [G loss: 0.5] [G acc: 0.000]\n",
      "595 (5, 1) [D loss: (0.3, -0.4, -0.0)] [D acc: (0.062, 0.961, 0.512)] [G loss: 0.5] [G acc: 0.016]\n",
      "596 (5, 1) [D loss: (0.3, -0.3, -0.0)] [D acc: (0.117, 0.898, 0.508)] [G loss: 0.5] [G acc: 0.016]\n",
      "597 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.188, 0.797, 0.492)] [G loss: 0.4] [G acc: 0.031]\n",
      "598 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.250, 0.727, 0.488)] [G loss: 0.4] [G acc: 0.172]\n",
      "599 (5, 1) [D loss: (0.3, -0.2, 0.0)] [D acc: (0.086, 0.883, 0.484)] [G loss: 0.4] [G acc: 0.039]\n",
      "600 (5, 1) [D loss: (0.3, -0.3, -0.0)] [D acc: (0.109, 0.938, 0.523)] [G loss: 0.5] [G acc: 0.016]\n",
      "601 (5, 1) [D loss: (0.3, -0.3, 0.0)] [D acc: (0.109, 0.836, 0.473)] [G loss: 0.4] [G acc: 0.031]\n",
      "602 (5, 1) [D loss: (0.3, -0.3, -0.0)] [D acc: (0.023, 0.984, 0.504)] [G loss: 0.5] [G acc: 0.000]\n",
      "603 (5, 1) [D loss: (0.3, -0.3, 0.0)] [D acc: (0.102, 0.945, 0.523)] [G loss: 0.4] [G acc: 0.008]\n",
      "604 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.141, 0.938, 0.539)] [G loss: 0.4] [G acc: 0.016]\n",
      "605 (5, 1) [D loss: (0.2, -0.3, -0.0)] [D acc: (0.195, 0.742, 0.469)] [G loss: 0.4] [G acc: 0.070]\n",
      "606 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.164, 0.953, 0.559)] [G loss: 0.3] [G acc: 0.055]\n",
      "607 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.234, 0.906, 0.570)] [G loss: 0.3] [G acc: 0.047]\n",
      "608 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.203, 0.844, 0.523)] [G loss: 0.4] [G acc: 0.016]\n",
      "609 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.250, 0.781, 0.516)] [G loss: 0.3] [G acc: 0.109]\n",
      "610 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.305, 0.789, 0.547)] [G loss: 0.3] [G acc: 0.039]\n",
      "611 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.359, 0.758, 0.559)] [G loss: 0.2] [G acc: 0.141]\n",
      "612 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.664, 0.523)] [G loss: 0.2] [G acc: 0.227]\n",
      "613 (5, 1) [D loss: (0.1, -0.0, 0.0)] [D acc: (0.359, 0.547, 0.453)] [G loss: 0.2] [G acc: 0.156]\n",
      "614 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.344, 0.656, 0.500)] [G loss: 0.3] [G acc: 0.195]\n",
      "615 (5, 1) [D loss: (0.1, -0.1, 0.0)] [D acc: (0.312, 0.695, 0.504)] [G loss: 0.3] [G acc: 0.086]\n",
      "616 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.297, 0.797, 0.547)] [G loss: 0.3] [G acc: 0.039]\n",
      "617 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.266, 0.797, 0.531)] [G loss: 0.3] [G acc: 0.141]\n",
      "618 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.281, 0.883, 0.582)] [G loss: 0.3] [G acc: 0.094]\n",
      "619 (5, 1) [D loss: (0.2, -0.3, -0.1)] [D acc: (0.234, 0.930, 0.582)] [G loss: 0.4] [G acc: 0.023]\n",
      "620 (5, 1) [D loss: (0.1, -0.2, -0.1)] [D acc: (0.289, 0.891, 0.590)] [G loss: 0.4] [G acc: 0.016]\n",
      "621 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.180, 0.930, 0.555)] [G loss: 0.4] [G acc: 0.000]\n",
      "622 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.211, 0.906, 0.559)] [G loss: 0.3] [G acc: 0.008]\n",
      "623 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.336, 0.758, 0.547)] [G loss: 0.3] [G acc: 0.102]\n",
      "624 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.227, 0.789, 0.508)] [G loss: 0.2] [G acc: 0.039]\n",
      "625 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.328, 0.789, 0.559)] [G loss: 0.2] [G acc: 0.055]\n",
      "626 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.500, 0.586, 0.543)] [G loss: 0.2] [G acc: 0.219]\n",
      "627 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.648, 0.531)] [G loss: 0.2] [G acc: 0.133]\n",
      "628 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.750, 0.559)] [G loss: 0.2] [G acc: 0.000]\n",
      "629 (5, 1) [D loss: (0.0, -0.2, -0.1)] [D acc: (0.398, 0.898, 0.648)] [G loss: 0.3] [G acc: 0.008]\n",
      "630 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.289, 0.852, 0.570)] [G loss: 0.3] [G acc: 0.008]\n",
      "631 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.375, 0.906, 0.641)] [G loss: 0.3] [G acc: 0.016]\n",
      "632 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.297, 0.828, 0.562)] [G loss: 0.3] [G acc: 0.008]\n",
      "633 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.281, 0.898, 0.590)] [G loss: 0.3] [G acc: 0.016]\n",
      "634 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.164, 0.883, 0.523)] [G loss: 0.3] [G acc: 0.016]\n",
      "635 (5, 1) [D loss: (0.2, -0.2, -0.0)] [D acc: (0.234, 0.844, 0.539)] [G loss: 0.3] [G acc: 0.133]\n",
      "636 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.188, 0.742, 0.465)] [G loss: 0.2] [G acc: 0.156]\n",
      "637 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.211, 0.898, 0.555)] [G loss: 0.3] [G acc: 0.023]\n",
      "638 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.195, 0.844, 0.520)] [G loss: 0.3] [G acc: 0.039]\n",
      "639 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.242, 0.797, 0.520)] [G loss: 0.3] [G acc: 0.016]\n",
      "640 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.078, 0.906, 0.492)] [G loss: 0.3] [G acc: 0.062]\n",
      "641 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.211, 0.875, 0.543)] [G loss: 0.3] [G acc: 0.078]\n",
      "642 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.273, 0.844, 0.559)] [G loss: 0.3] [G acc: 0.039]\n",
      "643 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.211, 0.875, 0.543)] [G loss: 0.3] [G acc: 0.008]\n",
      "644 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.250, 0.953, 0.602)] [G loss: 0.3] [G acc: 0.016]\n",
      "645 (5, 1) [D loss: (0.1, -0.2, -0.1)] [D acc: (0.391, 0.844, 0.617)] [G loss: 0.3] [G acc: 0.188]\n",
      "646 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.773, 0.566)] [G loss: 0.2] [G acc: 0.008]\n",
      "647 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.680, 0.590)] [G loss: 0.2] [G acc: 0.156]\n",
      "648 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.430, 0.711, 0.570)] [G loss: 0.1] [G acc: 0.266]\n",
      "649 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.523, 0.539, 0.531)] [G loss: 0.1] [G acc: 0.078]\n",
      "650 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.562, 0.516, 0.539)] [G loss: 0.1] [G acc: 0.055]\n",
      "651 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.570, 0.469, 0.520)] [G loss: 0.1] [G acc: 0.133]\n",
      "652 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.562, 0.516, 0.539)] [G loss: 0.1] [G acc: 0.117]\n",
      "653 (5, 1) [D loss: (-0.0, 0.0, 0.0)] [D acc: (0.516, 0.516, 0.516)] [G loss: 0.1] [G acc: 0.195]\n",
      "654 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.602, 0.551)] [G loss: 0.1] [G acc: 0.164]\n",
      "655 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.391, 0.680, 0.535)] [G loss: 0.2] [G acc: 0.055]\n",
      "656 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.438, 0.648, 0.543)] [G loss: 0.2] [G acc: 0.195]\n",
      "657 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.281, 0.805, 0.543)] [G loss: 0.3] [G acc: 0.188]\n",
      "658 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.148, 0.945, 0.547)] [G loss: 0.3] [G acc: 0.008]\n",
      "659 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.164, 0.859, 0.512)] [G loss: 0.2] [G acc: 0.016]\n",
      "660 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.250, 0.844, 0.547)] [G loss: 0.2] [G acc: 0.055]\n",
      "661 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.336, 0.703, 0.520)] [G loss: 0.2] [G acc: 0.141]\n",
      "662 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.273, 0.820, 0.547)] [G loss: 0.2] [G acc: 0.133]\n",
      "663 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.391, 0.625, 0.508)] [G loss: 0.2] [G acc: 0.102]\n",
      "664 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.375, 0.727, 0.551)] [G loss: 0.2] [G acc: 0.016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665 (5, 1) [D loss: (0.1, -0.2, -0.1)] [D acc: (0.500, 0.633, 0.566)] [G loss: 0.2] [G acc: 0.359]\n",
      "666 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.469, 0.531, 0.500)] [G loss: 0.2] [G acc: 0.102]\n",
      "667 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.688, 0.551)] [G loss: 0.1] [G acc: 0.234]\n",
      "668 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.461, 0.734, 0.598)] [G loss: 0.2] [G acc: 0.172]\n",
      "669 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.305, 0.828, 0.566)] [G loss: 0.2] [G acc: 0.234]\n",
      "670 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.719, 0.531)] [G loss: 0.2] [G acc: 0.164]\n",
      "671 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.461, 0.781, 0.621)] [G loss: 0.2] [G acc: 0.188]\n",
      "672 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.641, 0.512)] [G loss: 0.2] [G acc: 0.203]\n",
      "673 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.891, 0.617)] [G loss: 0.3] [G acc: 0.008]\n",
      "674 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.438, 0.750, 0.594)] [G loss: 0.2] [G acc: 0.211]\n",
      "675 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.773, 0.547)] [G loss: 0.2] [G acc: 0.047]\n",
      "676 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.727, 0.551)] [G loss: 0.2] [G acc: 0.031]\n",
      "677 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.812, 0.582)] [G loss: 0.2] [G acc: 0.016]\n",
      "678 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.812, 0.590)] [G loss: 0.2] [G acc: 0.008]\n",
      "679 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.281, 0.812, 0.547)] [G loss: 0.2] [G acc: 0.078]\n",
      "680 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.734, 0.555)] [G loss: 0.2] [G acc: 0.188]\n",
      "681 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.453, 0.703, 0.578)] [G loss: 0.2] [G acc: 0.141]\n",
      "682 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.727, 0.578)] [G loss: 0.1] [G acc: 0.188]\n",
      "683 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.805, 0.602)] [G loss: 0.2] [G acc: 0.211]\n",
      "684 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.406, 0.656, 0.531)] [G loss: 0.1] [G acc: 0.141]\n",
      "685 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.414, 0.695, 0.555)] [G loss: 0.2] [G acc: 0.086]\n",
      "686 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.469, 0.688, 0.578)] [G loss: 0.2] [G acc: 0.195]\n",
      "687 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.648, 0.555)] [G loss: 0.1] [G acc: 0.094]\n",
      "688 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.742, 0.594)] [G loss: 0.1] [G acc: 0.133]\n",
      "689 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.617, 0.547)] [G loss: 0.1] [G acc: 0.211]\n",
      "690 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.562, 0.578, 0.570)] [G loss: 0.2] [G acc: 0.258]\n",
      "691 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.492, 0.641, 0.566)] [G loss: 0.1] [G acc: 0.219]\n",
      "692 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.484, 0.781, 0.633)] [G loss: 0.1] [G acc: 0.078]\n",
      "693 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.508, 0.695, 0.602)] [G loss: 0.1] [G acc: 0.148]\n",
      "694 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.477, 0.688, 0.582)] [G loss: 0.1] [G acc: 0.078]\n",
      "695 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.602, 0.590)] [G loss: 0.1] [G acc: 0.266]\n",
      "696 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.586, 0.477, 0.531)] [G loss: 0.1] [G acc: 0.062]\n",
      "697 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.617, 0.633, 0.625)] [G loss: 0.1] [G acc: 0.273]\n",
      "698 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.680, 0.453, 0.566)] [G loss: 0.1] [G acc: 0.305]\n",
      "699 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.672, 0.523, 0.598)] [G loss: 0.1] [G acc: 0.250]\n",
      "700 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.555, 0.523, 0.539)] [G loss: 0.1] [G acc: 0.195]\n",
      "701 (5, 1) [D loss: (-0.0, 0.0, 0.0)] [D acc: (0.555, 0.547, 0.551)] [G loss: 0.1] [G acc: 0.227]\n",
      "702 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.547, 0.531, 0.539)] [G loss: 0.1] [G acc: 0.320]\n",
      "703 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.539, 0.727, 0.633)] [G loss: 0.2] [G acc: 0.164]\n",
      "704 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.445, 0.789, 0.617)] [G loss: 0.1] [G acc: 0.195]\n",
      "705 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.562, 0.680, 0.621)] [G loss: 0.2] [G acc: 0.164]\n",
      "706 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.500, 0.664, 0.582)] [G loss: 0.2] [G acc: 0.047]\n",
      "707 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.477, 0.664, 0.570)] [G loss: 0.1] [G acc: 0.148]\n",
      "708 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.602, 0.531)] [G loss: 0.1] [G acc: 0.125]\n",
      "709 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.586, 0.535)] [G loss: 0.1] [G acc: 0.102]\n",
      "710 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.547, 0.625, 0.586)] [G loss: 0.1] [G acc: 0.180]\n",
      "711 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.406, 0.680, 0.543)] [G loss: 0.1] [G acc: 0.109]\n",
      "712 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.461, 0.805, 0.633)] [G loss: 0.1] [G acc: 0.055]\n",
      "713 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.781, 0.629)] [G loss: 0.1] [G acc: 0.023]\n",
      "714 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.758, 0.656)] [G loss: 0.1] [G acc: 0.086]\n",
      "715 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.680, 0.602)] [G loss: 0.1] [G acc: 0.023]\n",
      "716 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.617, 0.562)] [G loss: 0.1] [G acc: 0.164]\n",
      "717 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.695, 0.590)] [G loss: 0.1] [G acc: 0.156]\n",
      "718 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.586, 0.547)] [G loss: 0.1] [G acc: 0.156]\n",
      "719 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.609, 0.586)] [G loss: 0.1] [G acc: 0.164]\n",
      "720 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.570, 0.703, 0.637)] [G loss: 0.2] [G acc: 0.062]\n",
      "721 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.625, 0.664, 0.645)] [G loss: 0.1] [G acc: 0.117]\n",
      "722 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.555, 0.543)] [G loss: 0.1] [G acc: 0.172]\n",
      "723 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.484, 0.504)] [G loss: 0.1] [G acc: 0.211]\n",
      "724 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.617, 0.586)] [G loss: 0.1] [G acc: 0.242]\n",
      "725 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.539, 0.531, 0.535)] [G loss: 0.1] [G acc: 0.164]\n",
      "726 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.664, 0.430, 0.547)] [G loss: 0.1] [G acc: 0.141]\n",
      "727 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.641, 0.562, 0.602)] [G loss: 0.1] [G acc: 0.297]\n",
      "728 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.625, 0.598)] [G loss: 0.1] [G acc: 0.078]\n",
      "729 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.641, 0.492, 0.566)] [G loss: 0.1] [G acc: 0.086]\n",
      "730 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.617, 0.602)] [G loss: 0.1] [G acc: 0.242]\n",
      "731 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.586, 0.562)] [G loss: 0.1] [G acc: 0.141]\n",
      "732 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.555, 0.539)] [G loss: 0.1] [G acc: 0.141]\n",
      "733 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.625, 0.539)] [G loss: 0.1] [G acc: 0.086]\n",
      "734 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.703, 0.625)] [G loss: 0.1] [G acc: 0.211]\n",
      "735 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.547, 0.719, 0.633)] [G loss: 0.2] [G acc: 0.273]\n",
      "736 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.836, 0.617)] [G loss: 0.2] [G acc: 0.055]\n",
      "737 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.477, 0.719, 0.598)] [G loss: 0.2] [G acc: 0.039]\n",
      "738 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.789, 0.605)] [G loss: 0.2] [G acc: 0.047]\n",
      "739 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.648, 0.516)] [G loss: 0.2] [G acc: 0.148]\n",
      "740 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.406, 0.625, 0.516)] [G loss: 0.1] [G acc: 0.094]\n",
      "741 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.508, 0.742, 0.625)] [G loss: 0.1] [G acc: 0.250]\n",
      "742 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.766, 0.613)] [G loss: 0.1] [G acc: 0.047]\n",
      "743 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.664, 0.602, 0.633)] [G loss: 0.1] [G acc: 0.242]\n",
      "744 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.555, 0.695, 0.625)] [G loss: 0.1] [G acc: 0.172]\n",
      "745 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.562, 0.656, 0.609)] [G loss: 0.1] [G acc: 0.141]\n",
      "746 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.625, 0.508, 0.566)] [G loss: 0.1] [G acc: 0.297]\n",
      "747 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.539, 0.562, 0.551)] [G loss: 0.1] [G acc: 0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.570, 0.477, 0.523)] [G loss: 0.1] [G acc: 0.102]\n",
      "749 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.562, 0.477, 0.520)] [G loss: 0.1] [G acc: 0.141]\n",
      "750 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.586, 0.703, 0.645)] [G loss: 0.1] [G acc: 0.234]\n",
      "751 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.641, 0.633, 0.637)] [G loss: 0.2] [G acc: 0.250]\n",
      "752 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.539, 0.547)] [G loss: 0.1] [G acc: 0.133]\n",
      "753 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.602, 0.594)] [G loss: 0.1] [G acc: 0.047]\n",
      "754 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.555, 0.609, 0.582)] [G loss: 0.1] [G acc: 0.242]\n",
      "755 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.617, 0.414, 0.516)] [G loss: 0.1] [G acc: 0.133]\n",
      "756 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.656, 0.516, 0.586)] [G loss: 0.1] [G acc: 0.086]\n",
      "757 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.602, 0.578, 0.590)] [G loss: 0.1] [G acc: 0.148]\n",
      "758 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.672, 0.598)] [G loss: 0.2] [G acc: 0.070]\n",
      "759 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.648, 0.602)] [G loss: 0.1] [G acc: 0.117]\n",
      "760 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.680, 0.590)] [G loss: 0.1] [G acc: 0.117]\n",
      "761 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.625, 0.578)] [G loss: 0.1] [G acc: 0.141]\n",
      "762 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.609, 0.523, 0.566)] [G loss: 0.1] [G acc: 0.172]\n",
      "763 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.484, 0.531)] [G loss: 0.1] [G acc: 0.172]\n",
      "764 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.523, 0.648, 0.586)] [G loss: 0.1] [G acc: 0.125]\n",
      "765 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.484, 0.734, 0.609)] [G loss: 0.1] [G acc: 0.250]\n",
      "766 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.516, 0.758, 0.637)] [G loss: 0.2] [G acc: 0.125]\n",
      "767 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.477, 0.750, 0.613)] [G loss: 0.2] [G acc: 0.109]\n",
      "768 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.742, 0.582)] [G loss: 0.2] [G acc: 0.156]\n",
      "769 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.531, 0.781, 0.656)] [G loss: 0.2] [G acc: 0.211]\n",
      "770 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.523, 0.734, 0.629)] [G loss: 0.2] [G acc: 0.141]\n",
      "771 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.656, 0.672, 0.664)] [G loss: 0.1] [G acc: 0.219]\n",
      "772 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.492, 0.547)] [G loss: 0.1] [G acc: 0.305]\n",
      "773 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.609, 0.609, 0.609)] [G loss: 0.1] [G acc: 0.086]\n",
      "774 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.555, 0.559)] [G loss: 0.1] [G acc: 0.289]\n",
      "775 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.688, 0.484, 0.586)] [G loss: 0.1] [G acc: 0.320]\n",
      "776 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.516, 0.711, 0.613)] [G loss: 0.1] [G acc: 0.195]\n",
      "777 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.742, 0.617, 0.680)] [G loss: 0.1] [G acc: 0.133]\n",
      "778 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.664, 0.547, 0.605)] [G loss: 0.1] [G acc: 0.430]\n",
      "779 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.758, 0.438, 0.598)] [G loss: 0.1] [G acc: 0.172]\n",
      "780 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.633, 0.328, 0.480)] [G loss: 0.1] [G acc: 0.133]\n",
      "781 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.750, 0.359, 0.555)] [G loss: 0.1] [G acc: 0.133]\n",
      "782 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.648, 0.594, 0.621)] [G loss: 0.1] [G acc: 0.203]\n",
      "783 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.703, 0.656, 0.680)] [G loss: 0.1] [G acc: 0.094]\n",
      "784 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.602, 0.609)] [G loss: 0.1] [G acc: 0.125]\n",
      "785 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.672, 0.539, 0.605)] [G loss: 0.1] [G acc: 0.328]\n",
      "786 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.664, 0.594)] [G loss: 0.1] [G acc: 0.188]\n",
      "787 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.562, 0.586, 0.574)] [G loss: 0.1] [G acc: 0.023]\n",
      "788 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.648, 0.566)] [G loss: 0.1] [G acc: 0.039]\n",
      "789 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.641, 0.594)] [G loss: 0.1] [G acc: 0.062]\n",
      "790 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.523, 0.648, 0.586)] [G loss: 0.2] [G acc: 0.023]\n",
      "791 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.625, 0.695, 0.660)] [G loss: 0.2] [G acc: 0.062]\n",
      "792 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.562, 0.648, 0.605)] [G loss: 0.1] [G acc: 0.195]\n",
      "793 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.562, 0.543)] [G loss: 0.1] [G acc: 0.055]\n",
      "794 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.594, 0.516, 0.555)] [G loss: 0.1] [G acc: 0.195]\n",
      "795 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.516, 0.520)] [G loss: 0.1] [G acc: 0.211]\n",
      "796 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.578, 0.559)] [G loss: 0.1] [G acc: 0.227]\n",
      "797 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.602, 0.590)] [G loss: 0.2] [G acc: 0.094]\n",
      "798 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.406, 0.789, 0.598)] [G loss: 0.2] [G acc: 0.133]\n",
      "799 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.656, 0.602)] [G loss: 0.2] [G acc: 0.078]\n",
      "800 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.648, 0.570)] [G loss: 0.2] [G acc: 0.047]\n",
      "801 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.555, 0.688, 0.621)] [G loss: 0.2] [G acc: 0.039]\n",
      "802 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.516, 0.633, 0.574)] [G loss: 0.1] [G acc: 0.219]\n",
      "803 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.570, 0.508)] [G loss: 0.1] [G acc: 0.125]\n",
      "804 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.539, 0.594, 0.566)] [G loss: 0.1] [G acc: 0.242]\n",
      "805 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.555, 0.648, 0.602)] [G loss: 0.2] [G acc: 0.305]\n",
      "806 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.766, 0.648)] [G loss: 0.1] [G acc: 0.016]\n",
      "807 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.602, 0.727, 0.664)] [G loss: 0.1] [G acc: 0.148]\n",
      "808 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.688, 0.652)] [G loss: 0.1] [G acc: 0.070]\n",
      "809 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.555, 0.523, 0.539)] [G loss: 0.2] [G acc: 0.219]\n",
      "810 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.578, 0.543)] [G loss: 0.1] [G acc: 0.234]\n",
      "811 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.570, 0.492, 0.531)] [G loss: 0.1] [G acc: 0.117]\n",
      "812 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.547, 0.578, 0.562)] [G loss: 0.1] [G acc: 0.133]\n",
      "813 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.531, 0.625, 0.578)] [G loss: 0.2] [G acc: 0.219]\n",
      "814 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.609, 0.508, 0.559)] [G loss: 0.2] [G acc: 0.242]\n",
      "815 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.461, 0.805, 0.633)] [G loss: 0.2] [G acc: 0.086]\n",
      "816 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.594, 0.586, 0.590)] [G loss: 0.2] [G acc: 0.273]\n",
      "817 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.578, 0.531)] [G loss: 0.2] [G acc: 0.117]\n",
      "818 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.547, 0.680, 0.613)] [G loss: 0.2] [G acc: 0.086]\n",
      "819 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.484, 0.680, 0.582)] [G loss: 0.1] [G acc: 0.227]\n",
      "820 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.438, 0.734, 0.586)] [G loss: 0.1] [G acc: 0.164]\n",
      "821 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.547, 0.539, 0.543)] [G loss: 0.1] [G acc: 0.328]\n",
      "822 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.672, 0.516, 0.594)] [G loss: 0.1] [G acc: 0.234]\n",
      "823 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.594, 0.727, 0.660)] [G loss: 0.2] [G acc: 0.211]\n",
      "824 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.570, 0.773, 0.672)] [G loss: 0.2] [G acc: 0.227]\n",
      "825 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.508, 0.742, 0.625)] [G loss: 0.2] [G acc: 0.055]\n",
      "826 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.594, 0.742, 0.668)] [G loss: 0.2] [G acc: 0.055]\n",
      "827 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.516, 0.711, 0.613)] [G loss: 0.2] [G acc: 0.180]\n",
      "828 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.633, 0.566)] [G loss: 0.1] [G acc: 0.227]\n",
      "829 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.617, 0.551)] [G loss: 0.1] [G acc: 0.195]\n",
      "830 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.625, 0.562)] [G loss: 0.2] [G acc: 0.039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.727, 0.562)] [G loss: 0.2] [G acc: 0.109]\n",
      "832 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.703, 0.539)] [G loss: 0.2] [G acc: 0.016]\n",
      "833 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.352, 0.820, 0.586)] [G loss: 0.3] [G acc: 0.000]\n",
      "834 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.906, 0.629)] [G loss: 0.2] [G acc: 0.047]\n",
      "835 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.312, 0.781, 0.547)] [G loss: 0.2] [G acc: 0.055]\n",
      "836 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.266, 0.805, 0.535)] [G loss: 0.2] [G acc: 0.094]\n",
      "837 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.719, 0.543)] [G loss: 0.2] [G acc: 0.016]\n",
      "838 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.742, 0.570)] [G loss: 0.2] [G acc: 0.055]\n",
      "839 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.609, 0.633, 0.621)] [G loss: 0.1] [G acc: 0.328]\n",
      "840 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.695, 0.602)] [G loss: 0.1] [G acc: 0.195]\n",
      "841 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.641, 0.578, 0.609)] [G loss: 0.2] [G acc: 0.141]\n",
      "842 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.531, 0.734, 0.633)] [G loss: 0.1] [G acc: 0.258]\n",
      "843 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.648, 0.594, 0.621)] [G loss: 0.1] [G acc: 0.102]\n",
      "844 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.594, 0.609, 0.602)] [G loss: 0.1] [G acc: 0.109]\n",
      "845 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.617, 0.551)] [G loss: 0.1] [G acc: 0.203]\n",
      "846 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.484, 0.742, 0.613)] [G loss: 0.2] [G acc: 0.094]\n",
      "847 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.578, 0.523)] [G loss: 0.1] [G acc: 0.125]\n",
      "848 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.695, 0.617)] [G loss: 0.2] [G acc: 0.125]\n",
      "849 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.461, 0.688, 0.574)] [G loss: 0.2] [G acc: 0.070]\n",
      "850 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.438, 0.633, 0.535)] [G loss: 0.2] [G acc: 0.070]\n",
      "851 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.461, 0.648, 0.555)] [G loss: 0.2] [G acc: 0.055]\n",
      "852 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.828, 0.559)] [G loss: 0.2] [G acc: 0.070]\n",
      "853 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.344, 0.859, 0.602)] [G loss: 0.2] [G acc: 0.117]\n",
      "854 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.414, 0.812, 0.613)] [G loss: 0.2] [G acc: 0.094]\n",
      "855 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.773, 0.582)] [G loss: 0.2] [G acc: 0.141]\n",
      "856 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.742, 0.629)] [G loss: 0.1] [G acc: 0.039]\n",
      "857 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.680, 0.582)] [G loss: 0.2] [G acc: 0.016]\n",
      "858 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.594, 0.656, 0.625)] [G loss: 0.2] [G acc: 0.031]\n",
      "859 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.750, 0.637)] [G loss: 0.2] [G acc: 0.023]\n",
      "860 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.703, 0.660)] [G loss: 0.1] [G acc: 0.094]\n",
      "861 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.555, 0.578)] [G loss: 0.2] [G acc: 0.016]\n",
      "862 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.648, 0.605)] [G loss: 0.2] [G acc: 0.055]\n",
      "863 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.789, 0.617)] [G loss: 0.2] [G acc: 0.000]\n",
      "864 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.406, 0.844, 0.625)] [G loss: 0.2] [G acc: 0.016]\n",
      "865 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.734, 0.527)] [G loss: 0.2] [G acc: 0.070]\n",
      "866 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.930, 0.633)] [G loss: 0.2] [G acc: 0.016]\n",
      "867 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.812, 0.594)] [G loss: 0.2] [G acc: 0.031]\n",
      "868 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.414, 0.680, 0.547)] [G loss: 0.2] [G acc: 0.016]\n",
      "869 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.703, 0.598)] [G loss: 0.2] [G acc: 0.117]\n",
      "870 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.516, 0.695, 0.605)] [G loss: 0.1] [G acc: 0.125]\n",
      "871 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.719, 0.586)] [G loss: 0.1] [G acc: 0.156]\n",
      "872 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.719, 0.574)] [G loss: 0.2] [G acc: 0.055]\n",
      "873 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.688, 0.566)] [G loss: 0.2] [G acc: 0.016]\n",
      "874 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.648, 0.555)] [G loss: 0.1] [G acc: 0.297]\n",
      "875 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.500, 0.695, 0.598)] [G loss: 0.2] [G acc: 0.195]\n",
      "876 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.500, 0.688, 0.594)] [G loss: 0.2] [G acc: 0.148]\n",
      "877 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.578, 0.648, 0.613)] [G loss: 0.2] [G acc: 0.227]\n",
      "878 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.453, 0.828, 0.641)] [G loss: 0.2] [G acc: 0.172]\n",
      "879 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.711, 0.598)] [G loss: 0.2] [G acc: 0.023]\n",
      "880 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.438, 0.766, 0.602)] [G loss: 0.2] [G acc: 0.031]\n",
      "881 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.438, 0.750, 0.594)] [G loss: 0.2] [G acc: 0.016]\n",
      "882 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.406, 0.836, 0.621)] [G loss: 0.2] [G acc: 0.047]\n",
      "883 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.773, 0.598)] [G loss: 0.2] [G acc: 0.172]\n",
      "884 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.750, 0.559)] [G loss: 0.2] [G acc: 0.102]\n",
      "885 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.531, 0.609, 0.570)] [G loss: 0.2] [G acc: 0.289]\n",
      "886 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.750, 0.586)] [G loss: 0.2] [G acc: 0.008]\n",
      "887 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.445, 0.766, 0.605)] [G loss: 0.2] [G acc: 0.125]\n",
      "888 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.453, 0.820, 0.637)] [G loss: 0.2] [G acc: 0.055]\n",
      "889 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.547, 0.688, 0.617)] [G loss: 0.2] [G acc: 0.312]\n",
      "890 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.375, 0.711, 0.543)] [G loss: 0.2] [G acc: 0.039]\n",
      "891 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.648, 0.555)] [G loss: 0.2] [G acc: 0.016]\n",
      "892 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.773, 0.582)] [G loss: 0.2] [G acc: 0.000]\n",
      "893 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.625, 0.559)] [G loss: 0.2] [G acc: 0.094]\n",
      "894 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.695, 0.605)] [G loss: 0.1] [G acc: 0.070]\n",
      "895 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.445, 0.742, 0.594)] [G loss: 0.2] [G acc: 0.133]\n",
      "896 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.500, 0.656, 0.578)] [G loss: 0.2] [G acc: 0.016]\n",
      "897 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.531, 0.758, 0.645)] [G loss: 0.2] [G acc: 0.047]\n",
      "898 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.516, 0.711, 0.613)] [G loss: 0.2] [G acc: 0.156]\n",
      "899 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.688, 0.574)] [G loss: 0.2] [G acc: 0.047]\n",
      "900 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.891, 0.605)] [G loss: 0.2] [G acc: 0.000]\n",
      "901 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.883, 0.613)] [G loss: 0.2] [G acc: 0.039]\n",
      "902 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.672, 0.547)] [G loss: 0.2] [G acc: 0.023]\n",
      "903 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.406, 0.766, 0.586)] [G loss: 0.2] [G acc: 0.055]\n",
      "904 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.883, 0.586)] [G loss: 0.2] [G acc: 0.031]\n",
      "905 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.680, 0.520)] [G loss: 0.2] [G acc: 0.039]\n",
      "906 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.734, 0.539)] [G loss: 0.2] [G acc: 0.008]\n",
      "907 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.680, 0.523)] [G loss: 0.1] [G acc: 0.234]\n",
      "908 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.328, 0.719, 0.523)] [G loss: 0.2] [G acc: 0.062]\n",
      "909 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.906, 0.625)] [G loss: 0.2] [G acc: 0.016]\n",
      "910 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.383, 0.734, 0.559)] [G loss: 0.2] [G acc: 0.055]\n",
      "911 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.156, 0.836, 0.496)] [G loss: 0.3] [G acc: 0.000]\n",
      "912 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.180, 0.953, 0.566)] [G loss: 0.3] [G acc: 0.000]\n",
      "913 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.328, 0.859, 0.594)] [G loss: 0.3] [G acc: 0.008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.836, 0.555)] [G loss: 0.2] [G acc: 0.047]\n",
      "915 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.812, 0.586)] [G loss: 0.2] [G acc: 0.094]\n",
      "916 (5, 1) [D loss: (0.0, -0.2, -0.1)] [D acc: (0.461, 0.734, 0.598)] [G loss: 0.2] [G acc: 0.266]\n",
      "917 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.438, 0.789, 0.613)] [G loss: 0.2] [G acc: 0.086]\n",
      "918 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.539, 0.750, 0.645)] [G loss: 0.2] [G acc: 0.023]\n",
      "919 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.586, 0.758, 0.672)] [G loss: 0.2] [G acc: 0.039]\n",
      "920 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.523, 0.727, 0.625)] [G loss: 0.2] [G acc: 0.016]\n",
      "921 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.602, 0.641, 0.621)] [G loss: 0.1] [G acc: 0.109]\n",
      "922 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.562, 0.543)] [G loss: 0.1] [G acc: 0.133]\n",
      "923 (5, 1) [D loss: (-0.0, 0.0, 0.0)] [D acc: (0.461, 0.453, 0.457)] [G loss: 0.1] [G acc: 0.156]\n",
      "924 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.578, 0.516, 0.547)] [G loss: 0.1] [G acc: 0.297]\n",
      "925 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.625, 0.453, 0.539)] [G loss: 0.1] [G acc: 0.164]\n",
      "926 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.578, 0.648, 0.613)] [G loss: 0.1] [G acc: 0.289]\n",
      "927 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.602, 0.516, 0.559)] [G loss: 0.2] [G acc: 0.211]\n",
      "928 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.461, 0.852, 0.656)] [G loss: 0.2] [G acc: 0.008]\n",
      "929 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.547, 0.719, 0.633)] [G loss: 0.2] [G acc: 0.047]\n",
      "930 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.750, 0.551)] [G loss: 0.2] [G acc: 0.109]\n",
      "931 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.508, 0.570, 0.539)] [G loss: 0.2] [G acc: 0.289]\n",
      "932 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.602, 0.633, 0.617)] [G loss: 0.1] [G acc: 0.250]\n",
      "933 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.727, 0.598)] [G loss: 0.2] [G acc: 0.125]\n",
      "934 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.461, 0.617, 0.539)] [G loss: 0.1] [G acc: 0.234]\n",
      "935 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.641, 0.625, 0.633)] [G loss: 0.2] [G acc: 0.188]\n",
      "936 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.562, 0.719, 0.641)] [G loss: 0.2] [G acc: 0.086]\n",
      "937 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.453, 0.688, 0.570)] [G loss: 0.2] [G acc: 0.195]\n",
      "938 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.781, 0.586)] [G loss: 0.2] [G acc: 0.039]\n",
      "939 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.797, 0.562)] [G loss: 0.2] [G acc: 0.047]\n",
      "940 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.211, 0.883, 0.547)] [G loss: 0.2] [G acc: 0.031]\n",
      "941 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.211, 0.812, 0.512)] [G loss: 0.2] [G acc: 0.000]\n",
      "942 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.883, 0.578)] [G loss: 0.2] [G acc: 0.016]\n",
      "943 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.422, 0.742, 0.582)] [G loss: 0.2] [G acc: 0.062]\n",
      "944 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.406, 0.812, 0.609)] [G loss: 0.2] [G acc: 0.008]\n",
      "945 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.414, 0.812, 0.613)] [G loss: 0.2] [G acc: 0.047]\n",
      "946 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.359, 0.789, 0.574)] [G loss: 0.2] [G acc: 0.008]\n",
      "947 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.797, 0.598)] [G loss: 0.2] [G acc: 0.180]\n",
      "948 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.453, 0.648, 0.551)] [G loss: 0.2] [G acc: 0.234]\n",
      "949 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.438, 0.758, 0.598)] [G loss: 0.1] [G acc: 0.227]\n",
      "950 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.656, 0.555)] [G loss: 0.2] [G acc: 0.055]\n",
      "951 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.414, 0.703, 0.559)] [G loss: 0.2] [G acc: 0.031]\n",
      "952 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.469, 0.703, 0.586)] [G loss: 0.2] [G acc: 0.062]\n",
      "953 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.516, 0.742, 0.629)] [G loss: 0.2] [G acc: 0.023]\n",
      "954 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.766, 0.570)] [G loss: 0.2] [G acc: 0.008]\n",
      "955 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.469, 0.750, 0.609)] [G loss: 0.1] [G acc: 0.109]\n",
      "956 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.539, 0.781, 0.660)] [G loss: 0.2] [G acc: 0.094]\n",
      "957 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.539, 0.695, 0.617)] [G loss: 0.2] [G acc: 0.266]\n",
      "958 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.547, 0.742, 0.645)] [G loss: 0.2] [G acc: 0.078]\n",
      "959 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.461, 0.773, 0.617)] [G loss: 0.2] [G acc: 0.164]\n",
      "960 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.688, 0.543)] [G loss: 0.2] [G acc: 0.031]\n",
      "961 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.516, 0.680, 0.598)] [G loss: 0.1] [G acc: 0.281]\n",
      "962 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.578, 0.508, 0.543)] [G loss: 0.2] [G acc: 0.406]\n",
      "963 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.586, 0.641, 0.613)] [G loss: 0.1] [G acc: 0.078]\n",
      "964 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.648, 0.582)] [G loss: 0.1] [G acc: 0.289]\n",
      "965 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.711, 0.438, 0.574)] [G loss: 0.1] [G acc: 0.258]\n",
      "966 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.594, 0.602, 0.598)] [G loss: 0.1] [G acc: 0.109]\n",
      "967 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.516, 0.711, 0.613)] [G loss: 0.1] [G acc: 0.211]\n",
      "968 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.570, 0.656, 0.613)] [G loss: 0.1] [G acc: 0.219]\n",
      "969 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.609, 0.547, 0.578)] [G loss: 0.1] [G acc: 0.109]\n",
      "970 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.570, 0.520)] [G loss: 0.1] [G acc: 0.094]\n",
      "971 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.547, 0.559)] [G loss: 0.2] [G acc: 0.047]\n",
      "972 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.492, 0.797, 0.645)] [G loss: 0.2] [G acc: 0.008]\n",
      "973 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.867, 0.656)] [G loss: 0.2] [G acc: 0.039]\n",
      "974 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.719, 0.602)] [G loss: 0.2] [G acc: 0.055]\n",
      "975 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.773, 0.586)] [G loss: 0.2] [G acc: 0.008]\n",
      "976 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.430, 0.672, 0.551)] [G loss: 0.2] [G acc: 0.094]\n",
      "977 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.734, 0.535)] [G loss: 0.2] [G acc: 0.133]\n",
      "978 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.672, 0.609)] [G loss: 0.1] [G acc: 0.078]\n",
      "979 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.766, 0.605)] [G loss: 0.2] [G acc: 0.047]\n",
      "980 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.812, 0.598)] [G loss: 0.2] [G acc: 0.133]\n",
      "981 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.703, 0.531)] [G loss: 0.2] [G acc: 0.094]\n",
      "982 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.438, 0.688, 0.562)] [G loss: 0.2] [G acc: 0.039]\n",
      "983 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.852, 0.562)] [G loss: 0.2] [G acc: 0.062]\n",
      "984 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.711, 0.520)] [G loss: 0.2] [G acc: 0.109]\n",
      "985 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.398, 0.867, 0.633)] [G loss: 0.2] [G acc: 0.117]\n",
      "986 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.773, 0.609)] [G loss: 0.2] [G acc: 0.078]\n",
      "987 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.406, 0.695, 0.551)] [G loss: 0.2] [G acc: 0.148]\n",
      "988 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.789, 0.570)] [G loss: 0.2] [G acc: 0.023]\n",
      "989 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.359, 0.766, 0.562)] [G loss: 0.2] [G acc: 0.117]\n",
      "990 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.867, 0.598)] [G loss: 0.2] [G acc: 0.070]\n",
      "991 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.359, 0.852, 0.605)] [G loss: 0.2] [G acc: 0.102]\n",
      "992 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.633, 0.539)] [G loss: 0.2] [G acc: 0.234]\n",
      "993 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.438, 0.898, 0.668)] [G loss: 0.2] [G acc: 0.016]\n",
      "994 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.469, 0.812, 0.641)] [G loss: 0.2] [G acc: 0.031]\n",
      "995 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.523, 0.648, 0.586)] [G loss: 0.2] [G acc: 0.008]\n",
      "996 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.602, 0.688, 0.645)] [G loss: 0.1] [G acc: 0.039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.648, 0.680, 0.664)] [G loss: 0.2] [G acc: 0.109]\n",
      "998 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.594, 0.602, 0.598)] [G loss: 0.1] [G acc: 0.125]\n",
      "999 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.578, 0.469, 0.523)] [G loss: 0.1] [G acc: 0.148]\n",
      "1000 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.648, 0.559)] [G loss: 0.1] [G acc: 0.281]\n",
      "1001 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.641, 0.578, 0.609)] [G loss: 0.1] [G acc: 0.250]\n",
      "1002 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.672, 0.492, 0.582)] [G loss: 0.1] [G acc: 0.109]\n",
      "1003 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.516, 0.594, 0.555)] [G loss: 0.1] [G acc: 0.367]\n",
      "1004 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.555, 0.535)] [G loss: 0.1] [G acc: 0.078]\n",
      "1005 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.586, 0.535)] [G loss: 0.2] [G acc: 0.078]\n",
      "1006 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.414, 0.797, 0.605)] [G loss: 0.2] [G acc: 0.078]\n",
      "1007 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.477, 0.766, 0.621)] [G loss: 0.2] [G acc: 0.133]\n",
      "1008 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.438, 0.719, 0.578)] [G loss: 0.2] [G acc: 0.094]\n",
      "1009 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.359, 0.727, 0.543)] [G loss: 0.2] [G acc: 0.164]\n",
      "1010 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.773, 0.555)] [G loss: 0.2] [G acc: 0.016]\n",
      "1011 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.758, 0.578)] [G loss: 0.2] [G acc: 0.156]\n",
      "1012 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.406, 0.812, 0.609)] [G loss: 0.2] [G acc: 0.109]\n",
      "1013 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.469, 0.742, 0.605)] [G loss: 0.2] [G acc: 0.062]\n",
      "1014 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.508, 0.609, 0.559)] [G loss: 0.1] [G acc: 0.125]\n",
      "1015 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.547, 0.578, 0.562)] [G loss: 0.1] [G acc: 0.156]\n",
      "1016 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.453, 0.695, 0.574)] [G loss: 0.1] [G acc: 0.305]\n",
      "1017 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.648, 0.574)] [G loss: 0.2] [G acc: 0.023]\n",
      "1018 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.406, 0.680, 0.543)] [G loss: 0.1] [G acc: 0.094]\n",
      "1019 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.523, 0.656, 0.590)] [G loss: 0.2] [G acc: 0.242]\n",
      "1020 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.844, 0.617)] [G loss: 0.2] [G acc: 0.102]\n",
      "1021 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.352, 0.680, 0.516)] [G loss: 0.2] [G acc: 0.039]\n",
      "1022 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.219, 0.984, 0.602)] [G loss: 0.3] [G acc: 0.000]\n",
      "1023 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.227, 0.969, 0.598)] [G loss: 0.2] [G acc: 0.078]\n",
      "1024 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.336, 0.750, 0.543)] [G loss: 0.2] [G acc: 0.141]\n",
      "1025 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.852, 0.602)] [G loss: 0.2] [G acc: 0.109]\n",
      "1026 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.469, 0.633, 0.551)] [G loss: 0.1] [G acc: 0.180]\n",
      "1027 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.648, 0.609, 0.629)] [G loss: 0.2] [G acc: 0.219]\n",
      "1028 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.641, 0.629)] [G loss: 0.2] [G acc: 0.188]\n",
      "1029 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.594, 0.570, 0.582)] [G loss: 0.1] [G acc: 0.305]\n",
      "1030 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.609, 0.586, 0.598)] [G loss: 0.1] [G acc: 0.234]\n",
      "1031 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.641, 0.590)] [G loss: 0.1] [G acc: 0.344]\n",
      "1032 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.602, 0.602)] [G loss: 0.2] [G acc: 0.094]\n",
      "1033 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.656, 0.590)] [G loss: 0.1] [G acc: 0.133]\n",
      "1034 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.406, 0.586, 0.496)] [G loss: 0.1] [G acc: 0.109]\n",
      "1035 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.531, 0.617, 0.574)] [G loss: 0.2] [G acc: 0.164]\n",
      "1036 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.602, 0.586)] [G loss: 0.2] [G acc: 0.094]\n",
      "1037 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.477, 0.695, 0.586)] [G loss: 0.1] [G acc: 0.180]\n",
      "1038 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.492, 0.695, 0.594)] [G loss: 0.1] [G acc: 0.141]\n",
      "1039 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.406, 0.703, 0.555)] [G loss: 0.1] [G acc: 0.047]\n",
      "1040 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.578, 0.578, 0.578)] [G loss: 0.2] [G acc: 0.234]\n",
      "1041 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.477, 0.750, 0.613)] [G loss: 0.2] [G acc: 0.039]\n",
      "1042 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.523, 0.727, 0.625)] [G loss: 0.1] [G acc: 0.109]\n",
      "1043 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.539, 0.750, 0.645)] [G loss: 0.2] [G acc: 0.016]\n",
      "1044 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.500, 0.758, 0.629)] [G loss: 0.2] [G acc: 0.039]\n",
      "1045 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.492, 0.758, 0.625)] [G loss: 0.1] [G acc: 0.195]\n",
      "1046 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.672, 0.586)] [G loss: 0.2] [G acc: 0.055]\n",
      "1047 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.469, 0.773, 0.621)] [G loss: 0.1] [G acc: 0.195]\n",
      "1048 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.742, 0.582)] [G loss: 0.2] [G acc: 0.102]\n",
      "1049 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.836, 0.598)] [G loss: 0.2] [G acc: 0.039]\n",
      "1050 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.500, 0.711, 0.605)] [G loss: 0.2] [G acc: 0.031]\n",
      "1051 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.812, 0.570)] [G loss: 0.2] [G acc: 0.000]\n",
      "1052 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.445, 0.688, 0.566)] [G loss: 0.1] [G acc: 0.195]\n",
      "1053 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.711, 0.617)] [G loss: 0.1] [G acc: 0.055]\n",
      "1054 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.703, 0.562)] [G loss: 0.2] [G acc: 0.195]\n",
      "1055 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.602, 0.516)] [G loss: 0.2] [G acc: 0.195]\n",
      "1056 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.367, 0.773, 0.570)] [G loss: 0.1] [G acc: 0.203]\n",
      "1057 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.836, 0.598)] [G loss: 0.2] [G acc: 0.031]\n",
      "1058 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.469, 0.727, 0.598)] [G loss: 0.2] [G acc: 0.023]\n",
      "1059 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.430, 0.758, 0.594)] [G loss: 0.2] [G acc: 0.117]\n",
      "1060 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.461, 0.766, 0.613)] [G loss: 0.2] [G acc: 0.141]\n",
      "1061 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.477, 0.750, 0.613)] [G loss: 0.2] [G acc: 0.055]\n",
      "1062 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.422, 0.734, 0.578)] [G loss: 0.2] [G acc: 0.047]\n",
      "1063 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.648, 0.539)] [G loss: 0.2] [G acc: 0.016]\n",
      "1064 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.672, 0.566)] [G loss: 0.1] [G acc: 0.188]\n",
      "1065 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.648, 0.547)] [G loss: 0.1] [G acc: 0.227]\n",
      "1066 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.641, 0.539)] [G loss: 0.1] [G acc: 0.062]\n",
      "1067 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.680, 0.559)] [G loss: 0.2] [G acc: 0.062]\n",
      "1068 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.531, 0.656, 0.594)] [G loss: 0.1] [G acc: 0.156]\n",
      "1069 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.570, 0.551)] [G loss: 0.1] [G acc: 0.336]\n",
      "1070 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.336, 0.695, 0.516)] [G loss: 0.1] [G acc: 0.164]\n",
      "1071 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.633, 0.535)] [G loss: 0.2] [G acc: 0.141]\n",
      "1072 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.641, 0.586, 0.613)] [G loss: 0.1] [G acc: 0.070]\n",
      "1073 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.602, 0.539)] [G loss: 0.1] [G acc: 0.094]\n",
      "1074 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.680, 0.594)] [G loss: 0.1] [G acc: 0.109]\n",
      "1075 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.492, 0.719, 0.605)] [G loss: 0.2] [G acc: 0.148]\n",
      "1076 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.484, 0.828, 0.656)] [G loss: 0.2] [G acc: 0.133]\n",
      "1077 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.406, 0.750, 0.578)] [G loss: 0.1] [G acc: 0.180]\n",
      "1078 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.828, 0.598)] [G loss: 0.2] [G acc: 0.023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1079 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.281, 0.789, 0.535)] [G loss: 0.2] [G acc: 0.109]\n",
      "1080 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.750, 0.559)] [G loss: 0.2] [G acc: 0.086]\n",
      "1081 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.781, 0.551)] [G loss: 0.2] [G acc: 0.109]\n",
      "1082 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.703, 0.531)] [G loss: 0.2] [G acc: 0.102]\n",
      "1083 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.344, 0.820, 0.582)] [G loss: 0.2] [G acc: 0.016]\n",
      "1084 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.812, 0.543)] [G loss: 0.2] [G acc: 0.031]\n",
      "1085 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.672, 0.535)] [G loss: 0.2] [G acc: 0.086]\n",
      "1086 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.719, 0.551)] [G loss: 0.2] [G acc: 0.008]\n",
      "1087 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.453, 0.836, 0.645)] [G loss: 0.2] [G acc: 0.062]\n",
      "1088 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.414, 0.773, 0.594)] [G loss: 0.1] [G acc: 0.188]\n",
      "1089 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.461, 0.773, 0.617)] [G loss: 0.2] [G acc: 0.047]\n",
      "1090 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.469, 0.719, 0.594)] [G loss: 0.2] [G acc: 0.141]\n",
      "1091 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.312, 0.875, 0.594)] [G loss: 0.2] [G acc: 0.008]\n",
      "1092 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.812, 0.570)] [G loss: 0.2] [G acc: 0.062]\n",
      "1093 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.828, 0.602)] [G loss: 0.2] [G acc: 0.141]\n",
      "1094 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.695, 0.523)] [G loss: 0.2] [G acc: 0.062]\n",
      "1095 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.789, 0.562)] [G loss: 0.2] [G acc: 0.055]\n",
      "1096 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.531, 0.594, 0.562)] [G loss: 0.2] [G acc: 0.164]\n",
      "1097 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.531, 0.703, 0.617)] [G loss: 0.1] [G acc: 0.109]\n",
      "1098 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.625, 0.625, 0.625)] [G loss: 0.1] [G acc: 0.266]\n",
      "1099 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.555, 0.555, 0.555)] [G loss: 0.2] [G acc: 0.164]\n",
      "1100 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.445, 0.711, 0.578)] [G loss: 0.1] [G acc: 0.227]\n",
      "1101 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.406, 0.680, 0.543)] [G loss: 0.1] [G acc: 0.047]\n",
      "1102 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.438, 0.703, 0.570)] [G loss: 0.2] [G acc: 0.102]\n",
      "1103 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.477, 0.602, 0.539)] [G loss: 0.2] [G acc: 0.211]\n",
      "1104 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.633, 0.578)] [G loss: 0.2] [G acc: 0.047]\n",
      "1105 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.570, 0.559)] [G loss: 0.1] [G acc: 0.078]\n",
      "1106 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.422, 0.680, 0.551)] [G loss: 0.1] [G acc: 0.102]\n",
      "1107 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.758, 0.594)] [G loss: 0.1] [G acc: 0.070]\n",
      "1108 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.555, 0.672, 0.613)] [G loss: 0.1] [G acc: 0.102]\n",
      "1109 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.453, 0.781, 0.617)] [G loss: 0.2] [G acc: 0.047]\n",
      "1110 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.828, 0.602)] [G loss: 0.2] [G acc: 0.023]\n",
      "1111 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.805, 0.566)] [G loss: 0.2] [G acc: 0.023]\n",
      "1112 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.766, 0.543)] [G loss: 0.2] [G acc: 0.008]\n",
      "1113 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.789, 0.562)] [G loss: 0.2] [G acc: 0.078]\n",
      "1114 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.234, 0.906, 0.570)] [G loss: 0.2] [G acc: 0.016]\n",
      "1115 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.391, 0.820, 0.605)] [G loss: 0.2] [G acc: 0.070]\n",
      "1116 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.391, 0.672, 0.531)] [G loss: 0.2] [G acc: 0.109]\n",
      "1117 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.430, 0.680, 0.555)] [G loss: 0.2] [G acc: 0.039]\n",
      "1118 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.781, 0.527)] [G loss: 0.2] [G acc: 0.055]\n",
      "1119 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.695, 0.574)] [G loss: 0.2] [G acc: 0.203]\n",
      "1120 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.719, 0.559)] [G loss: 0.2] [G acc: 0.062]\n",
      "1121 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.227, 0.852, 0.539)] [G loss: 0.2] [G acc: 0.023]\n",
      "1122 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.734, 0.582)] [G loss: 0.1] [G acc: 0.047]\n",
      "1123 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.656, 0.551)] [G loss: 0.1] [G acc: 0.031]\n",
      "1124 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.594, 0.625, 0.609)] [G loss: 0.1] [G acc: 0.156]\n",
      "1125 (5, 1) [D loss: (-0.1, -0.0, -0.1)] [D acc: (0.617, 0.688, 0.652)] [G loss: 0.1] [G acc: 0.195]\n",
      "1126 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.531, 0.711, 0.621)] [G loss: 0.2] [G acc: 0.180]\n",
      "1127 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.484, 0.602, 0.543)] [G loss: 0.1] [G acc: 0.281]\n",
      "1128 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.570, 0.500, 0.535)] [G loss: 0.1] [G acc: 0.273]\n",
      "1129 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.492, 0.520)] [G loss: 0.2] [G acc: 0.203]\n",
      "1130 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.586, 0.656, 0.621)] [G loss: 0.1] [G acc: 0.164]\n",
      "1131 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.594, 0.547)] [G loss: 0.1] [G acc: 0.289]\n",
      "1132 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.586, 0.559)] [G loss: 0.1] [G acc: 0.250]\n",
      "1133 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.633, 0.539)] [G loss: 0.2] [G acc: 0.094]\n",
      "1134 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.742, 0.566)] [G loss: 0.1] [G acc: 0.094]\n",
      "1135 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.703, 0.590)] [G loss: 0.1] [G acc: 0.219]\n",
      "1136 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.531, 0.695, 0.613)] [G loss: 0.1] [G acc: 0.164]\n",
      "1137 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.586, 0.512)] [G loss: 0.1] [G acc: 0.148]\n",
      "1138 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.711, 0.555)] [G loss: 0.1] [G acc: 0.102]\n",
      "1139 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.828, 0.582)] [G loss: 0.2] [G acc: 0.062]\n",
      "1140 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.344, 0.812, 0.578)] [G loss: 0.2] [G acc: 0.000]\n",
      "1141 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.180, 0.867, 0.523)] [G loss: 0.2] [G acc: 0.031]\n",
      "1142 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.227, 0.875, 0.551)] [G loss: 0.2] [G acc: 0.039]\n",
      "1143 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.305, 0.805, 0.555)] [G loss: 0.2] [G acc: 0.039]\n",
      "1144 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.953, 0.641)] [G loss: 0.2] [G acc: 0.008]\n",
      "1145 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.820, 0.605)] [G loss: 0.2] [G acc: 0.047]\n",
      "1146 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.789, 0.582)] [G loss: 0.2] [G acc: 0.031]\n",
      "1147 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.859, 0.598)] [G loss: 0.2] [G acc: 0.000]\n",
      "1148 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.883, 0.605)] [G loss: 0.2] [G acc: 0.133]\n",
      "1149 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.312, 0.789, 0.551)] [G loss: 0.2] [G acc: 0.102]\n",
      "1150 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.383, 0.711, 0.547)] [G loss: 0.2] [G acc: 0.008]\n",
      "1151 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.758, 0.602)] [G loss: 0.2] [G acc: 0.039]\n",
      "1152 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.414, 0.680, 0.547)] [G loss: 0.1] [G acc: 0.172]\n",
      "1153 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.422, 0.648, 0.535)] [G loss: 0.1] [G acc: 0.125]\n",
      "1154 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.672, 0.574)] [G loss: 0.1] [G acc: 0.148]\n",
      "1155 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.703, 0.594)] [G loss: 0.2] [G acc: 0.070]\n",
      "1156 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.414, 0.656, 0.535)] [G loss: 0.1] [G acc: 0.109]\n",
      "1157 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.422, 0.766, 0.594)] [G loss: 0.2] [G acc: 0.148]\n",
      "1158 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.508, 0.648, 0.578)] [G loss: 0.2] [G acc: 0.305]\n",
      "1159 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.734, 0.578)] [G loss: 0.1] [G acc: 0.070]\n",
      "1160 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.617, 0.539)] [G loss: 0.1] [G acc: 0.227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.375, 0.703, 0.539)] [G loss: 0.1] [G acc: 0.078]\n",
      "1162 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.758, 0.570)] [G loss: 0.2] [G acc: 0.008]\n",
      "1163 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.625, 0.527)] [G loss: 0.1] [G acc: 0.109]\n",
      "1164 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.641, 0.590)] [G loss: 0.1] [G acc: 0.289]\n",
      "1165 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.469, 0.648, 0.559)] [G loss: 0.1] [G acc: 0.125]\n",
      "1166 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.422, 0.664, 0.543)] [G loss: 0.1] [G acc: 0.219]\n",
      "1167 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.547, 0.664, 0.605)] [G loss: 0.1] [G acc: 0.273]\n",
      "1168 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.461, 0.570, 0.516)] [G loss: 0.1] [G acc: 0.086]\n",
      "1169 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.406, 0.828, 0.617)] [G loss: 0.1] [G acc: 0.109]\n",
      "1170 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.727, 0.574)] [G loss: 0.2] [G acc: 0.016]\n",
      "1171 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.297, 0.891, 0.594)] [G loss: 0.2] [G acc: 0.031]\n",
      "1172 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.297, 0.750, 0.523)] [G loss: 0.2] [G acc: 0.047]\n",
      "1173 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.344, 0.930, 0.637)] [G loss: 0.2] [G acc: 0.023]\n",
      "1174 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.297, 0.719, 0.508)] [G loss: 0.2] [G acc: 0.117]\n",
      "1175 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.312, 0.789, 0.551)] [G loss: 0.2] [G acc: 0.125]\n",
      "1176 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.836, 0.586)] [G loss: 0.2] [G acc: 0.016]\n",
      "1177 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.266, 0.867, 0.566)] [G loss: 0.2] [G acc: 0.008]\n",
      "1178 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.883, 0.633)] [G loss: 0.2] [G acc: 0.016]\n",
      "1179 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.445, 0.711, 0.578)] [G loss: 0.2] [G acc: 0.023]\n",
      "1180 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.734, 0.586)] [G loss: 0.1] [G acc: 0.039]\n",
      "1181 (5, 1) [D loss: (-0.1, -0.1, -0.1)] [D acc: (0.594, 0.703, 0.648)] [G loss: 0.1] [G acc: 0.242]\n",
      "1182 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.594, 0.711, 0.652)] [G loss: 0.1] [G acc: 0.078]\n",
      "1183 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.578, 0.656, 0.617)] [G loss: 0.1] [G acc: 0.102]\n",
      "1184 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.656, 0.602)] [G loss: 0.1] [G acc: 0.086]\n",
      "1185 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.727, 0.578)] [G loss: 0.1] [G acc: 0.102]\n",
      "1186 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.406, 0.719, 0.562)] [G loss: 0.1] [G acc: 0.062]\n",
      "1187 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.750, 0.520)] [G loss: 0.2] [G acc: 0.000]\n",
      "1188 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.773, 0.586)] [G loss: 0.2] [G acc: 0.117]\n",
      "1189 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.773, 0.551)] [G loss: 0.2] [G acc: 0.102]\n",
      "1190 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.305, 0.898, 0.602)] [G loss: 0.2] [G acc: 0.008]\n",
      "1191 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.242, 0.789, 0.516)] [G loss: 0.2] [G acc: 0.000]\n",
      "1192 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.266, 0.820, 0.543)] [G loss: 0.2] [G acc: 0.039]\n",
      "1193 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.812, 0.594)] [G loss: 0.2] [G acc: 0.055]\n",
      "1194 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.352, 0.758, 0.555)] [G loss: 0.2] [G acc: 0.039]\n",
      "1195 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.414, 0.797, 0.605)] [G loss: 0.1] [G acc: 0.102]\n",
      "1196 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.938, 0.629)] [G loss: 0.2] [G acc: 0.062]\n",
      "1197 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.875, 0.582)] [G loss: 0.2] [G acc: 0.055]\n",
      "1198 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.805, 0.574)] [G loss: 0.2] [G acc: 0.031]\n",
      "1199 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.703, 0.523)] [G loss: 0.2] [G acc: 0.164]\n",
      "1200 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.203, 0.867, 0.535)] [G loss: 0.2] [G acc: 0.023]\n",
      "1201 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.312, 0.852, 0.582)] [G loss: 0.2] [G acc: 0.109]\n",
      "1202 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.266, 0.867, 0.566)] [G loss: 0.2] [G acc: 0.039]\n",
      "1203 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.273, 0.820, 0.547)] [G loss: 0.2] [G acc: 0.016]\n",
      "1204 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.312, 0.844, 0.578)] [G loss: 0.2] [G acc: 0.094]\n",
      "1205 (5, 1) [D loss: (0.0, -0.1, -0.1)] [D acc: (0.406, 0.734, 0.570)] [G loss: 0.2] [G acc: 0.211]\n",
      "1206 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.312, 0.789, 0.551)] [G loss: 0.1] [G acc: 0.047]\n",
      "1207 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.328, 0.719, 0.523)] [G loss: 0.2] [G acc: 0.102]\n",
      "1208 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.805, 0.578)] [G loss: 0.2] [G acc: 0.055]\n",
      "1209 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.727, 0.602)] [G loss: 0.1] [G acc: 0.195]\n",
      "1210 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.602, 0.547, 0.574)] [G loss: 0.1] [G acc: 0.117]\n",
      "1211 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.664, 0.562)] [G loss: 0.1] [G acc: 0.070]\n",
      "1212 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.406, 0.656, 0.531)] [G loss: 0.1] [G acc: 0.094]\n",
      "1213 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.672, 0.543)] [G loss: 0.1] [G acc: 0.117]\n",
      "1214 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.719, 0.555)] [G loss: 0.1] [G acc: 0.164]\n",
      "1215 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.406, 0.789, 0.598)] [G loss: 0.1] [G acc: 0.039]\n",
      "1216 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.430, 0.820, 0.625)] [G loss: 0.2] [G acc: 0.031]\n",
      "1217 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.406, 0.812, 0.609)] [G loss: 0.2] [G acc: 0.055]\n",
      "1218 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.867, 0.598)] [G loss: 0.2] [G acc: 0.016]\n",
      "1219 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.844, 0.559)] [G loss: 0.2] [G acc: 0.031]\n",
      "1220 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.695, 0.520)] [G loss: 0.1] [G acc: 0.078]\n",
      "1221 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.297, 0.852, 0.574)] [G loss: 0.2] [G acc: 0.039]\n",
      "1222 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.812, 0.582)] [G loss: 0.2] [G acc: 0.039]\n",
      "1223 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.242, 0.844, 0.543)] [G loss: 0.2] [G acc: 0.008]\n",
      "1224 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.305, 0.797, 0.551)] [G loss: 0.1] [G acc: 0.078]\n",
      "1225 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.352, 0.727, 0.539)] [G loss: 0.2] [G acc: 0.016]\n",
      "1226 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.430, 0.648, 0.539)] [G loss: 0.2] [G acc: 0.180]\n",
      "1227 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.367, 0.742, 0.555)] [G loss: 0.1] [G acc: 0.180]\n",
      "1228 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.281, 0.859, 0.570)] [G loss: 0.2] [G acc: 0.016]\n",
      "1229 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.430, 0.789, 0.609)] [G loss: 0.1] [G acc: 0.180]\n",
      "1230 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.656, 0.562)] [G loss: 0.2] [G acc: 0.102]\n",
      "1231 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.414, 0.758, 0.586)] [G loss: 0.2] [G acc: 0.203]\n",
      "1232 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.266, 0.852, 0.559)] [G loss: 0.2] [G acc: 0.023]\n",
      "1233 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.805, 0.562)] [G loss: 0.2] [G acc: 0.102]\n",
      "1234 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.445, 0.672, 0.559)] [G loss: 0.1] [G acc: 0.188]\n",
      "1235 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.672, 0.582)] [G loss: 0.1] [G acc: 0.062]\n",
      "1236 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.609, 0.578)] [G loss: 0.1] [G acc: 0.133]\n",
      "1237 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.711, 0.609)] [G loss: 0.1] [G acc: 0.156]\n",
      "1238 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.391, 0.672, 0.531)] [G loss: 0.1] [G acc: 0.172]\n",
      "1239 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.586, 0.512)] [G loss: 0.1] [G acc: 0.016]\n",
      "1240 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.586, 0.617, 0.602)] [G loss: 0.1] [G acc: 0.188]\n",
      "1241 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.609, 0.566)] [G loss: 0.1] [G acc: 0.258]\n",
      "1242 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.422, 0.766, 0.594)] [G loss: 0.1] [G acc: 0.133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.773, 0.578)] [G loss: 0.2] [G acc: 0.141]\n",
      "1244 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.695, 0.570)] [G loss: 0.1] [G acc: 0.148]\n",
      "1245 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.836, 0.578)] [G loss: 0.2] [G acc: 0.031]\n",
      "1246 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.781, 0.559)] [G loss: 0.1] [G acc: 0.125]\n",
      "1247 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.297, 0.922, 0.609)] [G loss: 0.2] [G acc: 0.102]\n",
      "1248 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.859, 0.566)] [G loss: 0.2] [G acc: 0.008]\n",
      "1249 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.219, 0.852, 0.535)] [G loss: 0.2] [G acc: 0.039]\n",
      "1250 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.820, 0.555)] [G loss: 0.2] [G acc: 0.156]\n",
      "1251 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.305, 0.797, 0.551)] [G loss: 0.2] [G acc: 0.008]\n",
      "1252 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.312, 0.891, 0.602)] [G loss: 0.2] [G acc: 0.016]\n",
      "1253 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.258, 0.867, 0.562)] [G loss: 0.2] [G acc: 0.031]\n",
      "1254 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.172, 0.883, 0.527)] [G loss: 0.2] [G acc: 0.047]\n",
      "1255 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.227, 0.883, 0.555)] [G loss: 0.2] [G acc: 0.016]\n",
      "1256 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.312, 0.922, 0.617)] [G loss: 0.2] [G acc: 0.023]\n",
      "1257 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.164, 0.969, 0.566)] [G loss: 0.2] [G acc: 0.008]\n",
      "1258 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.148, 0.969, 0.559)] [G loss: 0.2] [G acc: 0.008]\n",
      "1259 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.812, 0.605)] [G loss: 0.1] [G acc: 0.125]\n",
      "1260 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.758, 0.559)] [G loss: 0.1] [G acc: 0.039]\n",
      "1261 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.789, 0.555)] [G loss: 0.1] [G acc: 0.047]\n",
      "1262 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.562, 0.734, 0.648)] [G loss: 0.1] [G acc: 0.133]\n",
      "1263 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.531, 0.609, 0.570)] [G loss: 0.1] [G acc: 0.148]\n",
      "1264 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.609, 0.590)] [G loss: 0.2] [G acc: 0.203]\n",
      "1265 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.633, 0.598)] [G loss: 0.1] [G acc: 0.156]\n",
      "1266 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.672, 0.605)] [G loss: 0.1] [G acc: 0.148]\n",
      "1267 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.523, 0.543)] [G loss: 0.1] [G acc: 0.258]\n",
      "1268 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.609, 0.574)] [G loss: 0.1] [G acc: 0.297]\n",
      "1269 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.641, 0.508, 0.574)] [G loss: 0.1] [G acc: 0.125]\n",
      "1270 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.594, 0.484, 0.539)] [G loss: 0.1] [G acc: 0.367]\n",
      "1271 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.609, 0.562)] [G loss: 0.1] [G acc: 0.141]\n",
      "1272 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.594, 0.586)] [G loss: 0.1] [G acc: 0.172]\n",
      "1273 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.688, 0.621)] [G loss: 0.1] [G acc: 0.188]\n",
      "1274 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.664, 0.555)] [G loss: 0.1] [G acc: 0.297]\n",
      "1275 (5, 1) [D loss: (-0.1, 0.0, -0.0)] [D acc: (0.562, 0.531, 0.547)] [G loss: 0.1] [G acc: 0.250]\n",
      "1276 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.609, 0.574)] [G loss: 0.1] [G acc: 0.164]\n",
      "1277 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.406, 0.648, 0.527)] [G loss: 0.1] [G acc: 0.133]\n",
      "1278 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.766, 0.551)] [G loss: 0.2] [G acc: 0.109]\n",
      "1279 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.297, 0.820, 0.559)] [G loss: 0.2] [G acc: 0.070]\n",
      "1280 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.812, 0.574)] [G loss: 0.2] [G acc: 0.023]\n",
      "1281 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.414, 0.805, 0.609)] [G loss: 0.2] [G acc: 0.180]\n",
      "1282 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.273, 0.828, 0.551)] [G loss: 0.2] [G acc: 0.094]\n",
      "1283 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.305, 0.898, 0.602)] [G loss: 0.2] [G acc: 0.000]\n",
      "1284 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.734, 0.531)] [G loss: 0.2] [G acc: 0.023]\n",
      "1285 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.852, 0.613)] [G loss: 0.2] [G acc: 0.062]\n",
      "1286 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.430, 0.781, 0.605)] [G loss: 0.1] [G acc: 0.125]\n",
      "1287 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.406, 0.727, 0.566)] [G loss: 0.1] [G acc: 0.070]\n",
      "1288 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.797, 0.570)] [G loss: 0.1] [G acc: 0.062]\n",
      "1289 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.414, 0.633, 0.523)] [G loss: 0.1] [G acc: 0.109]\n",
      "1290 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.500, 0.656, 0.578)] [G loss: 0.1] [G acc: 0.180]\n",
      "1291 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.844, 0.586)] [G loss: 0.2] [G acc: 0.008]\n",
      "1292 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.859, 0.617)] [G loss: 0.2] [G acc: 0.133]\n",
      "1293 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.836, 0.594)] [G loss: 0.1] [G acc: 0.180]\n",
      "1294 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.930, 0.625)] [G loss: 0.2] [G acc: 0.047]\n",
      "1295 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.898, 0.613)] [G loss: 0.1] [G acc: 0.078]\n",
      "1296 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.281, 0.891, 0.586)] [G loss: 0.2] [G acc: 0.000]\n",
      "1297 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.750, 0.539)] [G loss: 0.1] [G acc: 0.148]\n",
      "1298 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.773, 0.598)] [G loss: 0.1] [G acc: 0.070]\n",
      "1299 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.367, 0.812, 0.590)] [G loss: 0.1] [G acc: 0.102]\n",
      "1300 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.531, 0.758, 0.645)] [G loss: 0.1] [G acc: 0.258]\n",
      "1301 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.594, 0.648, 0.621)] [G loss: 0.1] [G acc: 0.164]\n",
      "1302 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.609, 0.551)] [G loss: 0.1] [G acc: 0.109]\n",
      "1303 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.703, 0.566)] [G loss: 0.1] [G acc: 0.078]\n",
      "1304 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.398, 0.680, 0.539)] [G loss: 0.1] [G acc: 0.125]\n",
      "1305 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.812, 0.590)] [G loss: 0.1] [G acc: 0.062]\n",
      "1306 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.477, 0.766, 0.621)] [G loss: 0.2] [G acc: 0.094]\n",
      "1307 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.445, 0.711, 0.578)] [G loss: 0.1] [G acc: 0.039]\n",
      "1308 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.406, 0.742, 0.574)] [G loss: 0.1] [G acc: 0.156]\n",
      "1309 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.742, 0.551)] [G loss: 0.1] [G acc: 0.266]\n",
      "1310 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.633, 0.539)] [G loss: 0.1] [G acc: 0.125]\n",
      "1311 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.383, 0.703, 0.543)] [G loss: 0.1] [G acc: 0.117]\n",
      "1312 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.344, 0.672, 0.508)] [G loss: 0.1] [G acc: 0.086]\n",
      "1313 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.406, 0.742, 0.574)] [G loss: 0.1] [G acc: 0.070]\n",
      "1314 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.430, 0.789, 0.609)] [G loss: 0.1] [G acc: 0.188]\n",
      "1315 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.430, 0.688, 0.559)] [G loss: 0.1] [G acc: 0.242]\n",
      "1316 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.422, 0.688, 0.555)] [G loss: 0.1] [G acc: 0.219]\n",
      "1317 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.719, 0.566)] [G loss: 0.1] [G acc: 0.203]\n",
      "1318 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.781, 0.559)] [G loss: 0.1] [G acc: 0.055]\n",
      "1319 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.859, 0.574)] [G loss: 0.2] [G acc: 0.008]\n",
      "1320 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.875, 0.605)] [G loss: 0.2] [G acc: 0.023]\n",
      "1321 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.891, 0.609)] [G loss: 0.2] [G acc: 0.070]\n",
      "1322 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.289, 0.891, 0.590)] [G loss: 0.2] [G acc: 0.023]\n",
      "1323 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.297, 0.891, 0.594)] [G loss: 0.2] [G acc: 0.031]\n",
      "1324 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.305, 0.773, 0.539)] [G loss: 0.2] [G acc: 0.094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1325 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.867, 0.594)] [G loss: 0.2] [G acc: 0.039]\n",
      "1326 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.453, 0.805, 0.629)] [G loss: 0.1] [G acc: 0.109]\n",
      "1327 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.406, 0.719, 0.562)] [G loss: 0.1] [G acc: 0.141]\n",
      "1328 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.766, 0.578)] [G loss: 0.2] [G acc: 0.016]\n",
      "1329 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.859, 0.629)] [G loss: 0.2] [G acc: 0.086]\n",
      "1330 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.867, 0.617)] [G loss: 0.2] [G acc: 0.031]\n",
      "1331 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.773, 0.574)] [G loss: 0.2] [G acc: 0.031]\n",
      "1332 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.352, 0.734, 0.543)] [G loss: 0.1] [G acc: 0.102]\n",
      "1333 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.430, 0.789, 0.609)] [G loss: 0.1] [G acc: 0.070]\n",
      "1334 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.789, 0.613)] [G loss: 0.1] [G acc: 0.117]\n",
      "1335 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.438, 0.797, 0.617)] [G loss: 0.1] [G acc: 0.086]\n",
      "1336 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.414, 0.680, 0.547)] [G loss: 0.1] [G acc: 0.086]\n",
      "1337 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.680, 0.516)] [G loss: 0.1] [G acc: 0.195]\n",
      "1338 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.719, 0.523)] [G loss: 0.1] [G acc: 0.039]\n",
      "1339 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.430, 0.688, 0.559)] [G loss: 0.2] [G acc: 0.094]\n",
      "1340 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.391, 0.805, 0.598)] [G loss: 0.1] [G acc: 0.023]\n",
      "1341 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.453, 0.734, 0.594)] [G loss: 0.1] [G acc: 0.070]\n",
      "1342 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.484, 0.766, 0.625)] [G loss: 0.1] [G acc: 0.023]\n",
      "1343 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.484, 0.695, 0.590)] [G loss: 0.1] [G acc: 0.156]\n",
      "1344 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.742, 0.590)] [G loss: 0.1] [G acc: 0.055]\n",
      "1345 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.406, 0.781, 0.594)] [G loss: 0.1] [G acc: 0.109]\n",
      "1346 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.414, 0.703, 0.559)] [G loss: 0.1] [G acc: 0.180]\n",
      "1347 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.719, 0.574)] [G loss: 0.1] [G acc: 0.102]\n",
      "1348 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.734, 0.574)] [G loss: 0.1] [G acc: 0.070]\n",
      "1349 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.828, 0.590)] [G loss: 0.1] [G acc: 0.094]\n",
      "1350 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.875, 0.574)] [G loss: 0.1] [G acc: 0.016]\n",
      "1351 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.844, 0.582)] [G loss: 0.2] [G acc: 0.055]\n",
      "1352 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.414, 0.695, 0.555)] [G loss: 0.2] [G acc: 0.047]\n",
      "1353 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.266, 0.711, 0.488)] [G loss: 0.2] [G acc: 0.031]\n",
      "1354 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.211, 0.883, 0.547)] [G loss: 0.2] [G acc: 0.016]\n",
      "1355 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.281, 0.844, 0.562)] [G loss: 0.1] [G acc: 0.008]\n",
      "1356 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.781, 0.535)] [G loss: 0.2] [G acc: 0.070]\n",
      "1357 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.250, 0.891, 0.570)] [G loss: 0.1] [G acc: 0.039]\n",
      "1358 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.281, 0.820, 0.551)] [G loss: 0.2] [G acc: 0.000]\n",
      "1359 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.242, 0.914, 0.578)] [G loss: 0.2] [G acc: 0.016]\n",
      "1360 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.234, 0.961, 0.598)] [G loss: 0.2] [G acc: 0.016]\n",
      "1361 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.266, 0.852, 0.559)] [G loss: 0.1] [G acc: 0.016]\n",
      "1362 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.844, 0.566)] [G loss: 0.1] [G acc: 0.023]\n",
      "1363 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.383, 0.688, 0.535)] [G loss: 0.1] [G acc: 0.086]\n",
      "1364 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.805, 0.582)] [G loss: 0.1] [G acc: 0.133]\n",
      "1365 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.297, 0.820, 0.559)] [G loss: 0.1] [G acc: 0.031]\n",
      "1366 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.836, 0.609)] [G loss: 0.1] [G acc: 0.078]\n",
      "1367 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.734, 0.551)] [G loss: 0.1] [G acc: 0.188]\n",
      "1368 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.859, 0.602)] [G loss: 0.1] [G acc: 0.102]\n",
      "1369 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.461, 0.695, 0.578)] [G loss: 0.1] [G acc: 0.227]\n",
      "1370 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.406, 0.852, 0.629)] [G loss: 0.1] [G acc: 0.086]\n",
      "1371 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.820, 0.582)] [G loss: 0.2] [G acc: 0.039]\n",
      "1372 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.344, 0.828, 0.586)] [G loss: 0.2] [G acc: 0.016]\n",
      "1373 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.836, 0.578)] [G loss: 0.2] [G acc: 0.141]\n",
      "1374 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.312, 0.727, 0.520)] [G loss: 0.1] [G acc: 0.047]\n",
      "1375 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.852, 0.602)] [G loss: 0.1] [G acc: 0.039]\n",
      "1376 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.898, 0.613)] [G loss: 0.2] [G acc: 0.016]\n",
      "1377 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.422, 0.797, 0.609)] [G loss: 0.2] [G acc: 0.023]\n",
      "1378 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.711, 0.602)] [G loss: 0.1] [G acc: 0.141]\n",
      "1379 (5, 1) [D loss: (-0.0, -0.1, -0.1)] [D acc: (0.523, 0.789, 0.656)] [G loss: 0.1] [G acc: 0.156]\n",
      "1380 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.633, 0.551)] [G loss: 0.1] [G acc: 0.250]\n",
      "1381 (5, 1) [D loss: (0.0, -0.0, 0.0)] [D acc: (0.336, 0.578, 0.457)] [G loss: 0.2] [G acc: 0.062]\n",
      "1382 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.258, 0.789, 0.523)] [G loss: 0.2] [G acc: 0.102]\n",
      "1383 (5, 1) [D loss: (0.1, -0.2, -0.0)] [D acc: (0.219, 0.891, 0.555)] [G loss: 0.2] [G acc: 0.023]\n",
      "1384 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.266, 0.781, 0.523)] [G loss: 0.2] [G acc: 0.133]\n",
      "1385 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.242, 0.867, 0.555)] [G loss: 0.2] [G acc: 0.031]\n",
      "1386 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.203, 0.891, 0.547)] [G loss: 0.2] [G acc: 0.031]\n",
      "1387 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.352, 0.750, 0.551)] [G loss: 0.1] [G acc: 0.133]\n",
      "1388 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.695, 0.586)] [G loss: 0.1] [G acc: 0.047]\n",
      "1389 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.656, 0.594)] [G loss: 0.1] [G acc: 0.219]\n",
      "1390 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.641, 0.598)] [G loss: 0.1] [G acc: 0.312]\n",
      "1391 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.680, 0.555)] [G loss: 0.1] [G acc: 0.062]\n",
      "1392 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.883, 0.602)] [G loss: 0.1] [G acc: 0.055]\n",
      "1393 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.820, 0.590)] [G loss: 0.1] [G acc: 0.086]\n",
      "1394 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.328, 0.719, 0.523)] [G loss: 0.1] [G acc: 0.070]\n",
      "1395 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.742, 0.539)] [G loss: 0.1] [G acc: 0.070]\n",
      "1396 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.469, 0.750, 0.609)] [G loss: 0.1] [G acc: 0.086]\n",
      "1397 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.797, 0.590)] [G loss: 0.1] [G acc: 0.055]\n",
      "1398 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.883, 0.629)] [G loss: 0.1] [G acc: 0.047]\n",
      "1399 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.438, 0.820, 0.629)] [G loss: 0.1] [G acc: 0.117]\n",
      "1400 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.805, 0.562)] [G loss: 0.1] [G acc: 0.094]\n",
      "1401 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.883, 0.664)] [G loss: 0.2] [G acc: 0.039]\n",
      "1402 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.875, 0.582)] [G loss: 0.1] [G acc: 0.039]\n",
      "1403 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.383, 0.664, 0.523)] [G loss: 0.1] [G acc: 0.102]\n",
      "1404 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.742, 0.609)] [G loss: 0.1] [G acc: 0.094]\n",
      "1405 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.680, 0.559)] [G loss: 0.1] [G acc: 0.047]\n",
      "1406 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.781, 0.617)] [G loss: 0.1] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.414, 0.633, 0.523)] [G loss: 0.1] [G acc: 0.227]\n",
      "1408 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.406, 0.711, 0.559)] [G loss: 0.1] [G acc: 0.195]\n",
      "1409 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.438, 0.695, 0.566)] [G loss: 0.1] [G acc: 0.086]\n",
      "1410 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.734, 0.543)] [G loss: 0.1] [G acc: 0.016]\n",
      "1411 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.688, 0.555)] [G loss: 0.1] [G acc: 0.070]\n",
      "1412 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.758, 0.566)] [G loss: 0.1] [G acc: 0.102]\n",
      "1413 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.750, 0.539)] [G loss: 0.1] [G acc: 0.125]\n",
      "1414 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.922, 0.598)] [G loss: 0.2] [G acc: 0.023]\n",
      "1415 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.750, 0.586)] [G loss: 0.1] [G acc: 0.031]\n",
      "1416 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.430, 0.828, 0.629)] [G loss: 0.1] [G acc: 0.109]\n",
      "1417 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.500, 0.680, 0.590)] [G loss: 0.1] [G acc: 0.078]\n",
      "1418 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.633, 0.531)] [G loss: 0.1] [G acc: 0.164]\n",
      "1419 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.414, 0.758, 0.586)] [G loss: 0.1] [G acc: 0.258]\n",
      "1420 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.727, 0.547)] [G loss: 0.1] [G acc: 0.039]\n",
      "1421 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.859, 0.590)] [G loss: 0.2] [G acc: 0.016]\n",
      "1422 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.203, 0.875, 0.539)] [G loss: 0.2] [G acc: 0.000]\n",
      "1423 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.219, 0.828, 0.523)] [G loss: 0.2] [G acc: 0.102]\n",
      "1424 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.227, 0.867, 0.547)] [G loss: 0.2] [G acc: 0.039]\n",
      "1425 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.844, 0.582)] [G loss: 0.1] [G acc: 0.070]\n",
      "1426 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.219, 0.805, 0.512)] [G loss: 0.1] [G acc: 0.023]\n",
      "1427 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.758, 0.559)] [G loss: 0.1] [G acc: 0.078]\n",
      "1428 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.430, 0.750, 0.590)] [G loss: 0.1] [G acc: 0.195]\n",
      "1429 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.742, 0.582)] [G loss: 0.1] [G acc: 0.078]\n",
      "1430 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.352, 0.758, 0.555)] [G loss: 0.1] [G acc: 0.078]\n",
      "1431 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.375, 0.742, 0.559)] [G loss: 0.1] [G acc: 0.055]\n",
      "1432 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.797, 0.566)] [G loss: 0.1] [G acc: 0.070]\n",
      "1433 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.828, 0.559)] [G loss: 0.1] [G acc: 0.047]\n",
      "1434 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.734, 0.621)] [G loss: 0.1] [G acc: 0.102]\n",
      "1435 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.859, 0.652)] [G loss: 0.1] [G acc: 0.016]\n",
      "1436 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.742, 0.617)] [G loss: 0.1] [G acc: 0.125]\n",
      "1437 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.664, 0.547)] [G loss: 0.1] [G acc: 0.156]\n",
      "1438 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.367, 0.711, 0.539)] [G loss: 0.1] [G acc: 0.109]\n",
      "1439 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.734, 0.547)] [G loss: 0.1] [G acc: 0.156]\n",
      "1440 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.359, 0.633, 0.496)] [G loss: 0.1] [G acc: 0.195]\n",
      "1441 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.516, 0.695, 0.605)] [G loss: 0.1] [G acc: 0.070]\n",
      "1442 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.781, 0.570)] [G loss: 0.1] [G acc: 0.180]\n",
      "1443 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.625, 0.527)] [G loss: 0.1] [G acc: 0.266]\n",
      "1444 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.766, 0.574)] [G loss: 0.1] [G acc: 0.281]\n",
      "1445 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.773, 0.598)] [G loss: 0.1] [G acc: 0.016]\n",
      "1446 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.688, 0.566)] [G loss: 0.1] [G acc: 0.094]\n",
      "1447 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.867, 0.609)] [G loss: 0.1] [G acc: 0.062]\n",
      "1448 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.492, 0.703, 0.598)] [G loss: 0.1] [G acc: 0.227]\n",
      "1449 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.625, 0.562)] [G loss: 0.1] [G acc: 0.203]\n",
      "1450 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.648, 0.531)] [G loss: 0.1] [G acc: 0.039]\n",
      "1451 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.648, 0.590)] [G loss: 0.1] [G acc: 0.312]\n",
      "1452 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.523, 0.664, 0.594)] [G loss: 0.1] [G acc: 0.070]\n",
      "1453 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.719, 0.578)] [G loss: 0.1] [G acc: 0.078]\n",
      "1454 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.617, 0.523)] [G loss: 0.1] [G acc: 0.031]\n",
      "1455 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.641, 0.527)] [G loss: 0.1] [G acc: 0.156]\n",
      "1456 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.656, 0.543)] [G loss: 0.1] [G acc: 0.039]\n",
      "1457 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.609, 0.551)] [G loss: 0.1] [G acc: 0.172]\n",
      "1458 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.594, 0.520)] [G loss: 0.1] [G acc: 0.219]\n",
      "1459 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.734, 0.582)] [G loss: 0.1] [G acc: 0.086]\n",
      "1460 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.750, 0.574)] [G loss: 0.1] [G acc: 0.117]\n",
      "1461 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.719, 0.570)] [G loss: 0.1] [G acc: 0.094]\n",
      "1462 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.328, 0.719, 0.523)] [G loss: 0.1] [G acc: 0.125]\n",
      "1463 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.711, 0.551)] [G loss: 0.1] [G acc: 0.086]\n",
      "1464 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.391, 0.695, 0.543)] [G loss: 0.1] [G acc: 0.102]\n",
      "1465 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.469, 0.664, 0.566)] [G loss: 0.1] [G acc: 0.094]\n",
      "1466 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.820, 0.602)] [G loss: 0.1] [G acc: 0.055]\n",
      "1467 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.719, 0.539)] [G loss: 0.1] [G acc: 0.141]\n",
      "1468 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.500, 0.742, 0.621)] [G loss: 0.1] [G acc: 0.125]\n",
      "1469 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.688, 0.590)] [G loss: 0.1] [G acc: 0.125]\n",
      "1470 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.414, 0.766, 0.590)] [G loss: 0.1] [G acc: 0.086]\n",
      "1471 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.297, 0.719, 0.508)] [G loss: 0.1] [G acc: 0.055]\n",
      "1472 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.750, 0.539)] [G loss: 0.1] [G acc: 0.039]\n",
      "1473 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.188, 0.852, 0.520)] [G loss: 0.1] [G acc: 0.102]\n",
      "1474 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.234, 0.766, 0.500)] [G loss: 0.1] [G acc: 0.062]\n",
      "1475 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.297, 0.852, 0.574)] [G loss: 0.1] [G acc: 0.023]\n",
      "1476 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.227, 0.820, 0.523)] [G loss: 0.1] [G acc: 0.000]\n",
      "1477 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.266, 0.898, 0.582)] [G loss: 0.1] [G acc: 0.047]\n",
      "1478 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.305, 0.828, 0.566)] [G loss: 0.1] [G acc: 0.055]\n",
      "1479 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.672, 0.516)] [G loss: 0.1] [G acc: 0.094]\n",
      "1480 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.281, 0.859, 0.570)] [G loss: 0.1] [G acc: 0.047]\n",
      "1481 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.281, 0.805, 0.543)] [G loss: 0.1] [G acc: 0.055]\n",
      "1482 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.820, 0.555)] [G loss: 0.1] [G acc: 0.055]\n",
      "1483 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.773, 0.582)] [G loss: 0.1] [G acc: 0.180]\n",
      "1484 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.406, 0.719, 0.562)] [G loss: 0.1] [G acc: 0.195]\n",
      "1485 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.625, 0.559)] [G loss: 0.1] [G acc: 0.141]\n",
      "1486 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.711, 0.547)] [G loss: 0.1] [G acc: 0.266]\n",
      "1487 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.375, 0.594, 0.484)] [G loss: 0.1] [G acc: 0.211]\n",
      "1488 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.625, 0.547)] [G loss: 0.1] [G acc: 0.031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.430, 0.688, 0.559)] [G loss: 0.1] [G acc: 0.078]\n",
      "1490 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.461, 0.734, 0.598)] [G loss: 0.1] [G acc: 0.109]\n",
      "1491 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.406, 0.750, 0.578)] [G loss: 0.1] [G acc: 0.086]\n",
      "1492 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.383, 0.641, 0.512)] [G loss: 0.1] [G acc: 0.227]\n",
      "1493 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.758, 0.578)] [G loss: 0.1] [G acc: 0.086]\n",
      "1494 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.672, 0.582)] [G loss: 0.1] [G acc: 0.164]\n",
      "1495 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.648, 0.633)] [G loss: 0.1] [G acc: 0.117]\n",
      "1496 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.484, 0.570, 0.527)] [G loss: 0.1] [G acc: 0.203]\n",
      "1497 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.609, 0.520)] [G loss: 0.1] [G acc: 0.281]\n",
      "1498 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.367, 0.695, 0.531)] [G loss: 0.1] [G acc: 0.070]\n",
      "1499 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.383, 0.711, 0.547)] [G loss: 0.1] [G acc: 0.070]\n",
      "1500 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.719, 0.543)] [G loss: 0.1] [G acc: 0.086]\n",
      "1501 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.227, 0.883, 0.555)] [G loss: 0.1] [G acc: 0.016]\n",
      "1502 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.773, 0.562)] [G loss: 0.1] [G acc: 0.102]\n",
      "1503 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.891, 0.590)] [G loss: 0.1] [G acc: 0.000]\n",
      "1504 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.234, 0.898, 0.566)] [G loss: 0.1] [G acc: 0.023]\n",
      "1505 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.258, 0.812, 0.535)] [G loss: 0.1] [G acc: 0.164]\n",
      "1506 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.250, 0.836, 0.543)] [G loss: 0.1] [G acc: 0.141]\n",
      "1507 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.328, 0.867, 0.598)] [G loss: 0.1] [G acc: 0.109]\n",
      "1508 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.648, 0.523)] [G loss: 0.1] [G acc: 0.188]\n",
      "1509 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.312, 0.805, 0.559)] [G loss: 0.1] [G acc: 0.062]\n",
      "1510 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.805, 0.586)] [G loss: 0.1] [G acc: 0.062]\n",
      "1511 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.766, 0.578)] [G loss: 0.1] [G acc: 0.102]\n",
      "1512 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.656, 0.586)] [G loss: 0.1] [G acc: 0.227]\n",
      "1513 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.656, 0.613)] [G loss: 0.1] [G acc: 0.180]\n",
      "1514 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.609, 0.609, 0.609)] [G loss: 0.1] [G acc: 0.102]\n",
      "1515 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.586, 0.562)] [G loss: 0.1] [G acc: 0.188]\n",
      "1516 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.602, 0.566)] [G loss: 0.1] [G acc: 0.188]\n",
      "1517 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.609, 0.703, 0.656)] [G loss: 0.1] [G acc: 0.180]\n",
      "1518 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.656, 0.586)] [G loss: 0.1] [G acc: 0.125]\n",
      "1519 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.422, 0.641, 0.531)] [G loss: 0.1] [G acc: 0.125]\n",
      "1520 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.391, 0.633, 0.512)] [G loss: 0.1] [G acc: 0.117]\n",
      "1521 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.625, 0.539)] [G loss: 0.1] [G acc: 0.109]\n",
      "1522 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.367, 0.766, 0.566)] [G loss: 0.1] [G acc: 0.078]\n",
      "1523 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.758, 0.578)] [G loss: 0.1] [G acc: 0.055]\n",
      "1524 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.281, 0.852, 0.566)] [G loss: 0.1] [G acc: 0.141]\n",
      "1525 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.242, 0.852, 0.547)] [G loss: 0.1] [G acc: 0.047]\n",
      "1526 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.852, 0.562)] [G loss: 0.1] [G acc: 0.039]\n",
      "1527 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.203, 0.812, 0.508)] [G loss: 0.1] [G acc: 0.164]\n",
      "1528 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.258, 0.805, 0.531)] [G loss: 0.1] [G acc: 0.078]\n",
      "1529 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.266, 0.656, 0.461)] [G loss: 0.1] [G acc: 0.094]\n",
      "1530 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.805, 0.570)] [G loss: 0.1] [G acc: 0.039]\n",
      "1531 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.758, 0.523)] [G loss: 0.1] [G acc: 0.031]\n",
      "1532 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.391, 0.781, 0.586)] [G loss: 0.1] [G acc: 0.039]\n",
      "1533 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.406, 0.781, 0.594)] [G loss: 0.1] [G acc: 0.039]\n",
      "1534 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.750, 0.629)] [G loss: 0.1] [G acc: 0.164]\n",
      "1535 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.508, 0.492)] [G loss: 0.1] [G acc: 0.250]\n",
      "1536 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.477, 0.523, 0.500)] [G loss: 0.1] [G acc: 0.266]\n",
      "1537 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.617, 0.578)] [G loss: 0.0] [G acc: 0.281]\n",
      "1538 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.477, 0.578, 0.527)] [G loss: 0.1] [G acc: 0.133]\n",
      "1539 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.812, 0.637)] [G loss: 0.1] [G acc: 0.047]\n",
      "1540 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.812, 0.543)] [G loss: 0.1] [G acc: 0.016]\n",
      "1541 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.297, 0.836, 0.566)] [G loss: 0.1] [G acc: 0.125]\n",
      "1542 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.883, 0.578)] [G loss: 0.1] [G acc: 0.047]\n",
      "1543 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.227, 0.781, 0.504)] [G loss: 0.1] [G acc: 0.031]\n",
      "1544 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.734, 0.504)] [G loss: 0.1] [G acc: 0.031]\n",
      "1545 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.242, 0.891, 0.566)] [G loss: 0.1] [G acc: 0.047]\n",
      "1546 (5, 1) [D loss: (0.1, -0.1, -0.0)] [D acc: (0.195, 0.938, 0.566)] [G loss: 0.1] [G acc: 0.008]\n",
      "1547 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.273, 0.773, 0.523)] [G loss: 0.1] [G acc: 0.039]\n",
      "1548 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.250, 0.828, 0.539)] [G loss: 0.1] [G acc: 0.078]\n",
      "1549 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.719, 0.559)] [G loss: 0.1] [G acc: 0.078]\n",
      "1550 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.336, 0.742, 0.539)] [G loss: 0.1] [G acc: 0.086]\n",
      "1551 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.414, 0.734, 0.574)] [G loss: 0.1] [G acc: 0.172]\n",
      "1552 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.656, 0.562)] [G loss: 0.1] [G acc: 0.211]\n",
      "1553 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.781, 0.566)] [G loss: 0.1] [G acc: 0.133]\n",
      "1554 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.609, 0.574)] [G loss: 0.1] [G acc: 0.148]\n",
      "1555 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.406, 0.789, 0.598)] [G loss: 0.1] [G acc: 0.039]\n",
      "1556 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.445, 0.672, 0.559)] [G loss: 0.1] [G acc: 0.125]\n",
      "1557 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.719, 0.566)] [G loss: 0.1] [G acc: 0.133]\n",
      "1558 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.500, 0.617, 0.559)] [G loss: 0.1] [G acc: 0.117]\n",
      "1559 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.539, 0.688, 0.613)] [G loss: 0.1] [G acc: 0.039]\n",
      "1560 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.469, 0.617, 0.543)] [G loss: 0.1] [G acc: 0.164]\n",
      "1561 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.469, 0.648, 0.559)] [G loss: 0.1] [G acc: 0.109]\n",
      "1562 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.617, 0.590)] [G loss: 0.1] [G acc: 0.125]\n",
      "1563 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.609, 0.562)] [G loss: 0.0] [G acc: 0.273]\n",
      "1564 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.680, 0.629)] [G loss: 0.1] [G acc: 0.133]\n",
      "1565 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.555, 0.523)] [G loss: 0.1] [G acc: 0.156]\n",
      "1566 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.656, 0.578)] [G loss: 0.1] [G acc: 0.180]\n",
      "1567 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.633, 0.547)] [G loss: 0.1] [G acc: 0.148]\n",
      "1568 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.633, 0.566)] [G loss: 0.1] [G acc: 0.195]\n",
      "1569 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.633, 0.535)] [G loss: 0.1] [G acc: 0.180]\n",
      "1570 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.633, 0.539)] [G loss: 0.1] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1571 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.766, 0.578)] [G loss: 0.1] [G acc: 0.125]\n",
      "1572 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.680, 0.508)] [G loss: 0.1] [G acc: 0.141]\n",
      "1573 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.789, 0.562)] [G loss: 0.1] [G acc: 0.094]\n",
      "1574 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.766, 0.551)] [G loss: 0.1] [G acc: 0.117]\n",
      "1575 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.367, 0.773, 0.570)] [G loss: 0.1] [G acc: 0.102]\n",
      "1576 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.289, 0.836, 0.562)] [G loss: 0.1] [G acc: 0.102]\n",
      "1577 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.250, 0.742, 0.496)] [G loss: 0.1] [G acc: 0.109]\n",
      "1578 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.227, 0.773, 0.500)] [G loss: 0.1] [G acc: 0.070]\n",
      "1579 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.320, 0.820, 0.570)] [G loss: 0.1] [G acc: 0.109]\n",
      "1580 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.375, 0.766, 0.570)] [G loss: 0.1] [G acc: 0.164]\n",
      "1581 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.438, 0.766, 0.602)] [G loss: 0.1] [G acc: 0.188]\n",
      "1582 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.430, 0.664, 0.547)] [G loss: 0.1] [G acc: 0.258]\n",
      "1583 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.695, 0.598)] [G loss: 0.1] [G acc: 0.234]\n",
      "1584 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.484, 0.516, 0.500)] [G loss: 0.1] [G acc: 0.219]\n",
      "1585 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.461, 0.633, 0.547)] [G loss: 0.1] [G acc: 0.305]\n",
      "1586 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.539, 0.484)] [G loss: 0.1] [G acc: 0.234]\n",
      "1587 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.602, 0.574)] [G loss: 0.1] [G acc: 0.172]\n",
      "1588 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.578, 0.551)] [G loss: 0.1] [G acc: 0.180]\n",
      "1589 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.695, 0.598)] [G loss: 0.1] [G acc: 0.164]\n",
      "1590 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.391, 0.648, 0.520)] [G loss: 0.1] [G acc: 0.078]\n",
      "1591 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.633, 0.547)] [G loss: 0.1] [G acc: 0.148]\n",
      "1592 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.406, 0.547, 0.477)] [G loss: 0.1] [G acc: 0.102]\n",
      "1593 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.352, 0.656, 0.504)] [G loss: 0.1] [G acc: 0.125]\n",
      "1594 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.391, 0.727, 0.559)] [G loss: 0.1] [G acc: 0.109]\n",
      "1595 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.406, 0.703, 0.555)] [G loss: 0.1] [G acc: 0.078]\n",
      "1596 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.383, 0.734, 0.559)] [G loss: 0.1] [G acc: 0.078]\n",
      "1597 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.469, 0.758, 0.613)] [G loss: 0.1] [G acc: 0.109]\n",
      "1598 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.445, 0.727, 0.586)] [G loss: 0.1] [G acc: 0.172]\n",
      "1599 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.734, 0.582)] [G loss: 0.1] [G acc: 0.062]\n",
      "1600 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.234, 0.836, 0.535)] [G loss: 0.1] [G acc: 0.055]\n",
      "1601 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.406, 0.750, 0.578)] [G loss: 0.1] [G acc: 0.102]\n",
      "1602 (5, 1) [D loss: (0.0, -0.0, 0.0)] [D acc: (0.320, 0.734, 0.527)] [G loss: 0.1] [G acc: 0.055]\n",
      "1603 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.383, 0.727, 0.555)] [G loss: 0.1] [G acc: 0.133]\n",
      "1604 (5, 1) [D loss: (0.0, -0.0, 0.0)] [D acc: (0.375, 0.602, 0.488)] [G loss: 0.1] [G acc: 0.070]\n",
      "1605 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.445, 0.711, 0.578)] [G loss: 0.1] [G acc: 0.133]\n",
      "1606 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.695, 0.578)] [G loss: 0.1] [G acc: 0.125]\n",
      "1607 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.680, 0.555)] [G loss: 0.1] [G acc: 0.156]\n",
      "1608 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.680, 0.633)] [G loss: 0.1] [G acc: 0.102]\n",
      "1609 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.625, 0.594, 0.609)] [G loss: 0.1] [G acc: 0.266]\n",
      "1610 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.680, 0.582)] [G loss: 0.1] [G acc: 0.117]\n",
      "1611 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.617, 0.555)] [G loss: 0.1] [G acc: 0.125]\n",
      "1612 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.625, 0.578)] [G loss: 0.1] [G acc: 0.203]\n",
      "1613 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.422, 0.586, 0.504)] [G loss: 0.1] [G acc: 0.148]\n",
      "1614 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.516, 0.512)] [G loss: 0.1] [G acc: 0.148]\n",
      "1615 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.391, 0.609, 0.500)] [G loss: 0.1] [G acc: 0.227]\n",
      "1616 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.672, 0.602)] [G loss: 0.1] [G acc: 0.133]\n",
      "1617 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.398, 0.695, 0.547)] [G loss: 0.1] [G acc: 0.109]\n",
      "1618 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.359, 0.789, 0.574)] [G loss: 0.1] [G acc: 0.117]\n",
      "1619 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.438, 0.617, 0.527)] [G loss: 0.1] [G acc: 0.180]\n",
      "1620 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.719, 0.570)] [G loss: 0.1] [G acc: 0.164]\n",
      "1621 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.680, 0.555)] [G loss: 0.1] [G acc: 0.141]\n",
      "1622 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.305, 0.758, 0.531)] [G loss: 0.1] [G acc: 0.141]\n",
      "1623 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.656, 0.527)] [G loss: 0.1] [G acc: 0.172]\n",
      "1624 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.242, 0.852, 0.547)] [G loss: 0.1] [G acc: 0.109]\n",
      "1625 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.312, 0.781, 0.547)] [G loss: 0.1] [G acc: 0.062]\n",
      "1626 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.758, 0.578)] [G loss: 0.1] [G acc: 0.141]\n",
      "1627 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.797, 0.609)] [G loss: 0.1] [G acc: 0.148]\n",
      "1628 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.367, 0.695, 0.531)] [G loss: 0.1] [G acc: 0.078]\n",
      "1629 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.742, 0.578)] [G loss: 0.1] [G acc: 0.156]\n",
      "1630 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.688, 0.590)] [G loss: 0.1] [G acc: 0.133]\n",
      "1631 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.625, 0.562)] [G loss: 0.1] [G acc: 0.227]\n",
      "1632 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.734, 0.641)] [G loss: 0.1] [G acc: 0.148]\n",
      "1633 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.617, 0.551)] [G loss: 0.1] [G acc: 0.312]\n",
      "1634 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.531, 0.555)] [G loss: 0.0] [G acc: 0.328]\n",
      "1635 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.570, 0.566)] [G loss: 0.0] [G acc: 0.297]\n",
      "1636 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.578, 0.562)] [G loss: 0.1] [G acc: 0.156]\n",
      "1637 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.516, 0.527)] [G loss: 0.1] [G acc: 0.109]\n",
      "1638 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.633, 0.602)] [G loss: 0.1] [G acc: 0.195]\n",
      "1639 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.531, 0.727, 0.629)] [G loss: 0.1] [G acc: 0.297]\n",
      "1640 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.625, 0.547, 0.586)] [G loss: 0.1] [G acc: 0.305]\n",
      "1641 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.617, 0.570)] [G loss: 0.0] [G acc: 0.414]\n",
      "1642 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.602, 0.516, 0.559)] [G loss: 0.0] [G acc: 0.422]\n",
      "1643 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.500, 0.500, 0.500)] [G loss: 0.1] [G acc: 0.258]\n",
      "1644 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.578, 0.539)] [G loss: 0.1] [G acc: 0.336]\n",
      "1645 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.664, 0.543)] [G loss: 0.1] [G acc: 0.039]\n",
      "1646 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.750, 0.586)] [G loss: 0.1] [G acc: 0.062]\n",
      "1647 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.352, 0.711, 0.531)] [G loss: 0.1] [G acc: 0.172]\n",
      "1648 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.609, 0.566)] [G loss: 0.1] [G acc: 0.227]\n",
      "1649 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.680, 0.570)] [G loss: 0.1] [G acc: 0.273]\n",
      "1650 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.648, 0.562)] [G loss: 0.1] [G acc: 0.172]\n",
      "1651 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.641, 0.547)] [G loss: 0.0] [G acc: 0.242]\n",
      "1652 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.609, 0.508, 0.559)] [G loss: 0.0] [G acc: 0.352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1653 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.711, 0.629)] [G loss: 0.1] [G acc: 0.234]\n",
      "1654 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.555, 0.535)] [G loss: 0.1] [G acc: 0.242]\n",
      "1655 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.633, 0.641, 0.637)] [G loss: 0.1] [G acc: 0.234]\n",
      "1656 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.531, 0.574)] [G loss: 0.1] [G acc: 0.375]\n",
      "1657 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.641, 0.422, 0.531)] [G loss: 0.0] [G acc: 0.461]\n",
      "1658 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.500, 0.559)] [G loss: 0.0] [G acc: 0.508]\n",
      "1659 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.570, 0.500, 0.535)] [G loss: 0.0] [G acc: 0.422]\n",
      "1660 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.602, 0.547, 0.574)] [G loss: 0.0] [G acc: 0.219]\n",
      "1661 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.578, 0.570, 0.574)] [G loss: 0.1] [G acc: 0.195]\n",
      "1662 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.633, 0.578)] [G loss: 0.1] [G acc: 0.242]\n",
      "1663 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.516, 0.586, 0.551)] [G loss: 0.1] [G acc: 0.203]\n",
      "1664 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.625, 0.566)] [G loss: 0.1] [G acc: 0.164]\n",
      "1665 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.312, 0.789, 0.551)] [G loss: 0.1] [G acc: 0.055]\n",
      "1666 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.391, 0.766, 0.578)] [G loss: 0.1] [G acc: 0.070]\n",
      "1667 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.742, 0.570)] [G loss: 0.1] [G acc: 0.055]\n",
      "1668 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.297, 0.805, 0.551)] [G loss: 0.1] [G acc: 0.109]\n",
      "1669 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.359, 0.859, 0.609)] [G loss: 0.1] [G acc: 0.094]\n",
      "1670 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.352, 0.836, 0.594)] [G loss: 0.1] [G acc: 0.109]\n",
      "1671 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.344, 0.781, 0.562)] [G loss: 0.1] [G acc: 0.109]\n",
      "1672 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.602, 0.500)] [G loss: 0.1] [G acc: 0.227]\n",
      "1673 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.609, 0.527)] [G loss: 0.1] [G acc: 0.203]\n",
      "1674 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.336, 0.812, 0.574)] [G loss: 0.1] [G acc: 0.055]\n",
      "1675 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.438, 0.797, 0.617)] [G loss: 0.1] [G acc: 0.047]\n",
      "1676 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.477, 0.703, 0.590)] [G loss: 0.1] [G acc: 0.047]\n",
      "1677 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.633, 0.605)] [G loss: 0.1] [G acc: 0.211]\n",
      "1678 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.656, 0.523, 0.590)] [G loss: 0.0] [G acc: 0.328]\n",
      "1679 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.609, 0.586, 0.598)] [G loss: 0.1] [G acc: 0.266]\n",
      "1680 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.734, 0.461, 0.598)] [G loss: 0.0] [G acc: 0.289]\n",
      "1681 (5, 1) [D loss: (-0.1, -0.0, -0.0)] [D acc: (0.727, 0.547, 0.637)] [G loss: 0.0] [G acc: 0.281]\n",
      "1682 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.602, 0.453, 0.527)] [G loss: 0.0] [G acc: 0.258]\n",
      "1683 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.547, 0.484, 0.516)] [G loss: 0.0] [G acc: 0.258]\n",
      "1684 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.672, 0.523, 0.598)] [G loss: 0.0] [G acc: 0.297]\n",
      "1685 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.586, 0.566)] [G loss: 0.1] [G acc: 0.188]\n",
      "1686 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.656, 0.629)] [G loss: 0.1] [G acc: 0.172]\n",
      "1687 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.617, 0.559)] [G loss: 0.1] [G acc: 0.188]\n",
      "1688 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.594, 0.598)] [G loss: 0.1] [G acc: 0.305]\n",
      "1689 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.547, 0.520)] [G loss: 0.0] [G acc: 0.414]\n",
      "1690 (5, 1) [D loss: (0.0, -0.0, 0.0)] [D acc: (0.477, 0.555, 0.516)] [G loss: 0.1] [G acc: 0.258]\n",
      "1691 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.664, 0.602)] [G loss: 0.1] [G acc: 0.164]\n",
      "1692 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.805, 0.613)] [G loss: 0.1] [G acc: 0.086]\n",
      "1693 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.398, 0.789, 0.594)] [G loss: 0.1] [G acc: 0.078]\n",
      "1694 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.383, 0.727, 0.555)] [G loss: 0.1] [G acc: 0.164]\n",
      "1695 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.664, 0.570)] [G loss: 0.1] [G acc: 0.172]\n",
      "1696 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.633, 0.574)] [G loss: 0.1] [G acc: 0.180]\n",
      "1697 (5, 1) [D loss: (0.0, -0.0, 0.0)] [D acc: (0.359, 0.578, 0.469)] [G loss: 0.1] [G acc: 0.219]\n",
      "1698 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.531, 0.672, 0.602)] [G loss: 0.1] [G acc: 0.234]\n",
      "1699 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.766, 0.602)] [G loss: 0.1] [G acc: 0.195]\n",
      "1700 (5, 1) [D loss: (-0.0, 0.0, 0.0)] [D acc: (0.414, 0.516, 0.465)] [G loss: 0.1] [G acc: 0.117]\n",
      "1701 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.594, 0.617, 0.605)] [G loss: 0.1] [G acc: 0.219]\n",
      "1702 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.578, 0.516, 0.547)] [G loss: 0.1] [G acc: 0.164]\n",
      "1703 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.672, 0.484, 0.578)] [G loss: 0.0] [G acc: 0.266]\n",
      "1704 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.711, 0.406, 0.559)] [G loss: 0.0] [G acc: 0.164]\n",
      "1705 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.664, 0.422, 0.543)] [G loss: 0.0] [G acc: 0.195]\n",
      "1706 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.656, 0.531, 0.594)] [G loss: 0.0] [G acc: 0.305]\n",
      "1707 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.555, 0.578)] [G loss: 0.0] [G acc: 0.172]\n",
      "1708 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.641, 0.516, 0.578)] [G loss: 0.0] [G acc: 0.234]\n",
      "1709 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.617, 0.516, 0.566)] [G loss: 0.0] [G acc: 0.227]\n",
      "1710 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.648, 0.500, 0.574)] [G loss: 0.0] [G acc: 0.242]\n",
      "1711 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.523, 0.570)] [G loss: 0.1] [G acc: 0.164]\n",
      "1712 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.531, 0.539)] [G loss: 0.1] [G acc: 0.234]\n",
      "1713 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.609, 0.523, 0.566)] [G loss: 0.0] [G acc: 0.281]\n",
      "1714 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.641, 0.551)] [G loss: 0.1] [G acc: 0.172]\n",
      "1715 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.766, 0.641)] [G loss: 0.1] [G acc: 0.055]\n",
      "1716 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.477, 0.672, 0.574)] [G loss: 0.1] [G acc: 0.125]\n",
      "1717 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.492, 0.695, 0.594)] [G loss: 0.1] [G acc: 0.086]\n",
      "1718 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.391, 0.672, 0.531)] [G loss: 0.1] [G acc: 0.055]\n",
      "1719 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.352, 0.812, 0.582)] [G loss: 0.1] [G acc: 0.109]\n",
      "1720 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.734, 0.578)] [G loss: 0.1] [G acc: 0.117]\n",
      "1721 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.781, 0.605)] [G loss: 0.1] [G acc: 0.203]\n",
      "1722 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.391, 0.625, 0.508)] [G loss: 0.1] [G acc: 0.109]\n",
      "1723 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.719, 0.578)] [G loss: 0.1] [G acc: 0.250]\n",
      "1724 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.289, 0.727, 0.508)] [G loss: 0.1] [G acc: 0.070]\n",
      "1725 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.570, 0.566)] [G loss: 0.1] [G acc: 0.156]\n",
      "1726 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.719, 0.641)] [G loss: 0.1] [G acc: 0.219]\n",
      "1727 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.586, 0.574)] [G loss: 0.1] [G acc: 0.266]\n",
      "1728 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.508, 0.562)] [G loss: 0.0] [G acc: 0.344]\n",
      "1729 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.523, 0.562)] [G loss: 0.1] [G acc: 0.141]\n",
      "1730 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.578, 0.590)] [G loss: 0.1] [G acc: 0.164]\n",
      "1731 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.555, 0.531)] [G loss: 0.0] [G acc: 0.250]\n",
      "1732 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.602, 0.523, 0.562)] [G loss: 0.1] [G acc: 0.188]\n",
      "1733 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.633, 0.551)] [G loss: 0.1] [G acc: 0.242]\n",
      "1734 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.617, 0.574)] [G loss: 0.1] [G acc: 0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1735 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.617, 0.578)] [G loss: 0.1] [G acc: 0.195]\n",
      "1736 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.727, 0.633)] [G loss: 0.1] [G acc: 0.102]\n",
      "1737 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.516, 0.688, 0.602)] [G loss: 0.1] [G acc: 0.305]\n",
      "1738 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.695, 0.598)] [G loss: 0.1] [G acc: 0.148]\n",
      "1739 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.719, 0.590)] [G loss: 0.1] [G acc: 0.109]\n",
      "1740 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.422, 0.680, 0.551)] [G loss: 0.1] [G acc: 0.375]\n",
      "1741 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.453, 0.695, 0.574)] [G loss: 0.1] [G acc: 0.180]\n",
      "1742 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.680, 0.594)] [G loss: 0.1] [G acc: 0.117]\n",
      "1743 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.633, 0.590)] [G loss: 0.1] [G acc: 0.180]\n",
      "1744 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.641, 0.594)] [G loss: 0.1] [G acc: 0.125]\n",
      "1745 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.594, 0.617, 0.605)] [G loss: 0.1] [G acc: 0.289]\n",
      "1746 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.625, 0.613)] [G loss: 0.1] [G acc: 0.289]\n",
      "1747 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.523, 0.539)] [G loss: 0.0] [G acc: 0.352]\n",
      "1748 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.539, 0.523)] [G loss: 0.0] [G acc: 0.211]\n",
      "1749 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.570, 0.586)] [G loss: 0.1] [G acc: 0.156]\n",
      "1750 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.688, 0.594)] [G loss: 0.1] [G acc: 0.180]\n",
      "1751 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.648, 0.609)] [G loss: 0.1] [G acc: 0.109]\n",
      "1752 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.570, 0.570)] [G loss: 0.1] [G acc: 0.172]\n",
      "1753 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.492, 0.688, 0.590)] [G loss: 0.1] [G acc: 0.156]\n",
      "1754 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.531, 0.508)] [G loss: 0.1] [G acc: 0.242]\n",
      "1755 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.641, 0.594)] [G loss: 0.1] [G acc: 0.320]\n",
      "1756 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.680, 0.602)] [G loss: 0.1] [G acc: 0.203]\n",
      "1757 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.594, 0.551)] [G loss: 0.1] [G acc: 0.219]\n",
      "1758 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.617, 0.559)] [G loss: 0.1] [G acc: 0.359]\n",
      "1759 (5, 1) [D loss: (0.0, -0.0, 0.0)] [D acc: (0.477, 0.531, 0.504)] [G loss: 0.1] [G acc: 0.242]\n",
      "1760 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.594, 0.566)] [G loss: 0.1] [G acc: 0.180]\n",
      "1761 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.633, 0.559)] [G loss: 0.1] [G acc: 0.250]\n",
      "1762 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.648, 0.582)] [G loss: 0.1] [G acc: 0.125]\n",
      "1763 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.453, 0.641, 0.547)] [G loss: 0.1] [G acc: 0.109]\n",
      "1764 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.609, 0.551)] [G loss: 0.1] [G acc: 0.141]\n",
      "1765 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.711, 0.633)] [G loss: 0.1] [G acc: 0.164]\n",
      "1766 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.688, 0.605)] [G loss: 0.1] [G acc: 0.211]\n",
      "1767 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.734, 0.633)] [G loss: 0.1] [G acc: 0.133]\n",
      "1768 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.688, 0.613)] [G loss: 0.1] [G acc: 0.141]\n",
      "1769 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.711, 0.566)] [G loss: 0.1] [G acc: 0.211]\n",
      "1770 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.609, 0.551)] [G loss: 0.1] [G acc: 0.227]\n",
      "1771 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.672, 0.609)] [G loss: 0.1] [G acc: 0.211]\n",
      "1772 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.664, 0.609)] [G loss: 0.1] [G acc: 0.102]\n",
      "1773 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.484, 0.680, 0.582)] [G loss: 0.1] [G acc: 0.219]\n",
      "1774 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.594, 0.566)] [G loss: 0.1] [G acc: 0.242]\n",
      "1775 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.695, 0.605)] [G loss: 0.1] [G acc: 0.125]\n",
      "1776 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.648, 0.566)] [G loss: 0.1] [G acc: 0.148]\n",
      "1777 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.469, 0.719, 0.594)] [G loss: 0.1] [G acc: 0.078]\n",
      "1778 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.609, 0.539)] [G loss: 0.1] [G acc: 0.117]\n",
      "1779 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.344, 0.750, 0.547)] [G loss: 0.1] [G acc: 0.062]\n",
      "1780 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.672, 0.543)] [G loss: 0.1] [G acc: 0.148]\n",
      "1781 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.453, 0.719, 0.586)] [G loss: 0.1] [G acc: 0.148]\n",
      "1782 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.648, 0.586)] [G loss: 0.1] [G acc: 0.320]\n",
      "1783 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.547, 0.520)] [G loss: 0.1] [G acc: 0.094]\n",
      "1784 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.742, 0.637)] [G loss: 0.1] [G acc: 0.109]\n",
      "1785 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.641, 0.602)] [G loss: 0.1] [G acc: 0.172]\n",
      "1786 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.492, 0.648, 0.570)] [G loss: 0.1] [G acc: 0.156]\n",
      "1787 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.500, 0.641, 0.570)] [G loss: 0.1] [G acc: 0.078]\n",
      "1788 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.609, 0.523)] [G loss: 0.1] [G acc: 0.133]\n",
      "1789 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.430, 0.641, 0.535)] [G loss: 0.1] [G acc: 0.211]\n",
      "1790 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.641, 0.551)] [G loss: 0.1] [G acc: 0.141]\n",
      "1791 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.617, 0.602)] [G loss: 0.1] [G acc: 0.281]\n",
      "1792 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.562, 0.570)] [G loss: 0.1] [G acc: 0.297]\n",
      "1793 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.555, 0.453, 0.504)] [G loss: 0.0] [G acc: 0.328]\n",
      "1794 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.625, 0.582)] [G loss: 0.0] [G acc: 0.297]\n",
      "1795 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.727, 0.621)] [G loss: 0.1] [G acc: 0.242]\n",
      "1796 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.680, 0.602)] [G loss: 0.1] [G acc: 0.156]\n",
      "1797 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.719, 0.633)] [G loss: 0.1] [G acc: 0.164]\n",
      "1798 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.703, 0.594)] [G loss: 0.1] [G acc: 0.164]\n",
      "1799 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.641, 0.586)] [G loss: 0.1] [G acc: 0.227]\n",
      "1800 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.555, 0.555)] [G loss: 0.1] [G acc: 0.266]\n",
      "1801 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.703, 0.621)] [G loss: 0.0] [G acc: 0.297]\n",
      "1802 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.586, 0.574)] [G loss: 0.0] [G acc: 0.297]\n",
      "1803 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.578, 0.555, 0.566)] [G loss: 0.0] [G acc: 0.328]\n",
      "1804 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.656, 0.562, 0.609)] [G loss: 0.1] [G acc: 0.281]\n",
      "1805 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.617, 0.602)] [G loss: 0.1] [G acc: 0.250]\n",
      "1806 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.656, 0.629)] [G loss: 0.1] [G acc: 0.289]\n",
      "1807 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.594, 0.594, 0.594)] [G loss: 0.1] [G acc: 0.266]\n",
      "1808 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.562, 0.492, 0.527)] [G loss: 0.1] [G acc: 0.188]\n",
      "1809 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.641, 0.477, 0.559)] [G loss: 0.1] [G acc: 0.258]\n",
      "1810 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.578, 0.574)] [G loss: 0.1] [G acc: 0.148]\n",
      "1811 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.578, 0.562)] [G loss: 0.1] [G acc: 0.266]\n",
      "1812 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.617, 0.578)] [G loss: 0.1] [G acc: 0.250]\n",
      "1813 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.656, 0.574)] [G loss: 0.0] [G acc: 0.281]\n",
      "1814 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.641, 0.578)] [G loss: 0.1] [G acc: 0.211]\n",
      "1815 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.688, 0.586)] [G loss: 0.1] [G acc: 0.133]\n",
      "1816 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.602, 0.590)] [G loss: 0.1] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1817 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.672, 0.613)] [G loss: 0.1] [G acc: 0.141]\n",
      "1818 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.672, 0.566)] [G loss: 0.1] [G acc: 0.133]\n",
      "1819 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.633, 0.609)] [G loss: 0.1] [G acc: 0.023]\n",
      "1820 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.398, 0.789, 0.594)] [G loss: 0.1] [G acc: 0.102]\n",
      "1821 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.484, 0.664, 0.574)] [G loss: 0.1] [G acc: 0.141]\n",
      "1822 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.352, 0.703, 0.527)] [G loss: 0.1] [G acc: 0.102]\n",
      "1823 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.375, 0.828, 0.602)] [G loss: 0.1] [G acc: 0.078]\n",
      "1824 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.391, 0.719, 0.555)] [G loss: 0.1] [G acc: 0.148]\n",
      "1825 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.758, 0.609)] [G loss: 0.1] [G acc: 0.109]\n",
      "1826 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.688, 0.551)] [G loss: 0.1] [G acc: 0.047]\n",
      "1827 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.602, 0.516)] [G loss: 0.1] [G acc: 0.156]\n",
      "1828 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.727, 0.594)] [G loss: 0.1] [G acc: 0.055]\n",
      "1829 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.594, 0.617, 0.605)] [G loss: 0.1] [G acc: 0.109]\n",
      "1830 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.609, 0.508, 0.559)] [G loss: 0.0] [G acc: 0.312]\n",
      "1831 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.531, 0.555)] [G loss: 0.0] [G acc: 0.328]\n",
      "1832 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.625, 0.602, 0.613)] [G loss: 0.1] [G acc: 0.172]\n",
      "1833 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.555, 0.570)] [G loss: 0.1] [G acc: 0.117]\n",
      "1834 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.633, 0.617)] [G loss: 0.0] [G acc: 0.148]\n",
      "1835 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.609, 0.559)] [G loss: 0.1] [G acc: 0.148]\n",
      "1836 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.633, 0.570)] [G loss: 0.1] [G acc: 0.125]\n",
      "1837 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.594, 0.547, 0.570)] [G loss: 0.1] [G acc: 0.188]\n",
      "1838 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.719, 0.570)] [G loss: 0.1] [G acc: 0.102]\n",
      "1839 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.461, 0.664, 0.562)] [G loss: 0.1] [G acc: 0.117]\n",
      "1840 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.648, 0.605)] [G loss: 0.1] [G acc: 0.148]\n",
      "1841 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.570, 0.566)] [G loss: 0.1] [G acc: 0.164]\n",
      "1842 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.555, 0.527)] [G loss: 0.1] [G acc: 0.117]\n",
      "1843 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.664, 0.586)] [G loss: 0.1] [G acc: 0.062]\n",
      "1844 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.688, 0.574)] [G loss: 0.1] [G acc: 0.234]\n",
      "1845 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.469, 0.719, 0.594)] [G loss: 0.1] [G acc: 0.180]\n",
      "1846 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.641, 0.605)] [G loss: 0.1] [G acc: 0.219]\n",
      "1847 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.562, 0.574)] [G loss: 0.1] [G acc: 0.195]\n",
      "1848 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.594, 0.559)] [G loss: 0.1] [G acc: 0.078]\n",
      "1849 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.555, 0.543)] [G loss: 0.1] [G acc: 0.172]\n",
      "1850 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.562, 0.539)] [G loss: 0.1] [G acc: 0.172]\n",
      "1851 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.453, 0.688, 0.570)] [G loss: 0.1] [G acc: 0.211]\n",
      "1852 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.750, 0.625)] [G loss: 0.1] [G acc: 0.203]\n",
      "1853 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.383, 0.641, 0.512)] [G loss: 0.1] [G acc: 0.141]\n",
      "1854 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.438, 0.562, 0.500)] [G loss: 0.1] [G acc: 0.219]\n",
      "1855 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.594, 0.582)] [G loss: 0.1] [G acc: 0.273]\n",
      "1856 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.609, 0.562)] [G loss: 0.1] [G acc: 0.164]\n",
      "1857 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.656, 0.605)] [G loss: 0.1] [G acc: 0.164]\n",
      "1858 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.562, 0.566)] [G loss: 0.1] [G acc: 0.305]\n",
      "1859 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.508, 0.641, 0.574)] [G loss: 0.1] [G acc: 0.266]\n",
      "1860 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.672, 0.645)] [G loss: 0.1] [G acc: 0.180]\n",
      "1861 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.656, 0.617)] [G loss: 0.0] [G acc: 0.297]\n",
      "1862 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.609, 0.539)] [G loss: 0.0] [G acc: 0.273]\n",
      "1863 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.648, 0.574)] [G loss: 0.1] [G acc: 0.281]\n",
      "1864 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.680, 0.586)] [G loss: 0.1] [G acc: 0.062]\n",
      "1865 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.578, 0.559)] [G loss: 0.1] [G acc: 0.109]\n",
      "1866 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.641, 0.648, 0.645)] [G loss: 0.1] [G acc: 0.234]\n",
      "1867 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.562, 0.516)] [G loss: 0.1] [G acc: 0.156]\n",
      "1868 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.477, 0.711, 0.594)] [G loss: 0.1] [G acc: 0.078]\n",
      "1869 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.453, 0.766, 0.609)] [G loss: 0.1] [G acc: 0.125]\n",
      "1870 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.672, 0.605)] [G loss: 0.1] [G acc: 0.180]\n",
      "1871 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.625, 0.664, 0.645)] [G loss: 0.1] [G acc: 0.281]\n",
      "1872 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.656, 0.633, 0.645)] [G loss: 0.0] [G acc: 0.234]\n",
      "1873 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.594, 0.617, 0.605)] [G loss: 0.1] [G acc: 0.219]\n",
      "1874 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.570, 0.543)] [G loss: 0.1] [G acc: 0.242]\n",
      "1875 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.445, 0.578, 0.512)] [G loss: 0.0] [G acc: 0.234]\n",
      "1876 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.602, 0.602)] [G loss: 0.0] [G acc: 0.258]\n",
      "1877 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.609, 0.578)] [G loss: 0.0] [G acc: 0.266]\n",
      "1878 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.578, 0.578)] [G loss: 0.1] [G acc: 0.227]\n",
      "1879 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.594, 0.562)] [G loss: 0.1] [G acc: 0.227]\n",
      "1880 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.625, 0.590)] [G loss: 0.1] [G acc: 0.109]\n",
      "1881 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.617, 0.547)] [G loss: 0.1] [G acc: 0.133]\n",
      "1882 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.633, 0.578)] [G loss: 0.1] [G acc: 0.141]\n",
      "1883 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.617, 0.535)] [G loss: 0.1] [G acc: 0.164]\n",
      "1884 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.820, 0.637)] [G loss: 0.1] [G acc: 0.211]\n",
      "1885 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.484, 0.617, 0.551)] [G loss: 0.1] [G acc: 0.234]\n",
      "1886 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.617, 0.574)] [G loss: 0.0] [G acc: 0.359]\n",
      "1887 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.617, 0.559)] [G loss: 0.0] [G acc: 0.430]\n",
      "1888 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.469, 0.500)] [G loss: 0.1] [G acc: 0.273]\n",
      "1889 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.562, 0.562)] [G loss: 0.1] [G acc: 0.133]\n",
      "1890 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.500, 0.539)] [G loss: 0.1] [G acc: 0.281]\n",
      "1891 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.570, 0.555)] [G loss: 0.1] [G acc: 0.250]\n",
      "1892 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.516, 0.516)] [G loss: 0.1] [G acc: 0.258]\n",
      "1893 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.609, 0.566)] [G loss: 0.1] [G acc: 0.305]\n",
      "1894 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.547, 0.555)] [G loss: 0.1] [G acc: 0.320]\n",
      "1895 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.703, 0.598)] [G loss: 0.1] [G acc: 0.219]\n",
      "1896 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.453, 0.773, 0.613)] [G loss: 0.1] [G acc: 0.086]\n",
      "1897 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.469, 0.797, 0.633)] [G loss: 0.1] [G acc: 0.180]\n",
      "1898 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.461, 0.773, 0.617)] [G loss: 0.1] [G acc: 0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.531, 0.758, 0.645)] [G loss: 0.1] [G acc: 0.250]\n",
      "1900 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.641, 0.574)] [G loss: 0.1] [G acc: 0.352]\n",
      "1901 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.508, 0.656, 0.582)] [G loss: 0.1] [G acc: 0.258]\n",
      "1902 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.641, 0.578)] [G loss: 0.0] [G acc: 0.320]\n",
      "1903 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.648, 0.555)] [G loss: 0.0] [G acc: 0.258]\n",
      "1904 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.695, 0.582)] [G loss: 0.1] [G acc: 0.078]\n",
      "1905 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.594, 0.703, 0.648)] [G loss: 0.1] [G acc: 0.109]\n",
      "1906 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.625, 0.598)] [G loss: 0.1] [G acc: 0.109]\n",
      "1907 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.594, 0.641, 0.617)] [G loss: 0.1] [G acc: 0.188]\n",
      "1908 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.617, 0.461, 0.539)] [G loss: 0.0] [G acc: 0.328]\n",
      "1909 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.594, 0.406, 0.500)] [G loss: 0.0] [G acc: 0.406]\n",
      "1910 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.617, 0.461, 0.539)] [G loss: 0.0] [G acc: 0.281]\n",
      "1911 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.648, 0.531, 0.590)] [G loss: 0.0] [G acc: 0.234]\n",
      "1912 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.703, 0.562, 0.633)] [G loss: 0.1] [G acc: 0.133]\n",
      "1913 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.562, 0.578, 0.570)] [G loss: 0.0] [G acc: 0.250]\n",
      "1914 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.555, 0.570, 0.562)] [G loss: 0.0] [G acc: 0.219]\n",
      "1915 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.602, 0.477, 0.539)] [G loss: 0.0] [G acc: 0.227]\n",
      "1916 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.594, 0.605)] [G loss: 0.1] [G acc: 0.258]\n",
      "1917 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.734, 0.617)] [G loss: 0.1] [G acc: 0.133]\n",
      "1918 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.477, 0.672, 0.574)] [G loss: 0.1] [G acc: 0.211]\n",
      "1919 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.633, 0.566)] [G loss: 0.1] [G acc: 0.297]\n",
      "1920 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.570, 0.547)] [G loss: 0.1] [G acc: 0.281]\n",
      "1921 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.586, 0.547)] [G loss: 0.1] [G acc: 0.094]\n",
      "1922 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.688, 0.621)] [G loss: 0.1] [G acc: 0.188]\n",
      "1923 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.578, 0.609, 0.594)] [G loss: 0.1] [G acc: 0.242]\n",
      "1924 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.664, 0.598)] [G loss: 0.1] [G acc: 0.062]\n",
      "1925 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.609, 0.711, 0.660)] [G loss: 0.1] [G acc: 0.156]\n",
      "1926 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.758, 0.648)] [G loss: 0.1] [G acc: 0.172]\n",
      "1927 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.695, 0.605)] [G loss: 0.1] [G acc: 0.164]\n",
      "1928 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.688, 0.598)] [G loss: 0.1] [G acc: 0.242]\n",
      "1929 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.703, 0.578)] [G loss: 0.0] [G acc: 0.297]\n",
      "1930 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.609, 0.566)] [G loss: 0.0] [G acc: 0.234]\n",
      "1931 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.742, 0.637)] [G loss: 0.1] [G acc: 0.219]\n",
      "1932 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.750, 0.645)] [G loss: 0.1] [G acc: 0.242]\n",
      "1933 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.672, 0.559)] [G loss: 0.1] [G acc: 0.203]\n",
      "1934 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.680, 0.547)] [G loss: 0.1] [G acc: 0.219]\n",
      "1935 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.445, 0.672, 0.559)] [G loss: 0.1] [G acc: 0.156]\n",
      "1936 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.625, 0.520)] [G loss: 0.1] [G acc: 0.070]\n",
      "1937 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.453, 0.578, 0.516)] [G loss: 0.1] [G acc: 0.312]\n",
      "1938 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.547, 0.500, 0.523)] [G loss: 0.0] [G acc: 0.227]\n",
      "1939 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.672, 0.598)] [G loss: 0.0] [G acc: 0.305]\n",
      "1940 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.570, 0.617, 0.594)] [G loss: 0.0] [G acc: 0.305]\n",
      "1941 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.656, 0.594)] [G loss: 0.0] [G acc: 0.195]\n",
      "1942 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.695, 0.582)] [G loss: 0.1] [G acc: 0.156]\n",
      "1943 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.758, 0.605)] [G loss: 0.1] [G acc: 0.172]\n",
      "1944 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.586, 0.547)] [G loss: 0.1] [G acc: 0.195]\n",
      "1945 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.633, 0.582)] [G loss: 0.1] [G acc: 0.164]\n",
      "1946 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.547, 0.641, 0.594)] [G loss: 0.1] [G acc: 0.203]\n",
      "1947 (5, 1) [D loss: (-0.0, -0.1, -0.0)] [D acc: (0.508, 0.633, 0.570)] [G loss: 0.1] [G acc: 0.297]\n",
      "1948 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.477, 0.625, 0.551)] [G loss: 0.1] [G acc: 0.180]\n",
      "1949 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.531, 0.633, 0.582)] [G loss: 0.1] [G acc: 0.266]\n",
      "1950 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.461, 0.602, 0.531)] [G loss: 0.1] [G acc: 0.219]\n",
      "1951 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.656, 0.586, 0.621)] [G loss: 0.0] [G acc: 0.242]\n",
      "1952 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.562, 0.535)] [G loss: 0.0] [G acc: 0.258]\n",
      "1953 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.539, 0.547)] [G loss: 0.0] [G acc: 0.141]\n",
      "1954 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.664, 0.461, 0.562)] [G loss: 0.0] [G acc: 0.180]\n",
      "1955 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.523, 0.539, 0.531)] [G loss: 0.0] [G acc: 0.180]\n",
      "1956 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.609, 0.641, 0.625)] [G loss: 0.1] [G acc: 0.227]\n",
      "1957 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.750, 0.641)] [G loss: 0.1] [G acc: 0.141]\n",
      "1958 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.633, 0.523)] [G loss: 0.1] [G acc: 0.227]\n",
      "1959 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.477, 0.688, 0.582)] [G loss: 0.1] [G acc: 0.203]\n",
      "1960 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.766, 0.594)] [G loss: 0.1] [G acc: 0.070]\n",
      "1961 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.430, 0.836, 0.633)] [G loss: 0.1] [G acc: 0.023]\n",
      "1962 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.508, 0.734, 0.621)] [G loss: 0.1] [G acc: 0.094]\n",
      "1963 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.453, 0.633, 0.543)] [G loss: 0.1] [G acc: 0.219]\n",
      "1964 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.789, 0.605)] [G loss: 0.1] [G acc: 0.148]\n",
      "1965 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.383, 0.781, 0.582)] [G loss: 0.1] [G acc: 0.055]\n",
      "1966 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.453, 0.773, 0.613)] [G loss: 0.1] [G acc: 0.078]\n",
      "1967 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.344, 0.719, 0.531)] [G loss: 0.1] [G acc: 0.211]\n",
      "1968 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.445, 0.688, 0.566)] [G loss: 0.1] [G acc: 0.133]\n",
      "1969 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.633, 0.582)] [G loss: 0.1] [G acc: 0.172]\n",
      "1970 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.578, 0.547)] [G loss: 0.0] [G acc: 0.211]\n",
      "1971 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.562, 0.574)] [G loss: 0.0] [G acc: 0.242]\n",
      "1972 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.547, 0.586, 0.566)] [G loss: 0.0] [G acc: 0.289]\n",
      "1973 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.516, 0.535)] [G loss: 0.0] [G acc: 0.367]\n",
      "1974 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.508, 0.547)] [G loss: 0.0] [G acc: 0.180]\n",
      "1975 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.617, 0.617, 0.617)] [G loss: 0.0] [G acc: 0.312]\n",
      "1976 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.602, 0.562, 0.582)] [G loss: 0.0] [G acc: 0.211]\n",
      "1977 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.648, 0.602, 0.625)] [G loss: 0.0] [G acc: 0.219]\n",
      "1978 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.633, 0.484, 0.559)] [G loss: 0.0] [G acc: 0.297]\n",
      "1979 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.586, 0.539, 0.562)] [G loss: 0.0] [G acc: 0.242]\n",
      "1980 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.695, 0.500, 0.598)] [G loss: 0.0] [G acc: 0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981 (5, 1) [D loss: (-0.0, 0.0, -0.0)] [D acc: (0.633, 0.461, 0.547)] [G loss: 0.0] [G acc: 0.266]\n",
      "1982 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.633, 0.602, 0.617)] [G loss: 0.0] [G acc: 0.320]\n",
      "1983 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.500, 0.602, 0.551)] [G loss: 0.0] [G acc: 0.219]\n",
      "1984 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.531, 0.742, 0.637)] [G loss: 0.0] [G acc: 0.219]\n",
      "1985 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.492, 0.711, 0.602)] [G loss: 0.1] [G acc: 0.164]\n",
      "1986 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.414, 0.641, 0.527)] [G loss: 0.1] [G acc: 0.188]\n",
      "1987 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.422, 0.711, 0.566)] [G loss: 0.1] [G acc: 0.211]\n",
      "1988 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.469, 0.844, 0.656)] [G loss: 0.1] [G acc: 0.102]\n",
      "1989 (5, 1) [D loss: (0.0, -0.1, -0.0)] [D acc: (0.406, 0.648, 0.527)] [G loss: 0.1] [G acc: 0.195]\n",
      "1990 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.367, 0.695, 0.531)] [G loss: 0.1] [G acc: 0.062]\n",
      "1991 (5, 1) [D loss: (0.0, -0.0, -0.0)] [D acc: (0.445, 0.633, 0.539)] [G loss: 0.1] [G acc: 0.172]\n",
      "1992 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.469, 0.789, 0.629)] [G loss: 0.1] [G acc: 0.117]\n",
      "1993 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.406, 0.656, 0.531)] [G loss: 0.1] [G acc: 0.227]\n",
      "1994 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.617, 0.570)] [G loss: 0.0] [G acc: 0.203]\n",
      "1995 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.555, 0.586, 0.570)] [G loss: 0.0] [G acc: 0.219]\n",
      "1996 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.523, 0.750, 0.637)] [G loss: 0.1] [G acc: 0.070]\n",
      "1997 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.539, 0.562, 0.551)] [G loss: 0.1] [G acc: 0.180]\n",
      "1998 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.516, 0.633, 0.574)] [G loss: 0.1] [G acc: 0.289]\n",
      "1999 (5, 1) [D loss: (-0.0, -0.0, -0.0)] [D acc: (0.414, 0.641, 0.527)] [G loss: 0.0] [G acc: 0.203]\n"
     ]
    }
   ],
   "source": [
    "d_losses, g_losses, d_accs, g_accs = gan.train(     \n",
    "    x_train\n",
    "    , batch_size = 128\n",
    "    , epochs = 2000\n",
    "    , run_folder = RUN_FOLDER\n",
    "    , print_every_n_batches = 10\n",
    "    , initial_epoch = 0\n",
    "    , n_critic = 5\n",
    "    , clip_threshold = 0.01\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdl",
   "language": "python",
   "name": "gdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
